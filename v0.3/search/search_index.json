{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"News & Updates","text":""},{"location":"#introduction","title":"\ud83d\udcdd Introduction","text":"<p>MiroFlow is a comprehensive framework for building intelligent AI agents that achieve state-of-the-art performance on complex reasoning tasks. It provides enhanced conversation management, flexible tool integration, and extensive benchmark evaluations across multiple datasets. </p> <p>MiroThinker is the open-source agentic model series built on this framework.</p>"},{"location":"#key-highlights","title":"\ud83c\udf1f Key Highlights","text":"<ul> <li>\ud83c\udfc6 State-of-the-Art Performance: #1 ranking across multiple agentic benchmarks</li> <li>\ud83d\udcca Premium Training Data: Curated datasets via MiroVerse</li> <li>\ud83e\udd16 Open Models: Complete collection at MiroThinker</li> <li>\ud83d\udd27 Full Training Stack: SFT/DPO recipes at MiroTrain</li> <li>\ud83c\udfaf Advanced RL: Reinforcement learning via MiroRL</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"all_about_agents/","title":"\ud83d\udcda All About Agents","text":"<p>Welcome to our comprehensive resource collection for AI agents. This page curates valuable tools, frameworks, research papers, and learning materials to help you understand and build sophisticated agent systems.</p>"},{"location":"all_about_agents/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Agent Frameworks</li> <li>Agent Memory</li> <li>Papers</li> <li>Evaluation</li> </ol>"},{"location":"all_about_agents/#agent-frameworks","title":"Agent Frameworks","text":"<ul> <li> <p>MiroFlow: Build, manage, and scale your AI agents with ease</p> <ul> <li>[GitHub]</li> </ul> </li> <li> <p>Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models</p> <ul> <li>[GitHub]</li> </ul> </li> <li> <p>OpenManus: No fortress, purely open ground. OpenManus is Coming</p> <ul> <li>[GitHub]</li> </ul> </li> <li> <p>OpenBB Platform: Financial data platform for analysts, quants and AI agents </p> <ul> <li>[Project]</li> </ul> </li> </ul>"},{"location":"all_about_agents/#agent-memory","title":"Agent Memory","text":"<ul> <li> <p>Mem0: Building Production- Ready AI Agents with Scalable Long-Term Memory</p> <ul> <li>[GitHub]</li> </ul> </li> <li> <p>memobase: Profile-Based Long-Term Memory for AI Applications</p> <ul> <li>[GitHub]</li> </ul> </li> <li> <p>Memento: Fine-tuning LLM Agents without Fine-tuning LLMs</p> <ul> <li>[Paper], [GitHub]</li> </ul> </li> </ul>"},{"location":"all_about_agents/#papers","title":"Papers","text":"<ul> <li> <p>Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>AFlow: Automating Agentic Workflow Generation </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>Throttling Web Agents Using Reasoning Gates</p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</p> <ul> <li>[Paper]</li> </ul> </li> </ul>"},{"location":"all_about_agents/#evaluation","title":"Evaluation","text":"<ul> <li> <p>LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>GAIA: a benchmark for General AI Assistants </p> <ul> <li>[Paper], [Leaderboard]</li> </ul> </li> <li> <p>xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction </p> <ul> <li>[Paper]</li> </ul> </li> <li> <p>Terminal-Bench is the benchmark for testing AI agents in real terminal environments </p> <ul> <li>[GitHub]</li> </ul> </li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"applications/","title":"\ud83d\udcf1 Applications","text":"<p>MiroFlow enables building a wide variety of intelligent applications across different domains and use cases.</p>"},{"location":"applications/#available-applications","title":"\ud83c\udfaf Available Applications","text":""},{"location":"applications/#gradio-demo","title":"Gradio Demo","text":"<p>Interactive web interface for testing MiroFlow agents locally. Currently available at MiroThinker Gradio Demo.</p>"},{"location":"applications/#live-demo","title":"Live Demo","text":"<p>Experience MiroFlow's capabilities through our online demo for deep research tasks.</p>"},{"location":"applications/#development-status","title":"\ud83d\udd04 Development Status","text":"<p>The MiroThinker model workflows are being integrated into the main MiroFlow framework. This will provide a unified experience for all applications and demos.</p> <p>Stay tuned for updates on application availability and new integrations!</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"claude-3.7-sonnet/","title":"Claude 3.7 Sonnet","text":""},{"location":"claude-3.7-sonnet/#what-this-is","title":"What This Is","text":"<p>Anthropic's Claude 3.7 Sonnet model with 200K context, strong reasoning, and tool use capabilities.</p>"},{"location":"claude-3.7-sonnet/#available-clients","title":"Available Clients","text":""},{"location":"claude-3.7-sonnet/#claudeanthropicclient-direct-api","title":"ClaudeAnthropicClient (Direct API)","text":"<p>Environment:</p> <pre><code>export ANTHROPIC_API_KEY=\"your-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.anthropic.com\"  # optional\n</code></pre> <p>Config:</p> <pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeAnthropicClient\"\n    model_name: \"claude-3-7-sonnet-20250219\"  # Use actual model name from Anthropic API\n    anthropic_api_key: \"${oc.env:ANTHROPIC_API_KEY,???}\"\n    anthropic_base_url: \"${oc.env:ANTHROPIC_BASE_URL,https://api.anthropic.com}\"\n    ...\n</code></pre>"},{"location":"claude-3.7-sonnet/#usage","title":"Usage","text":"<pre><code># Use existing config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_benchmarks/","title":"Add New Benchmarks","text":""},{"location":"contribute_benchmarks/#-coming-soon-","title":"- Coming Soon --","text":"<p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_llm_clients/","title":"Contributing New LLM Clients","text":""},{"location":"contribute_llm_clients/#what-this-does","title":"What This Does","text":"<p>Add support for new LLM providers to MiroFlow by creating a provider class.</p>"},{"location":"contribute_llm_clients/#client-structure","title":"Client Structure","text":"<p>Each LLM client inherits from <code>LLMProviderClientBase</code> and implements 4 methods: - <code>_create_client()</code> - Initialize API client - <code>_create_message()</code> - Make API calls - <code>process_llm_response()</code> - Handle responses - <code>extract_tool_calls_info()</code> - Parse tool calls</p>"},{"location":"contribute_llm_clients/#implementation-steps","title":"Implementation Steps","text":""},{"location":"contribute_llm_clients/#1-create-provider-file","title":"1. Create Provider File","text":"<p><code>src/llm/providers/your_provider_client.py</code>:</p> <pre><code>import dataclasses\nfrom src.llm.provider_client_base import LLMProviderClientBase\n\n@dataclasses.dataclass\nclass YourProviderClient(LLMProviderClientBase):\n    def _create_client(self, config):\n        # Initialize your API client\n        pass\n\n    async def _create_message(self, system_prompt, messages, tools_definitions, keep_tool_result=-1):\n        # Make API call\n        pass\n\n    def process_llm_response(self, llm_response, message_history, agent_type=\"main\"):\n        # Extract response text, return (text, should_exit)\n        pass\n\n    def extract_tool_calls_info(self, llm_response, assistant_response_text):\n        # Parse tool calls, return (tool_calls, tool_names)\n        pass\n</code></pre>"},{"location":"contribute_llm_clients/#2-register-provider","title":"2. Register Provider","text":"<p>Add to <code>src/llm/providers/__init__.py</code>:</p> <pre><code>from .your_provider_client import YourProviderClient\n</code></pre>"},{"location":"contribute_llm_clients/#3-create-config","title":"3. Create Config","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"YourProviderClient\"\n    model_name: \"your-model\"\n    your_api_key: \"${oc.env:YOUR_API_KEY,???}\"\n    your_base_url: \"${oc.env:YOUR_BASE_URL,https://api.yourprovider.com/v1}\"\n</code></pre>"},{"location":"contribute_llm_clients/#4-set-environment","title":"4. Set Environment","text":"<pre><code>export YOUR_API_KEY=\"your-key\"\nexport YOUR_BASE_URL=\"https://api.yourprovider.com/v1\"  # optional if using default\n</code></pre>"},{"location":"contribute_llm_clients/#examples","title":"Examples","text":"<p>See existing providers in <code>src/llm/providers/</code>: - <code>ClaudeAnthropicClient</code> - Direct API - <code>ClaudeOpenRouterClient</code> - Proxy API - <code>GPTOpenAIClient</code> - OpenAI API</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_tools/","title":"- Coming Soon -","text":"<p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contributors/","title":"Contributors","text":"<p>Thank you to all the amazing contributors who have helped make MiroFlow better! \ud83d\ude4f</p>"},{"location":"contributors/#core-team","title":"Core Team","text":"<p>The MiroFlow framework is developed and maintained by the MiroMind AI team.</p>"},{"location":"contributors/#community-contributors","title":"Community Contributors","text":"<p>We welcome contributions from the community! Whether you're fixing bugs, adding features, improving documentation, or helping with benchmarks, your contributions are valued.</p> <p> </p>"},{"location":"contributors/#how-to-contribute","title":"How to Contribute","text":"<p>There are many ways to contribute to MiroFlow:</p>"},{"location":"contributors/#bug-reports-feature-requests","title":"\ud83d\udc1b Bug Reports &amp; Feature Requests","text":"<ul> <li>Report bugs or request features via GitHub Issues</li> <li>Use clear, descriptive titles and provide detailed information</li> </ul>"},{"location":"contributors/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<ul> <li>Fork the repository and create a feature branch</li> <li>Follow our coding standards and include tests</li> <li>Submit a pull request with a clear description of your changes</li> </ul>"},{"location":"contributors/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Help improve our documentation</li> <li>Add examples and tutorials</li> <li>Fix typos and clarify explanations</li> </ul>"},{"location":"contributors/#testing-benchmarks","title":"\ud83e\uddea Testing &amp; Benchmarks","text":"<ul> <li>Help us test MiroFlow on different platforms</li> <li>Contribute new benchmark datasets</li> <li>Improve existing evaluation scripts</li> </ul>"},{"location":"contributors/#community-support","title":"\ud83d\udcac Community Support","text":"<ul> <li>Answer questions in our Discord community</li> <li>Help other users in GitHub discussions</li> <li>Share your experiences and use cases</li> </ul>"},{"location":"contributors/#recognition","title":"Recognition","text":"<p>All contributors are recognized in our: - GitHub contributors graph - Release notes for significant contributions - Community acknowledgments</p>"},{"location":"contributors/#getting-started","title":"Getting Started","text":"<ol> <li>Check out our GitHub repository</li> <li>Read the contributing guidelines in our README</li> <li>Join our Discord community to connect with other contributors</li> <li>Look for issues labeled \"good first issue\" to get started</li> </ol> <p>Thank you for helping us build the future of AI agents! \ud83d\ude80</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"core_concepts/","title":"\ud83e\udd16 MiroFlow Agentic Foundation Framework","text":"<p>MiroFlow provides a flexible framework for building and deploying intelligent agents capable of complex reasoning and tool use.</p>"},{"location":"core_concepts/#workflow-overview","title":"Workflow Overview","text":"<p>MiroFlow handles user queries through a multi-stage and agentic process designed for flexibility and depth. The workflow is organized as follows:</p> <ol> <li> <p>Intent Recognition &amp; Query Augmentation    LLMs analyze user input to detect intent and refine the query.</p> </li> <li> <p>Planning &amp; Task Orchestration    The main agent drafts an execution plan, invokes tools, and coordinates sub-agents.</p> </li> <li> <p>Delegation to Sub-Agents    Specialized agents (e.g., agent-browsing) handle complex or domain-specific tasks. Sub-agents independently plan, act, and execute tool calls as needed.</p> </li> <li> <p>Tool Access via MCP Servers    When external capabilities are required, agents leverage specialized tools by connecting to MCP (Model Context Protocol) servers.</p> </li> <li> <p>Result Synthesis &amp; Output Alignment    After task completion, a dedicated summary process synthesizes results, ensuring the output is high-quality and aligned with user instructions (or benchmark formats).</p> </li> </ol>"},{"location":"core_concepts/#architecture-components","title":"Architecture Components","text":"<p>All core components are located in the <code>src/</code> directory.</p> <pre><code>src/\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 pipeline.py                      # Pipeline: coordinates task execution\n\u2502   \u2514\u2500\u2500 orchestrator.py                  # Orchestrator: manages LLM \u2194 tool flow\n\u2502\n\u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 client.py                        # Unified LLM client interface\n\u2502   \u251c\u2500\u2500 provider_client_base.py          # Base class for LLM providers\n\u2502   \u251c\u2500\u2500 util.py                          # LLM utility functions\n\u2502   \u2514\u2500\u2500 providers/                       # Provider-specific implementations\n\u2502       \u251c\u2500\u2500 claude_anthropic_client.py   # Anthropic Claude client\n\u2502       \u251c\u2500\u2500 claude_newapi_client.py      # Claude via NewAPI\n\u2502       \u251c\u2500\u2500 claude_openrouter_client.py  # Claude via OpenRouter\n\u2502       \u251c\u2500\u2500 deepseek_newapi_client.py    # DeepSeek via NewAPI\n\u2502       \u251c\u2500\u2500 gpt_openai_client.py         # OpenAI GPT client\n\u2502       \u251c\u2500\u2500 gpt_openai_response_client.py # OpenAI response client\n\u2502       \u251c\u2500\u2500 mirothinker_sglang_client.py # MiroThinker via SGLang\n\u2502       \u2514\u2500\u2500 qwen_sglang_client.py        # Qwen via SGLang\n\u2502\n\u251c\u2500\u2500 tool/\n\u2502   \u251c\u2500\u2500 manager.py                       # Tool Manager: MCP server connector\n\u2502   \u2514\u2500\u2500 mcp_servers/                     # Individual MCP tool servers\n\u2502       \u251c\u2500\u2500 python_server.py             # Code execution (E2B integration)\n\u2502       \u251c\u2500\u2500 vision_mcp_server.py         # Visual perception &amp; image analysis\n\u2502       \u251c\u2500\u2500 searching_mcp_server.py      # Web search &amp; retrieval\n\u2502       \u251c\u2500\u2500 audio_mcp_server.py          # Audio transcription &amp; processing\n\u2502       \u251c\u2500\u2500 reasoning_mcp_server.py      # Enhanced reasoning capabilities\n\u2502       \u251c\u2500\u2500 reading_mcp_server.py        # Document processing &amp; file reading\n\u2502       \u251c\u2500\u2500 browser_session.py           # Persistent browser session management\n\u2502       \u2514\u2500\u2500 utils/                       # Tool utilities\n\u2502           \u2514\u2500\u2500 smart_request.py         # Smart request handling\n\u2502\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 io_utils.py                      # Output formatting utilities\n\u2502   \u251c\u2500\u2500 parsing_utils.py                 # Text parsing utilities\n\u2502   \u251c\u2500\u2500 summary_utils.py                 # Summary generation utilities\n\u2502   \u2514\u2500\u2500 tool_utils.py                    # Tool configuration helpers\n\u2502\n\u2514\u2500\u2500 logging/                             # Task logging &amp; metrics\n    \u251c\u2500\u2500 logger.py                        # Main logging interface\n    \u2514\u2500\u2500 task_tracer.py                   # Task execution tracing\n</code></pre>"},{"location":"core_concepts/#core-system","title":"Core System \ud83d\udcbb","text":"<ul> <li> <p>Pipeline (<code>src/core/pipeline.py</code>): Main entry point that coordinates task execution, creates and manages all components, handles error recovery, and returns final results. Initializes LLM clients, tool managers, and orchestrator for complete task processing.</p> </li> <li> <p>Orchestrator (<code>src/core/orchestrator.py</code>): Central coordination hub that manages multi-turn conversations, parses tool calls, executes tools, delegates to sub-agents, and handles the complete agent workflow. Supports features like message ID generation, Chinese context handling, and output formatting.</p> </li> <li> <p>LLM Client (<code>src/llm/client.py</code>): Unified interface supporting multiple language model providers including Anthropic Claude, OpenAI GPT, Google Gemini, Qwen, DeepSeek, and MiroThinker models. Provider-specific implementations in <code>src/llm/providers/</code> handle different API formats and capabilities.</p> </li> </ul>"},{"location":"core_concepts/#tool-integration","title":"Tool Integration \ud83d\udd27","text":"<ul> <li> <p>Tool Manager (<code>src/tool/manager.py</code>): Comprehensive MCP server connection manager that handles tool discovery, maintains persistent connections, manages tool blacklisting, and provides error handling. Supports both local and remote MCP servers with automatic tool definition retrieval.</p> </li> <li> <p>MCP Servers (<code>src/tool/mcp_servers/</code>): Individual tool implementations built on FastMCP protocol. Provides extensive capabilities including:</p> <ul> <li>Code Execution (<code>python_server.py</code>): E2B-powered Python sandbox for safe code execution with pre-installed packages</li> <li>Visual Perception (<code>vision_mcp_server.py</code>): Image and video analysis capabilities with format detection</li> <li>Web Search (<code>searching_mcp_server.py</code>): Google search integration with content filtering and retrieval</li> <li>Audio Processing (<code>audio_mcp_server.py</code>): Audio transcription and processing capabilities</li> <li>Enhanced Reasoning (<code>reasoning_mcp_server.py</code>): Advanced reasoning tool using high-quality language models</li> <li>Document Processing (<code>reading_mcp_server.py</code>): File reading and document analysis across multiple formats</li> <li>Browser Sessions (<code>browser_session.py</code>): Persistent browser session management for web interaction</li> <li>Smart Utilities (<code>utils/smart_request.py</code>): Intelligent request handling and optimization</li> </ul> </li> </ul>"},{"location":"core_concepts/#agent-system","title":"Agent System \ud83d\udc77","text":"<p>Main Agent The primary agent that receives user tasks and coordinates the overall execution. It can directly use reasoning tools and delegate complex tasks to specialized sub-agents. Main agents support different prompt classes for various benchmarks and use cases.</p> <p>Sub-Agents Specialized agents designed for specific domains and capabilities: - <code>agent-worker</code>: General-purpose sub-agent with access to comprehensive tool sets including web search, file processing, code execution, audio/video analysis, and document reading - Each sub-agent maintains dedicated tool configurations and custom prompts - Sub-agents can operate independently with their own LLM configurations and turn limits - Agent definitions and prompts are managed through the configuration system in <code>config/agent_prompts/</code></p>"},{"location":"core_concepts/#support-systems","title":"Support Systems \u2699\ufe0f","text":"<ul> <li> <p>Configuration System (<code>config/</code>): Hydra-powered YAML configuration for agents, LLMs, and benchmarks</p> </li> <li> <p>Output Formatter (<code>src/utils/io_utils.py</code>): Intelligent response formatting that adapts to various benchmark requirements</p> </li> <li> <p>Parsing Utilities (<code>src/utils/parsing_utils.py</code>): Text parsing and processing utilities</p> </li> <li> <p>Summary Utilities (<code>src/utils/summary_utils.py</code>): Summary generation and processing utilities</p> </li> <li> <p>Task Logger (<code>src/logging/</code>): Comprehensive logging for agent interactions, tool executions, and performance metrics</p> </li> </ul>"},{"location":"core_concepts/#configuration","title":"Configuration","text":"<p>MiroFlow uses a flat Hydra-based configuration system with agent configurations directly in the <code>config/</code> directory. Each agent configuration combines LLM settings, tool configurations, and agent behavior parameters.</p>"},{"location":"core_concepts/#configuration-structure","title":"Configuration Structure","text":"<pre><code>config/\n\u251c\u2500\u2500 agent_quickstart_1.yaml       # Quick start agent configuration\n\u251c\u2500\u2500 agent_gaia-validation.yaml    # GAIA validation agent configuration  \n\u251c\u2500\u2500 agent_mirothinker.yaml        # MiroThinker model configuration\n\u251c\u2500\u2500 agent_prompts/                # Agent prompt classes\n\u2502   \u251c\u2500\u2500 base_agent_prompt.py      # Base prompt class\n\u2502   \u251c\u2500\u2500 main_agent_prompt_gaia.py # GAIA-specific prompts\n\u2502   \u251c\u2500\u2500 main_boxed_answer.py      # Boxed answer extraction\n\u2502   \u251c\u2500\u2500 main_gaia.py             # GAIA main agent prompts\n\u2502   \u2514\u2500\u2500 sub_worker.py            # Sub-agent prompts\n\u251c\u2500\u2500 benchmark/                    # Benchmark configurations\n\u2502   \u251c\u2500\u2500 default.yaml              # Default benchmark settings\n\u2502   \u2514\u2500\u2500 gaia-validation.yaml      # GAIA validation benchmark\n\u251c\u2500\u2500 tool/                         # Tool configurations\n\u2502   \u251c\u2500\u2500 tool-code.yaml            # Code execution tool\n\u2502   \u251c\u2500\u2500 tool-searching.yaml       # Web search tool\n\u2502   \u251c\u2500\u2500 tool-reasoning.yaml       # Reasoning tool\n\u2502   \u251c\u2500\u2500 tool-reading.yaml         # Document reading tool\n\u2502   \u251c\u2500\u2500 tool-image-video.yaml     # Image/video processing tool\n\u2502   \u2514\u2500\u2500 tool-audio.yaml          # Audio processing tool\n\u2514\u2500\u2500 no-in-use-*/                  # Archive of legacy configurations\n</code></pre>"},{"location":"core_concepts/#agent-configuration-example-configagent_quickstart_1yaml","title":"Agent Configuration Example (<code>config/agent_quickstart_1.yaml</code>)","text":"<pre><code>defaults:\n  - benchmark: gaia-validation\n  - override hydra/job_logging: none\n  - _self_\n\nmain_agent:\n  prompt_class: MainAgentPromptBoxedAnswer\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.3\n    max_tokens: 32000\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n    openrouter_base_url: \"${oc.env:OPENROUTER_BASE_URL,https://openrouter.ai/api/v1}\"\n\n  tool_config: []  # Main agent with no tools (basic setup)\n  max_turns: -1\n  max_tool_calls_per_turn: 10\n  add_message_id: true\n  chinese_context: \"${oc.env:CHINESE_CONTEXT,false}\"\n\nsub_agents:\n  agent-worker:\n    prompt_class: SubAgentWorkerPrompt\n    llm: \n      provider_class: \"ClaudeOpenRouterClient\"\n      model_name: \"anthropic/claude-3.7-sonnet\"\n      temperature: 0.3\n      max_tokens: 32000\n      openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n\n    tool_config:\n      - tool-reading  # Document processing capability\n    max_turns: -1\n    max_tool_calls_per_turn: 10\n\noutput_dir: logs/\ndata_dir: \"${oc.env:DATA_DIR,data}\"\n</code></pre>"},{"location":"core_concepts/#advanced-agent-configuration-configagent_gaia-validationyaml","title":"Advanced Agent Configuration (<code>config/agent_gaia-validation.yaml</code>)","text":"<pre><code>main_agent:\n  prompt_class: MainAgentPrompt_GAIA\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.3\n    max_tokens: 32000\n\n  tool_config:\n    - tool-reasoning  # Enhanced reasoning capabilities\n\n  input_process:\n    o3_hint: true      # Use O3 hints for better performance\n  output_process:\n    o3_final_answer: true  # Extract final answers using O3\n\nsub_agents:\n  agent-worker:\n    tool_config:\n      - tool-searching     # Web search capabilities\n      - tool-image-video   # Visual content processing\n      - tool-reading       # Document processing\n      - tool-code         # Code execution\n      - tool-audio        # Audio processing\n</code></pre>"},{"location":"core_concepts/#tool-configuration-configtooltool-searchingyaml","title":"Tool Configuration (<code>config/tool/tool-searching.yaml</code>)","text":"<pre><code>name: \"tool-searching\"\ntool_command: \"python\"\nargs:\n  - \"-m\"\n  - \"src.tool.mcp_servers.searching_mcp_server\"\nenv:\n  SERPER_API_KEY: \"${oc.env:SERPER_API_KEY}\"\n  JINA_API_KEY: \"${oc.env:JINA_API_KEY}\"\n  REMOVE_SNIPPETS: \"${oc.env:REMOVE_SNIPPETS,false}\"\n  REMOVE_KNOWLEDGE_GRAPH: \"${oc.env:REMOVE_KNOWLEDGE_GRAPH,false}\"\n  REMOVE_ANSWER_BOX: \"${oc.env:REMOVE_ANSWER_BOX,false}\"\n</code></pre>"},{"location":"core_concepts/#benchmark-configuration-configbenchmarkgaia-validationyaml","title":"Benchmark Configuration (<code>config/benchmark/gaia-validation.yaml</code>)","text":"<pre><code>defaults:\n  - default\n  - _self_\n\nname: \"gaia-validation\"\n\ndata:\n  data_dir: \"${oc.env:DATA_DIR,data}/gaia-val\"\n\nexecution:\n  max_tasks: null  # null means no limit\n  max_concurrent: 5\n</code></pre>"},{"location":"core_concepts/#key-features","title":"Key Features","text":""},{"location":"core_concepts/#multi-agent-architecture","title":"Multi-Agent Architecture","text":"<ul> <li>Main Agent: Coordinates overall task execution and reasoning</li> <li>Sub-Agents: Specialized agents with dedicated tool sets for specific domains</li> <li>Dynamic Delegation: Intelligent task routing based on capability requirements</li> </ul>"},{"location":"core_concepts/#advanced-configuration","title":"Advanced Configuration","text":"<ul> <li>Flexible LLM Support: Multiple provider integrations with unified interface</li> <li>Tool Modularity: Mix and match tools based on task requirements  </li> <li>Benchmark Integration: Pre-configured setups for popular AI benchmarks</li> <li>Environment Management: Secure API key and environment variable handling</li> </ul>"},{"location":"core_concepts/#production-features","title":"Production Features","text":"<ul> <li>Error Recovery: Robust error handling and graceful degradation</li> <li>Logging &amp; Tracing: Comprehensive task execution monitoring</li> <li>Concurrent Execution: Parallel task processing capabilities</li> <li>Resource Management: Efficient tool connection pooling and cleanup</li> </ul>"},{"location":"core_concepts/#examples","title":"Examples","text":"<p>Check out our example applications to see agents in action.</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"data/","title":"\ud83d\udcca Data","text":""},{"location":"data/#news-updates","title":"\ud83d\udd25 News &amp; Updates","text":"<ul> <li>The data is released over Huggingface.</li> <li>MiroVerse v0.1 has been released. This dataset can be used with our training framework, MiroTrain. In MiroVerse v0.1, we provide both SFT and DPO data, making it easy to reproduce MiroThinker-v0.1\u2019s benchmark performance on Qwen3. Give it a try!</li> </ul>"},{"location":"data/#first-batch-of-miroverse","title":"\ud83d\udd25 First Batch of MiroVerse","text":"<p>\u2728 What makes this release special:</p> <ul> <li>\ud83d\udcda Diverse Verified Open Source Data \u2014 Carefully curated and validated community datasets</li> <li>\ud83e\udde0 Fresh Large-Scale Deep Research Data \u2014 Generated by our proprietary data engine</li> <li>\ud83d\udd04 Complete Trajectory Coverage \u2014 Every single sample includes full rollout trajectories</li> <li>\u2705 Quality Assurance: \u2014 Each trajectory has been verified, ensuring high-quality training data for your models.</li> <li>\ud83c\udf31 Always Growing, Always Open \u2014 Regular updates, powered by collaboration with the community</li> </ul>"},{"location":"data/#dataset-overview","title":"\ud83d\udce6 Dataset Overview","text":"<p>MiroVerse-v0.1 is a large-scale agent dataset with 147K+ samples featuring full rollout trajectories across diverse AI agent tasks including multi-hop QA, web navigation, and scientific reasoning. Every single sample includes complete execution traces with 1.9B+ tokens and 602K+ tool interactions, providing comprehensive training data for tool-using and web-browsing AI agents.</p> Split #Sample #Main Trace #Browse Trace #Token #Turns #Tools License MiroVerse-Voyager1.0 59097 19115 39982 1129113893 444723 325537 CC-BY-NC-4.0 MiroVerse-MuSiQue 29572 10422 19150 294351053 143080 90486 CC-BY-4.0 MiroVerse-HotpotQA 12942 6553 6389 67352039 46320 20524 CC-BY-SA-4.0 MiroVerse-WebWalkerQA-Silver 10817 4961 5856 107650324 67846 46215 Apache 2.0 MiroVerse-MegaScience 10615 8270 2345 111120264 63594 42443 CC-BY-NC-SA-4.0 MiroVerse-TaskCraft 8890 4277 4613 95518109 35013 17236 MIT MiroVerse-QA-Expert-Multi-Hop-V1.0 6187 2091 4096 63983151 31957 19585 Apache 2.0 MiroVerse-OneGen-TrainDataset-MultiHopQA 3289 1347 1942 33214386 17187 11449 MIT MiroVerse-2WikiMultihopQA 3001 1410 1591 28977451 13982 7981 Apache 2.0 MiroVerse-WikiTables 1606 1288 318 16461870 12089 8877 MIT MiroVerse-WebShaper 1514 486 1028 31240265 12126 9578 MIT MiroVerse-WebDancer 455 192 263 7817689 3170 2268 MIT MiroVerse-v0.1 147985 60412 87573 1993099086 891087 602179 / <p>Every sample includes successful MiroFlow rollout trajactories that reached the verified answer\u2014one JSON line, zero secrets. MiroVerse-v0.1 dataset follows a hybrid licensing model: query and answer data retain their original source licenses, while all trace data is licensed under CC-BY-NC-4.0; for commercial use, please contact us to request a commercial license.</p>"},{"location":"data/#why-were-different","title":"\ud83c\udd9a Why We're Different","text":"<p>While high-quality data is essential for training advanced models and often kept private, we believe that the path to truly general-purpose agents is still long. That\u2019s why we\u2019re committed to open-sourcing as much of our data as possible\u2014including raw samples and exploration traces\u2014to support and accelerate progress across the community.</p> Org Work Samples Trace Data Reproducible? OpenAI Deep Research \u2014 \u274c \u274c Gemini Gemini Deep Research \u2014 \u274c \u274c Tencent Cognitive Kernel-Pro 7 k \u274c \u274c Tongyi WebShaper 500 \u274c \u274c MiroMind (ours) this repo 147 k+ \u2705 \u2705"},{"location":"data/#benchmark-performance","title":"\ud83d\udcc8 Benchmark Performance","text":"<p>MiroVerse-v0.1 is used in the training of our MiroThinker-v0.1 models.</p> <p>By using this dataset, we achieved the following benchmark performance.</p>"},{"location":"data/#gaia-benchmark","title":"GAIA Benchmark","text":"Method Text-103Best Pass@1 Text-103Pass@1 (Avg@8) Val-165Best Pass@1 Val-165Pass@1 (Avg@8) Search-o1-7B 17.5 - - - R1-Searcher-7B 20.4 - - - WebDancer-7B 31.0 - - - WebSailor-7B 37.9 - - - CK-Pro-8B 43.7 - 35.2 - MiroThinker-8B-SFT-v0.1 44.7 40.1 34.6 31.8 + Commercial Tools 46.6 42.1 37.6 33.9 MiroThinker-8B-DPO-v0.1 46.6 44.8 37.0 35.4 + Commercial Tools 50.5 46.7 38.2 35.9 MiroThinker-14B-SFT-v0.1 47.6 44.4 37.0 34.4 + Commercial Tools 49.5 47.5 41.8 39.8 MiroThinker-14B-DPO-v0.1 48.5 46.6 42.4 39.2 + Commercial Tools 52.4 48.5 45.5 42.0 Qwen3-32B 31.1 26.7 29.7 26.4 Search-o1-32B 28.2 - - - WebThinker-32B-RL 48.5 - - - WebDancer-QwQ-32B 51.5 - - - WebSailor-32B 53.2 - - - WebShaper-QwQ-32B 53.3 - - - MiroThinker-32B-SFT-v0.1 55.3 51.3 44.9 42.7 + Commercial Tools 58.3 54.2 48.5 45.8 MiroThinker-32B-DPO-v0.1 57.3 54.1 48.5 45.9 + Commercial Tools 60.2 57.9 50.9 48.9 <ol> <li> <p>Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.</p> </li> <li> <p>For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.</p> </li> <li> <p>By default, we use open-source tools wherever possible, except for the code tool E2B and the Google search tool Serper. We use Whisper, Qwen2.5-VL-72B-Instruct, and Qwen3-235B-A22B-Thinking-2507 in our implementation. The framework can be easily extended to other open-source tools of your choice.</p> </li> <li> <p>Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our models.</p> </li> </ol>"},{"location":"data/#more-benchmarks","title":"More Benchmarks","text":"Method HLEPass@1 FramesPass@1 BrowseCompPass@1 BrowseComp-ZHPass@1 WebWalkerQAPass@1 OpenAI Deep Research 26.6 - 51.5 42.9 - Gemini Deep Research 26.9 - - - - Kimi-Researcher 26.9 78.8 - - - WebDancer-7B - - - - 36.0 WebSailor-7B - - 6.7 14.2 - MiroThinker-8B-SFT-v0.1 - 58.0 5.5 9.3 41.3 MiroThinker-8B-DPO-v0.1 - 64.4 8.7 13.5 45.7 WebThinker-32B-RL - - - - 46.5 WebDancer-QwQ-32B - - 3.8 18.0 47.9 WebSailor-32B - - 10.5 25.5 - WebShaper-32B - - - - 51.4 MiroThinker-32B-SFT-v0.1 10.2 70.4 10.6 13.8 45.7 MiroThinker-32B-DPO-v0.1 11.8 71.7 13.0 17.0 49.3 <ol> <li> <p>MiroThinker\u2019s performance was tested with this repository and open-source tools; other models\u2019 results are from their papers and official sites.</p> </li> <li> <p>As MiroVerse-v0.1 mainly contains English data, the model\u2019s Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.</p> </li> </ol>"},{"location":"data/#examples","title":"\ud83e\udde9 Examples","text":"<p>Below are two QA examples synthesized by our data engine (MiroVerse-Voyager1.0).</p>"},{"location":"data/#case-1","title":"Case 1","text":"<p>Q: A female lead actress received her first major annual Hindi film performance award for best actress for her role in a late-2000s comedy-drama, directed by the filmmaker who later created a sports-themed drama released in 2023 starring an actress known for completing an athletic triathlon event in Berlin. What is the title of the film for which this actress first won that award?</p> <p>A: Paa</p>"},{"location":"data/#case-2","title":"Case 2","text":"<p>Q: Identify the agricultural practice, unique to a mountain range that forms a border including an independent principality and known for spectacular geologic landforms, that was one of the key reasons for part of the range's inscription as a UNESCO World Heritage Site in the decade before the 21st century. This region's history features a brief early-1800s reorganization of provincial boundaries after a liberal revolution in the southern country, and the northern country is globally recognized as the leading tourist destination with the fourth-largest number of heritage sites. What is this traditional agricultural system called?</p> <p>A: transhumance</p>"},{"location":"data/#free-trace-rollout-let-us-help-you-train","title":"\ud83d\udee0\ufe0f Free Trace Rollout: Let Us Help You Train","text":"<p>Generating high-quality training trajectories is expensive \u2014 on average, $1.50 per sample using top-tier commercial models.</p> <p>To empower the community, we\u2019re offering free rollout services for qualifying seed data:</p>"},{"location":"data/#how-it-works","title":"How It Works:","text":"<ol> <li> <p>Submit a Request</p> <p>Open a ticket via this template and provide the basic info, rollout requirements, and up to 100 sample rows in one go.</p> </li> <li> <p>Review &amp; Rollout</p> <p>We\u2019ll review your submission within 48 hours. Once approved, we\u2019ll reach out to you for the full dataset and then launch the complete trace rollout using top-tier commercial models.</p> </li> <li> <p>Delivery &amp; Recognition</p> <p>Upon completion, we\u2019ll send the augmented dataset to you via email.</p> <p>With your explicit consent, we\u2019ll also publish it publicly and credit you as a Community Contributor \u2014 with a permanent badge in this README.</p> </li> </ol>"},{"location":"data/#license","title":"\ud83e\udd1d License","text":"<p>This project is released under the CC BY-NC 4.0. Parts of this project contain code and models from other sources, which are subject to their respective licenses. For commercial use cases, please contact us at: service@miromind.ai.</p>"},{"location":"data/#citation","title":"\ud83d\udcdc Citation","text":"<p>If you find this project useful in your research, please consider cite:</p> <pre><code>@misc{miromind2024opendata,\n  title={MiroVerse V0.1: A Reproducible, Full-Trajectory, Ever-Growing Deep Research Dataset},\n  author={MiroMind Data Team},\n  year={2025},\n  url={https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1}\n}\n</code></pre>"},{"location":"data/#contact-us","title":"Contact Us","text":"<p>MiroVerse is developed by the MiroMind Data Team. If you would like to leave us a message, feel free to get in touch.  In addition to GitHub,  Discord,  WeChat,  and RedNote,  you can also reach us via email at service@miromind.ai.</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"e2b_extension/","title":"E2B Extension","text":"<p>We provide an option for local E2B Sandbox.</p>"},{"location":"e2b_extension/#local-e2b-sandbox-deployment","title":"Local E2B Sandbox Deployment","text":"<p>To achieve our best benchmark results, we recommend using a pre-defined sandbox template that includes the most commonly used Python and apt packages. </p> <p>If you prefer not to use a sandbox template, you can disable it by commenting out the line <code>template=DEFAULT_TEMPLATE_ID,</code> in <code>libs/miroflow-tool/src/miroflow/tool/mcp_servers/python_server.py</code> (line 145).</p>"},{"location":"e2b_extension/#prepare-e2b-sandbox-optional","title":"Prepare E2B Sandbox (Optional)","text":"<p>[!TIP] We provide a public E2B sandbox template. Follow this step if you want to reproduce the best scores.</p> <p>For the E2B sandbox service, we recommend setting up a Linux Docker image with a comprehensive set of apt and Python packages pre-installed. Without these pre-installed packages, the agent will need to spend extra steps and context installing them, resulting in reduced token efficiency.</p> <p>you need to have <code>npm</code> install and <code>docker</code> running locally.</p> <ol> <li>Install <code>e2b</code> command line and login:</li> </ol> <pre><code>## install e2b\nnpm install -g @e2b/cli\n## check that it is available\nwhich e2b \n</code></pre> <ol> <li>Download our pre-configured Dockerfile: e2b.Dockerfile.</li> </ol> <pre><code>wget https://github.com/MiroMindAI/MiroFlow/blob/main/docs/e2b.Dockerfile\n</code></pre> <ol> <li>Run <code>e2b template build</code> command check official doc here, use <code>all_pip_apt_pkg</code> as the name of template.</li> </ol> <pre><code>## build the template with `docker build` locally\nE2B_ACCESS_TOKEN=${your-token}\ne2b template build -c \"/root/.jupyter/start-up.sh\" -n \"all_pip_apt_pkg\" -d ./e2b.Dockerfile\n## check that template is built successfully\nE2B_ACCESS_TOKEN=${your-token} e2b template list\n</code></pre> <p>You can also create your own custom sandbox template for specific use cases by following similar steps. For more information, please refer to the E2B Docker documentation.</p>"},{"location":"e2b_extension/#e2b-docker","title":"E2B Docker","text":"<p>This document describes the custom E2B Docker environment used by MiroFlow for code execution. The E2B extension provides a sandboxed environment with pre-installed scientific computing libraries, data analysis tools, and other dependencies commonly needed for AI agent tasks.</p> <pre><code># You can use most Debian-based base images\nFROM e2bdev/code-interpreter\n\n# Update package list and install Python 3.10 and pip\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    portaudio19-dev \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel\n\n# Install dependencies and customize sandbox\nRUN python3 -m pip install --no-cache-dir \\\n    Flask \\\n    IPython \\\n    Pillow \\\n    PyGithub \\\n    PyMuPDF \\\n    PyPDF2 \\\n    arch \\\n    arm-pyart \\\n    arxiv \\\n    ase \\\n    astropy \\\n    astroquery \\\n    awscli \\\n    beautifulsoup4 \\\n    biopython \\\n    boto3 \\\n    brian2 \\\n    cairosvg \\\n    cgt \\\n    chardet \\\n    chess \\\n    cinemagoer \\\n    clifford \\\n    contextily \\\n    control \\\n    cryptography \\\n    cvxpy \\\n    datasets \\\n    descarteslabs \\\n    duckduckgo-search \\\n    edalize \\\n    english_words \\\n    ephem \\\n    esp-docs \\\n    flask \\\n    folium \\\n    geopandas \\\n    geopy \\\n    google-search-results \\\n    googlesearch-python \\\n    googletrans \\\n    habanero \\\n    helics \\\n    hijri_converter \\\n    imbalanced-learn \\\n    inflect \\\n    isbnlib \\\n    kaggle \\\n    lifelines \\\n    lxml \\\n    lxml_html_clean \\\n    mapclassify \\\n    markdown \\\n    'matplotlib&gt;=3.8' \\\n    mendeleev \\\n    metpy \\\n    music21 \\\n    networkx \\\n    nipype \\\n    numba \\\n    'numpy&gt;=2' \\\n    opencv-python \\\n    openpyxl \\\n    'pandas&gt;=2' \\\n    pandas_datareader \\\n    parsl \\\n    pdf2image \\\n    pdfminer \\\n    pdfplumber \\\n    periodictable \\\n    plotly \\\n    polars \\\n    psycopg2-binary \\\n    pulp \\\n    pyXSteam \\\n    pybel \\\n    pycryptodome \\\n    pydot \\\n    pygplates \\\n    pymatgen \\\n    pymupdf \\\n    pypdf2 \\\n    pypinyin \\\n    pyscf \\\n    pytesseract \\\n    python-docx \\\n    pytube \\\n    pywavelets \\\n    rdflib \\\n    reportlab \\\n    requests \\\n    requests-html \\\n    scanpy \\\n    scikit-image \\\n    scikit-learn \\\n    scipy \\\n    scvelo \\\n    seaborn \\\n    selenium \\\n    semanticscholar \\\n    shap \\\n    shapely \\\n    siphon \\\n    skyfield \\\n    smbus2 \\\n    snappy \\\n    spglib \\\n    sphinx \\\n    splink \\\n    statsmodels \\\n    stockfish \\\n    sympy \\\n    tabulate \\\n    torch \\\n    torchvision \\\n    transformers \\\n    uncertainpy \\\n    us \\\n    virtualenv \\\n    wbdata \\\n    webdriver-manager \\\n    wikipedia-api \\\n    wolframalpha \\\n    wordfreq \\\n    yfinance \\\n    yt-dlp \\\n    docx2txt \\\n    rdkit \\\n    stockfish \\\n    yfinance \\\n    seaborn \\\n    python-pptx \\\n    pyaudio \\\n    pyshp \\\n    SpeechRecognition \\\n    waybackpy\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    # \u2500\u2500 Basic build &amp; Python \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    build-essential gfortran cmake pkg-config git curl wget ca-certificates \\\n    # \u2500\u2500 scientific computing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libopenblas-dev liblapack-dev libatlas-base-dev \\\n    libssl-dev libffi-dev zlib1g-dev \\\n    # \u2500\u2500 image / OpenCV / Pillow \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libgl1 libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender1 \\\n    libjpeg-dev libpng-dev libwebp-dev libfreetype6-dev libopenjp2-7 liblcms2-dev \\\n    # \u2500\u2500 video / audio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ffmpeg libsndfile1 sox portaudio19-dev \\\n    # \u2500\u2500 PDF / doc / OCR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    poppler-utils pdfgrep ghostscript \\\n    tesseract-ocr tesseract-ocr-deu \\\n    libxml2-dev libxslt1-dev \\\n    # \u2500\u2500 other tools \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    imagemagick unlambda stockfish \\\n    unzip zip tar nano &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"evaluation_overview/","title":"\ud83d\udcca Performance Benchmarks","text":"<p>MiroFlow achieves state-of-the-art performance across multiple agentic benchmarks, demonstrating its effectiveness in complex reasoning and tool-use tasks.</p>"},{"location":"evaluation_overview/#performance-on-future-prediction","title":"Performance on Future Prediction","text":""},{"location":"evaluation_overview/#performance-on-benchmarks","title":"\u2728 Performance on Benchmarks","text":"<p>We benchmark MiroFlow on a series of benchmarks including GAIA, HLE, BrowseComp and xBench-DeepSearch.</p>"},{"location":"evaluation_overview/#other-benchmark-results","title":"Other Benchmark Results","text":"Model/Framework GAIA Val HLE HLE-Text MiroFlow 82.4% 27.2% 29.5% OpenAI Deep Research 67.4% 26.6% - Gemini Deep Research - 26.9% - Kimi Researcher - - 26.9% WebSailor-72B 55.4% - - Manus 73.3% - - DeepSeek v3.1 - - 29.8% Model/Framework BrowserComp-EN BrowserComp-ZH xBench-DeepSearch MiroFlow 33.2% 47.1% 72.0% OpenAI Deep Research 51.5% 42.9% - Gemini Deep Research - - 50+% Kimi Researcher - - 69.0% WebSailor-72B - 30.1% 55.0% DeepSeek v3.1 - - 71.2% <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"faq_and_known_issues/","title":"\ud83d\udc1b FAQs and Known Issues","text":""},{"location":"faq_and_known_issues/#faq","title":"FAQ","text":"<p>Q: What is the estimated cost of running the GAIA validation set for a single run? A: The cost is approximately $250 USD for a run with cache.</p> <p>Q: How long does it take to run the GAIA validation set for a single run? A: With the <code>max_concurrent</code> parameter set to 20, a full run takes about 2 hours to complete.</p> <p>Q: Are all the specified APIs required? A: Yes. To fully reproduce our published results, access to all the listed APIs in corresponding benchmark is necessary.</p> <p>Q: What is the difference between MiroFlow and MiroThinker? A:  MiroFlow is primarily focused on interacting with proprietary models; MiroThinker is designed for our own open-source models.</p> <p>We plan to merge these two projects in the future to create a single, unified platform.</p>"},{"location":"faq_and_known_issues/#known-issues-roadmap","title":"Known Issues &amp; Roadmap","text":""},{"location":"faq_and_known_issues/#currently-in-development","title":"\ud83d\udd04 Currently in Development","text":"<ul> <li>FutureX Benchmark: Adding support for FutureX benchmark evaluation</li> <li>Token Usage &amp; Cost Tracking: Implementing detailed usage analytics and cost calculation features</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_test/","title":"GAIA Test","text":""},{"location":"gaia_test/#-coming-soon-","title":"- Coming Soon -","text":"<p>Last Updated: Sep 2025 Doc Contributor: Index @ MiroMind AI</p>"},{"location":"gaia_validation/","title":"GAIA Validation","text":""},{"location":"gaia_validation/#performance-comparison","title":"Performance Comparison","text":"<p>MiroFlow achieves state-of-the-art (SOTA) performance among open-source agent frameworks on the GAIA validation set:</p> <p>Key Performance Metrics:</p> <ul> <li>Pass@3: 81.8%</li> <li>Majority Vote: 82.4%</li> <li>Pass@1 (best@3): 74.5%</li> <li>Pass@1 (avg@3): 72.2%</li> </ul> <p>Unlike other frameworks with unclear evaluation methods, MiroFlow's results are fully reproducible. Note that Hugging Face access was disabled during inference to prevent direct answer retrieval</p>"},{"location":"gaia_validation/#reproduction-guide","title":"Reproduction Guide","text":"<p>This section provides step-by-step instructions to reproduce our GAIA validation benchmark results. All results are fully reproducible using our open-source framework.</p>"},{"location":"gaia_validation/#step-1-prepare-the-gaia-validation-dataset","title":"Step 1: Prepare the GAIA Validation Dataset","text":"<p>First, download and prepare the GAIA validation dataset:</p> <pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-val.zip\nunzip gaia-val.zip\n# The unzip passcode is: `pf4*`\n</code></pre>"},{"location":"gaia_validation/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> <pre><code>\n# For searching and scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use Claude-3.5-Sonnet with OpenRouter backend to initialize the LLM. The main reason is that OpenRouter provides better response rates\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n\n# Use for llm judge, reasoning, o3 hints, etc.\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n\n\n</code></pre>"},{"location":"gaia_validation/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the following command to run a single evaluation pass on the GAIA validation dataset:</p> <pre><code>uv run main.py common-benchmark --config_file_name=agent_gaia-validation output_dir=\"logs/gaia-validation/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>To check the progress while running:</p> <pre><code>uv run uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre>"},{"location":"gaia_validation/#traces","title":"Traces","text":"<p>We have released our complete execution traces for the <code>gaia-validation</code> dataset on Hugging Face. This comprehensive collection includes a full run of 165 tasks with an overall accuracy of 73.94%. You can download them using the following command:</p> <pre><code>wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia_validation_miroflow_trace_public_20250825.zip\nunzip gaia_validation_miroflow_trace_public_20250825.zip\n# The unzip passcode is: `pf4*`.\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"license/","title":"License","text":"<p>This project is licensed under the Apache License 2.0. Some components may have different licenses as specified in their respective file headers.</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"llm_clients_overview/","title":"LLM Clients Overview","text":""},{"location":"llm_clients_overview/#available-clients","title":"Available Clients","text":"Client Provider Model Environment Variables <code>ClaudeAnthropicClient</code> Anthropic Direct claude-3-7-sonnet <code>ANTHROPIC_API_KEY</code>, <code>ANTHROPIC_BASE_URL</code> <code>ClaudeOpenRouterClient</code> OpenRouter anthropic/claude-3.7-sonnet, and other supported models <code>OPENROUTER_API_KEY</code>, <code>OPENROUTER_BASE_URL</code> <code>GPTOpenAIClient</code> OpenAI gpt-4, gpt-3.5 <code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code> <code>MiroThinkerSGLangClient</code> SGLang MiroThinker series <code>OAI_MIROTHINKER_API_KEY</code>, <code>OAI_MIROTHINKER_BASE_URL</code>"},{"location":"llm_clients_overview/#basic-configuration","title":"Basic Configuration","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"ClientName\"\n    model_name: \"model-name\"\n    api_key_param: \"${oc.env:API_KEY,???}\"\n    base_url_param: \"${oc.env:BASE_URL,default-url}\"\n    ...\n</code></pre>"},{"location":"llm_clients_overview/#quick-start","title":"Quick Start","text":"<ol> <li>Set relevant environment variable for your chosen provider</li> <li>Update your yaml config file</li> <li>Run: <code>uv run main.py trace --config_file_name=your_config_file --task=\"task\" --task_file_name=\"file\"</code></li> </ol> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"mirothinker/","title":"MiroThinker","text":""},{"location":"mirothinker/#mirothinker","title":"\ud83c\udf1f MiroThinker","text":"<p>MiroThinker (4B/7B/14B/32B) is our suite of open-source agentic models, designed to work seamlessly with the MiroFlow framework. Our models are specifically built to handle complex, multi-tool tasks, leveraging the reproducible and robust foundation that MiroFlow provides.</p> <p>By combining MiroFlow's reliable orchestration with MiroThinker's advanced reasoning capabilities, we offer a powerful, end-to-end solution for building high-performing, reproducible AI agents. These models are a direct result of our extensive data collection efforts, utilizing MiroFlow to generate high-quality, post-training agent trace data. This unique approach enables MiroThinker to excel in planning, executing, and reasoning through complex multi-step tasks.</p>"},{"location":"mirothinker/#deploying-mirothinker-32b-with-miroflow","title":"Deploying MiroThinker-32B with MiroFlow","text":"<p>This guide explains how to deploy the MiroThinker-32B-DPO-v0.2 model from Hugging Face and integrate it with MiroFlow.</p>"},{"location":"mirothinker/#prerequisites","title":"Prerequisites","text":"<ul> <li>SGLang installed</li> <li>Sufficient GPU memory for the model</li> <li>MiroFlow repository set up</li> </ul>"},{"location":"mirothinker/#step-1-deploy-model-with-sglang","title":"Step 1: Deploy Model with SGLang","text":"<p>Deploy the MiroThinker-32B model using SGLang with the following command:</p> <pre><code>python3 -m sglang.launch_server \\\n    --model-path miromind-ai/MiroThinker-32B-DPO-v0.2 \\\n    --tp 8 \\\n    --dp 1 \\\n    --host 0.0.0.0 \\\n    --port 61005 \\\n    --trust-remote-code \\\n    --chat-template qwen3_nonthinking.jinja\n</code></pre> <p>Important Notes: - Adjust the <code>--tp</code> (tensor parallelism) parameter to match your number of GPUs - Download the chat template from: qwen3_nonthinking.jinja - Ensure the port you used (in this case 61005) is available on your system</p>"},{"location":"mirothinker/#step-2-configure-miroflow","title":"Step 2: Configure MiroFlow","text":"<p>Once the SGLang server is running, configure MiroFlow by adding the following to your <code>.env</code> file:</p> <pre><code>OAI_MIROTHINKER_API_KEY=\"dummy_key\"\nOAI_MIROTHINKER_BASE_URL=\"http://localhost:61005/v1\"\n</code></pre> <p>Note:  - If your model requires authentication, replace <code>dummy_key</code> with your actual API key - Replace <code>localhost</code> with the appropriate hostname if deploying on a remote server</p>"},{"location":"mirothinker/#step-3-test-the-integration","title":"Step 3: Test the Integration","text":"<p>Test your setup with the following command:</p> <pre><code>uv run main.py trace --config_file_name=agent_mirothinker \\\n    --task=\"What is the first country listed in the XLSX file that have names starting with Co?\" \\\n    --task_file_name=\"data/FSI-2023-DOWNLOAD.xlsx\"\n</code></pre> <p>This command will: - Use the <code>agent_mirothinker</code> configuration with the dedicated MiroThinkerSGLangClient - Process the specified Excel file - Query the model to find countries starting with \"Co\"</p>"},{"location":"mirothinker/#configuration-details","title":"Configuration Details","text":"<p>The <code>./config/agent_mirothinker.yaml</code> configuration file uses: - <code>provider_class: \"MiroThinkerSGLangClient\"</code> - A dedicated client for MiroThinker models deployed with SGLang - Model path and generation parameters (temperature, top_p, max_tokens, etc.) - Environment variables for API endpoint configuration</p> <p>Last Updated: Sep 2025 Doc Contributor: Xalp @MiroMind AI</p>"},{"location":"openai-gpt/","title":"OpenAI GPT Models","text":""},{"location":"openai-gpt/#what-this-is","title":"What This Is","text":"<p>OpenAI's latest models including GPT-4o and O3 reasoning models with strong coding, vision, and reasoning capabilities.</p>"},{"location":"openai-gpt/#client-used","title":"Client Used","text":"<p><code>GPTOpenAIClient</code></p>"},{"location":"openai-gpt/#environment-setup","title":"Environment Setup","text":"<pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # optional\n</code></pre>"},{"location":"openai-gpt/#configuration","title":"Configuration","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"GPTOpenAIClient\"\n    model_name: \"gpt-4o\"  # or o3, etc.\n    openai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n    openai_base_url: \"${oc.env:OPENAI_BASE_URL,https://api.openai.com/v1}\"\n    ...\n</code></pre>"},{"location":"openai-gpt/#usage","title":"Usage","text":"<pre><code># Create custom OpenAI config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"openrouter-claude-3.7-sonnet/","title":"OpenRouter Claude 3.7 Sonnet (Recommended)","text":""},{"location":"openrouter-claude-3.7-sonnet/#what-this-is","title":"What This Is","text":"<p>Access multiple models via OpenRouter using unified OpenAI chat format. Supports Claude, GPT, and other models with higher rate limits.</p>"},{"location":"openrouter-claude-3.7-sonnet/#client-used","title":"Client Used","text":"<p><code>ClaudeOpenRouterClient</code></p>"},{"location":"openrouter-claude-3.7-sonnet/#environment-setup","title":"Environment Setup","text":"<pre><code>export OPENROUTER_API_KEY=\"your-openrouter-key\"\nexport OPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"  # optional\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#configuration","title":"Configuration","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"  # or openai/gpt-4, etc.\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n    openrouter_base_url: \"${oc.env:OPENROUTER_BASE_URL,https://openrouter.ai/api/v1}\"\n    openrouter_provider: \"anthropic\"  # Force provider, or \"\" for auto\n    ...\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#other-supported-models","title":"Other Supported Models","text":"<ul> <li><code>openai/gpt-4</code></li> <li><code>openai/gpt-3.5-turbo</code></li> <li><code>anthropic/claude-3-opus</code></li> <li><code>google/gemini-pro</code></li> <li>Many others via unified OpenAI format</li> </ul>"},{"location":"openrouter-claude-3.7-sonnet/#usage","title":"Usage","text":"<pre><code># Use existing OpenRouter config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#benefits-vs-direct-api","title":"Benefits vs Direct API","text":"<ul> <li>Unified chat format</li> <li>Higher rate limits</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"prepare_benchmark_data_from_original_source/","title":"Prepare Official Benchmark Datasets","text":""},{"location":"prepare_benchmark_data_from_original_source/#-coming-soon-","title":"- Coming Soon -","text":"<p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"prepare_benchmark_data_from_original_source/#-","title":"- - - - - - - - - - - - - - - - - - - - - -","text":""},{"location":"prepare_benchmark_data_from_original_source/#-the-followings-are-kept-for-future-references-","title":"- The followings are kept for future references -","text":""},{"location":"prepare_benchmark_data_from_original_source/#evaluate-on-benchmark","title":"Evaluate on Benchmark","text":"<p>Prepare datasets according to your requirements. Some datasets may need to be downloaded manually into the <code>/data/&lt;benchmark&gt;</code> folder, and you should also create a corresponding <code>standardized_data.jsonl</code> metafile. We will support as many datasets as possible as soon as we can.</p> <pre><code>## supported benchmarks\ncd MiroFlow/apps/prepare-benchmark\nuv run main.py get gaia-val\nuv run main.py get browsecomp-test\nuv run main.py get browsecomp-zh-test\nuv run main.py get hle\n</code></pre> <p>Run evaluation using the default settings. (Not parallelized; not recommended.)</p> <pre><code>## run the code\ncd MiroFlow/apps/run-agent\nuv run main.py common-benchmark benchmark=gaia-validation\nuv run main.py common-benchmark benchmark=browsecomp\nuv run main.py common-benchmark benchmark=browsecomp-zh\nuv run main.py common-benchmark benchmark=hle\n</code></pre> <p>For parallel and multi-run evaluations, and to gain better control over environment settings using Hydra, we recommend using the provided script:</p> <pre><code>cd MiroFlow/apps/run-agent\nbash ./scripts/main-worker-dual/run_evaluate_multiple_runs_gaia-validation.sh\nbash ./scripts/main-worker-dual/run_evaluate_multiple_runs_browsecomp.sh\nbash ./scripts/main-worker-dual/run_evaluate_multiple_runs_browsecomp-zh.sh\nbash ./scripts/main-worker-dual/run_evaluate_multiple_runs_hle.sh\n</code></pre> <p>You can easily modify and customize these scripts to suit your needs. See Customized Configuration for more details.</p>"},{"location":"quickstart/","title":"\ud83d\ude80 Get Started in Under 5 Minutes","text":"<p>Clone the repository, configure your API key, and run your first intelligent agent. You'll just need one <code>OPENROUTER_API_KEY</code>.</p>"},{"location":"quickstart/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Python: 3.12 or higher</li> <li>Package Manager: <code>uv</code>, https://docs.astral.sh/uv/</li> <li>Operating System: Linux, macOS</li> </ul>"},{"location":"quickstart/#quick-setup","title":"\u26a1 Quick Setup","text":""},{"location":"quickstart/#example-1-intelligent-document-analysis-with-file-processing-capabilities","title":"Example 1: Intelligent document analysis with file processing capabilities","text":"<pre><code># 1. Clone and setup\ngit clone https://github.com/MiroMindAI/MiroFlow &amp;&amp; cd MiroFlow\nuv sync\n\n# 2. Configure API key\ncp .env.template .env\n# Edit .env and add your OPENROUTER_API_KEY\n\n# 3. Run your first agent\nuv run main.py trace --config_file_name=agent_quickstart_1 --task=\"What is the first country listed in the XLSX file that have names starting with Co?\" --task_file_name=\"data/FSI-2023-DOWNLOAD.xlsx\"\n</code></pre> <p>\ud83c\udf89 Expected Output: Your agent should return \\boxed{Congo Democratic Republic} \ud83d\ude0a</p> <p>\ud83d\udca1 Tip: If you encounter issues, check that your API key is correctly set in the <code>.env</code> file and that all dependencies are installed.</p> <p>Coming Soon: We will add a video demo for this example</p>"},{"location":"quickstart/#example-2-web-research-and-multi-agent-orchestration","title":"Example 2: Web research and multi-agent orchestration","text":"<p>The example is not complete yet, to be completed</p> <pre><code>uv run main.py trace --config_file_name=agent_quickstart_2 --task=\"What is the Nasdaq Composite Index at today?\"\n</code></pre> <p>Coming Soon: Web research and multi-agent orchestration example</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_overview/","title":"\ud83d\udd27 Tools","text":""},{"location":"tool_overview/#-coming-soon-","title":"- Coming Soon -","text":"<p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_reasoning/","title":"- Coming Soon -","text":"<p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_vqa/","title":"- Coming Soon -","text":"<p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"yaml_config/","title":"\ud83d\udccb YAML Configuration Guide","text":"<p>MiroFlow uses a flexible Hydra-based configuration system that allows you to customize every aspect of your AI agents. This guide explains all configuration features and how to use them effectively.</p>"},{"location":"yaml_config/#configuration-structure","title":"\ud83c\udfd7\ufe0f Configuration Structure","text":""},{"location":"yaml_config/#current-file-organization","title":"Current File Organization","text":"<pre><code>config/\n\u251c\u2500\u2500 agent_quickstart_1.yaml       # Quick start configuration\n\u251c\u2500\u2500 agent_gaia-validation.yaml    # GAIA benchmark configuration  \n\u251c\u2500\u2500 agent_mirothinker.yaml        # MiroThinker model configuration\n\u251c\u2500\u2500 agent_prompts/                # Agent prompt classes\n\u2502   \u251c\u2500\u2500 main_agent_prompt_gaia.py # GAIA-specific prompts\n\u2502   \u251c\u2500\u2500 main_boxed_answer.py      # Boxed answer extraction\n\u2502   \u2514\u2500\u2500 sub_worker.py            # Sub-agent prompts\n\u251c\u2500\u2500 benchmark/                    # Benchmark configurations\n\u2502   \u251c\u2500\u2500 default.yaml              # Default benchmark settings\n\u2502   \u2514\u2500\u2500 gaia-validation.yaml      # GAIA validation benchmark\n\u251c\u2500\u2500 tool/                         # Tool configurations\n\u2502   \u251c\u2500\u2500 tool-reasoning.yaml       # Enhanced reasoning tool\n\u2502   \u251c\u2500\u2500 tool-searching.yaml       # Web search capabilities\n\u2502   \u251c\u2500\u2500 tool-reading.yaml         # Document processing\n\u2502   \u251c\u2500\u2500 tool-code.yaml           # Code execution\n\u2502   \u251c\u2500\u2500 tool-image-video.yaml    # Visual content analysis\n\u2502   \u2514\u2500\u2500 tool-audio.yaml          # Audio processing\n\u2514\u2500\u2500 no-in-use-*/                  # Archive of legacy configurations\n</code></pre>"},{"location":"yaml_config/#quick-start-usage","title":"\ud83d\ude80 Quick Start Usage","text":""},{"location":"yaml_config/#basic-usage","title":"Basic Usage","text":"<pre><code># Run with a specific agent configuration\nuv run main.py trace --config_file_name=agent_quickstart_1 --task=\"Your task here\"\n\n# Run with file input\nuv run main.py trace --config_file_name=agent_quickstart_1 --task=\"Analyze this file\" --task_file_name=\"data/file.xlsx\"\n</code></pre>"},{"location":"yaml_config/#parameter-override","title":"Parameter Override","text":"<pre><code># Override specific parameters on the fly\nuv run main.py trace --config_file_name=agent_gaia-validation \\\n    main_agent.llm.temperature=0.1 \\\n    main_agent.max_turns=30 \\\n    --task=\"Your task\"\n</code></pre>"},{"location":"yaml_config/#configuration-features","title":"\u2699\ufe0f Configuration Features","text":""},{"location":"yaml_config/#agent-configuration","title":"\ud83e\udd16 Agent Configuration","text":""},{"location":"yaml_config/#main-agent-settings","title":"Main Agent Settings","text":"<ul> <li><code>prompt_class</code>: Defines the prompt template and behavior</li> <li><code>MainAgentPromptBoxedAnswer</code>: Basic boxed answer extraction</li> <li><code>MainAgentPrompt_GAIA</code>: GAIA benchmark optimized prompts</li> <li><code>llm</code>: Language model configuration (inline or reference)</li> <li><code>tool_config</code>: List of tools available to the main agent</li> <li><code>max_turns</code>: Maximum conversation turns (-1 = unlimited)</li> <li><code>max_tool_calls_per_turn</code>: Limit tool calls per turn (default: 10)</li> </ul>"},{"location":"yaml_config/#sub-agent-configuration","title":"Sub-Agent Configuration","text":"<ul> <li><code>agent-worker</code>: General-purpose sub-agent with comprehensive tools</li> <li>Individual LLM settings: Each sub-agent can use different models</li> <li>Specialized tool sets: Customize tools per sub-agent role</li> </ul>"},{"location":"yaml_config/#advanced-features","title":"Advanced Features","text":"<ul> <li><code>add_message_id</code>: Add unique IDs to messages for tracking</li> <li><code>chinese_context</code>: Enable Chinese language optimization</li> <li><code>keep_tool_result</code>: Control tool result retention (-1 = keep all)</li> </ul>"},{"location":"yaml_config/#llm-configuration","title":"\ud83d\udd27 LLM Configuration","text":""},{"location":"yaml_config/#provider-support","title":"Provider Support","text":"<pre><code>llm:\n  provider_class: \"ClaudeOpenRouterClient\"  # or \"MiroThinkerSGLangClient\"\n  model_name: \"anthropic/claude-3.7-sonnet\"\n  temperature: 0.3\n  max_tokens: 32000\n  async_client: true\n</code></pre>"},{"location":"yaml_config/#available-providers","title":"Available Providers","text":"<ul> <li>Claude (OpenRouter): <code>ClaudeOpenRouterClient</code></li> <li>MiroThinker: <code>MiroThinkerSGLangClient</code></li> <li>OpenAI: <code>GPTOpenAIClient</code></li> <li>DeepSeek: <code>DeepSeekNewAPIClient</code></li> </ul>"},{"location":"yaml_config/#model-parameters","title":"Model Parameters","text":"<ul> <li><code>temperature</code>: Creativity/randomness (0.0-1.0)</li> <li><code>top_p</code>: Nucleus sampling parameter</li> <li><code>max_tokens</code>: Maximum response length</li> <li><code>top_k</code>: Top-k sampling (-1 = disabled)</li> </ul>"},{"location":"yaml_config/#tool-configuration","title":"\ud83d\udee0\ufe0f Tool Configuration","text":""},{"location":"yaml_config/#available-tools","title":"Available Tools","text":"<ul> <li><code>tool-reasoning</code>: Enhanced reasoning with high-quality models</li> <li><code>tool-searching</code>: Web search with Google/Serper integration</li> <li><code>tool-reading</code>: Document processing (PDF, DOCX, TXT, etc.)</li> <li><code>tool-code</code>: Python code execution in E2B sandbox</li> <li><code>tool-image-video</code>: Visual content analysis</li> <li><code>tool-audio</code>: Audio transcription and processing</li> </ul>"},{"location":"yaml_config/#tool-environment-variables","title":"Tool Environment Variables","text":"<pre><code># tool-searching.yaml\nenv:\n  SERPER_API_KEY: \"${oc.env:SERPER_API_KEY}\"\n  JINA_API_KEY: \"${oc.env:JINA_API_KEY}\"\n  REMOVE_SNIPPETS: \"${oc.env:REMOVE_SNIPPETS,false}\"\n</code></pre>"},{"location":"yaml_config/#processing-features","title":"\ud83c\udfaf Processing Features","text":""},{"location":"yaml_config/#input-processing","title":"Input Processing","text":"<ul> <li><code>o3_hint</code>: Use O3 model for task hint generation</li> <li>Advanced prompt engineering for better task understanding</li> </ul>"},{"location":"yaml_config/#output-processing","title":"Output Processing","text":"<ul> <li><code>o3_final_answer</code>: Use O3 model for final answer extraction</li> <li>Boxed answer format for benchmark compliance</li> <li>Intelligent result synthesis</li> </ul>"},{"location":"yaml_config/#creating-custom-configurations","title":"\ud83d\udd27 Creating Custom Configurations","text":""},{"location":"yaml_config/#1-basic-custom-agent","title":"1. Basic Custom Agent","text":"<pre><code>defaults:\n  - benchmark: gaia-validation\n  - override hydra/job_logging: none\n  - _self_\n\nmain_agent:\n  prompt_class: MainAgentPromptBoxedAnswer\n  llm:\n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.5  # Custom temperature\n  tool_config:\n    - tool-reasoning  # Add reasoning capability\n  max_turns: 15      # Custom turn limit\n\nsub_agents:\n  agent-worker:\n    prompt_class: SubAgentWorkerPrompt\n    tool_config:\n      - tool-reading   # Document processing only\n    max_turns: 10\n</code></pre>"},{"location":"yaml_config/#2-multi-tool-configuration","title":"2. Multi-Tool Configuration","text":"<pre><code>main_agent:\n  tool_config:\n    - tool-reasoning    # Enhanced reasoning\n    - tool-searching    # Web search\n\nsub_agents:\n  agent-worker:\n    tool_config:\n      - tool-reading    # Document processing\n      - tool-code       # Code execution\n      - tool-image-video # Visual analysis\n      - tool-audio      # Audio processing\n</code></pre>"},{"location":"yaml_config/#3-environment-specific-settings","title":"3. Environment-Specific Settings","text":"<pre><code># Development configuration\nmain_agent:\n  llm:\n    temperature: 0.7     # Higher creativity for exploration\n  max_turns: -1          # Unlimited turns\n  add_message_id: true   # Enable debugging\n\n# Production configuration  \nmain_agent:\n  llm:\n    temperature: 0.3     # Lower temperature for consistency\n  max_turns: 20          # Controlled turn limit\n  add_message_id: false  # Disable for performance\n</code></pre>"},{"location":"yaml_config/#benchmark-integration","title":"\ud83d\udcca Benchmark Integration","text":""},{"location":"yaml_config/#gaia-validation-example","title":"GAIA Validation Example","text":"<pre><code>defaults:\n  - benchmark: gaia-validation\n\nmain_agent:\n  prompt_class: MainAgentPrompt_GAIA\n  input_process:\n    o3_hint: true          # Use O3 hints for better performance\n  output_process:\n    o3_final_answer: true  # Extract answers with O3\n</code></pre>"},{"location":"yaml_config/#custom-benchmark","title":"Custom Benchmark","text":"<pre><code># config/benchmark/my-benchmark.yaml\nname: \"my-benchmark\"\ndata:\n  data_dir: \"${oc.env:DATA_DIR,data}/my-data\"\nexecution:\n  max_tasks: 100         # Limit task count\n  max_concurrent: 5      # Parallel execution\n  pass_at_k: 1          # Success criteria\n</code></pre>"},{"location":"yaml_config/#script-based-execution","title":"\ud83d\udcdc Script-Based Execution","text":"<p>We recommend using the scripts provided under <code>./scripts/</code>, as script files are much easier to read, maintain, and customize compared to single-line commands.</p>"},{"location":"yaml_config/#benchmark-evaluation-script","title":"Benchmark Evaluation Script","text":"<pre><code>#!/bin/bash\n\n# Configuration\nNUM_RUNS=3                              # Number of parallel runs\nMAX_CONCURRENT=10                       # Concurrent tasks per run\nBENCHMARK_NAME=\"gaia-validation\"        # Benchmark configuration\nAGENT_CONFIG=\"agent_gaia-validation\"    # Agent configuration\nADD_MESSAGE_ID=\"true\"                   # Enable message tracking\nMAX_TURNS=-1                           # Unlimited turns (-1)\n\n# Auto-detect Chinese benchmarks\nif [[ $BENCHMARK_NAME == \"xbench-ds\" ]] || [[ $BENCHMARK_NAME == \"browsecomp-zh\" ]]; then\n    export CHINESE_CONTEXT=\"true\"\n    echo \"Detected Chinese benchmark, enabling Chinese context\"\nfi\n\n# Google search filtering (optional)\n# export REMOVE_SNIPPETS=\"true\"\n# export REMOVE_KNOWLEDGE_GRAPH=\"true\" \n# export REMOVE_ANSWER_BOX=\"true\"\n\nRESULTS_DIR=\"logs/${BENCHMARK_NAME}/${AGENT_CONFIG}\"\necho \"Starting evaluation with $NUM_RUNS parallel runs...\"\n\n# Launch parallel experiments\nfor i in $(seq 1 $NUM_RUNS); do\n    RUN_ID=\"run_$i\"\n    (\n        uv run main.py trace \\\n            --config_file_name=$AGENT_CONFIG \\\n            --benchmark_name=$BENCHMARK_NAME \\\n            --max_concurrent=$MAX_CONCURRENT \\\n            --output_dir=\"$RESULTS_DIR/$RUN_ID\" \\\n            &gt; \"$RESULTS_DIR/${RUN_ID}_output.log\" 2&gt;&amp;1\n    ) &amp;\n    sleep 2\ndone\n\nwait\necho \"All runs completed! Check results in: $RESULTS_DIR\"\n</code></pre>"},{"location":"yaml_config/#advanced-configuration-tips","title":"\ud83d\udd27 Advanced Configuration Tips","text":""},{"location":"yaml_config/#environment-variable-management","title":"Environment Variable Management","text":"<pre><code># .env file example\nOPENROUTER_API_KEY=your_key_here\nSERPER_API_KEY=your_serper_key\nJINA_API_KEY=your_jina_key\nE2B_API_KEY=your_e2b_key\n\n# Optional: Customize search behavior\nREMOVE_SNIPPETS=false\nREMOVE_KNOWLEDGE_GRAPH=false\nREMOVE_ANSWER_BOX=false\nCHINESE_CONTEXT=false\n</code></pre>"},{"location":"yaml_config/#configuration-validation","title":"Configuration Validation","text":"<pre><code># Add validation to your configs\nmain_agent:\n  llm:\n    temperature: 0.3        # Must be 0.0-1.0\n    max_tokens: 32000       # Model-specific limits\n  max_turns: 20             # Positive integer or -1\n  tool_config:              # Must be valid tool names\n    - tool-reasoning\n</code></pre>"},{"location":"yaml_config/#performance-optimization","title":"Performance Optimization","text":"<pre><code># High-performance configuration\nmain_agent:\n  llm:\n    async_client: true      # Enable async processing\n    max_tokens: 8192        # Reasonable token limit\n  max_tool_calls_per_turn: 5  # Limit tool calls for speed\n\nbenchmark:\n  execution:\n    max_concurrent: 15      # Parallel execution\n    pass_at_k: 1           # Single attempt per task\n</code></pre>"},{"location":"yaml_config/#debugging-configuration","title":"Debugging Configuration","text":"<pre><code># Debug-friendly settings\nmain_agent:\n  add_message_id: true      # Track message flow\n  keep_tool_result: -1      # Keep all tool results\n  max_turns: -1             # Unlimited exploration\n\nsub_agents:\n  agent-worker:\n    max_turns: -1           # Unlimited sub-agent turns\n</code></pre>"},{"location":"yaml_config/#configuration-best-practices","title":"\ud83c\udfaf Configuration Best Practices","text":""},{"location":"yaml_config/#1-start-simple","title":"1. Start Simple","text":"<p>Begin with basic configurations like <code>agent_quickstart_1.yaml</code> and add complexity gradually.</p>"},{"location":"yaml_config/#2-environment-separation","title":"2. Environment Separation","text":"<ul> <li>Development: Higher temperature, unlimited turns, debugging enabled</li> <li>Testing: Moderate settings with turn limits</li> <li>Production: Conservative settings, optimized for performance</li> </ul>"},{"location":"yaml_config/#3-tool-selection","title":"3. Tool Selection","text":"<p>Choose tools based on your use case: - Research Tasks: <code>tool-searching</code> + <code>tool-reasoning</code> - Document Analysis: <code>tool-reading</code> + <code>tool-reasoning</code> - Code Tasks: <code>tool-code</code> + <code>tool-reasoning</code> - Multimedia: <code>tool-image-video</code> + <code>tool-audio</code></p>"},{"location":"yaml_config/#4-resource-management","title":"4. Resource Management","text":"<ul> <li>Monitor <code>max_concurrent</code> to avoid API rate limits</li> <li>Set reasonable <code>max_tokens</code> for cost control</li> <li>Use <code>max_turns</code> to prevent infinite loops</li> </ul>"},{"location":"yaml_config/#5-api-key-security","title":"5. API Key Security","text":"<ul> <li>Always use environment variables for API keys</li> <li>Never commit API keys to version control</li> <li>Use different keys for development and production</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"}]}