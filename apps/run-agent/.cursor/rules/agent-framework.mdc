---
alwaysApply: true
---
# Agent Framework Development Guide

## Main Agent Application
The primary agent framework is located in `apps/reorg-modular-structure/` - this is the core application for building and testing intelligent agents.

## Key Components
- **Entry Points**: 
  - [main.py](mdc:apps/reorg-modular-structure/main.py) - Main application entry point
  - [main_gaia_example.py](mdc:apps/reorg-modular-structure/main_gaia_example.py) - GAIA benchmark example
- **Configuration**: `config/` directory contains YAML configs for agents, benchmarks, and LLM providers
- **Source Code**: `src/mirage_agent/` contains the core agent implementation
- **Scripts**: `scripts/` contains benchmark execution scripts

## Configuration Structure
- `config/agent/` - Agent configurations (default.yaml, for_debug.yaml, etc.)
- `config/benchmark/` - Benchmark configurations (gaia-validation.yaml, bbeh.yaml, etc.)
- `config/llm/` - LLM provider configurations (claude.yaml, openai.yaml, etc.)
- `config/pricing/` - Pricing configurations

## Development Workflow
1. **Setup**: `cd apps/reorg-modular-structure && uv sync`
2. **Test single case**: Modify task_description and task_file_name in main.py, then `uv run main.py`
3. **Run benchmarks**: Use scripts in `scripts/single-run-examples/`
4. **Debug logs**: Check `logs/debug_logs/` for output

## Testing Bad Cases
- **GAIA bad case 74**: `bash scripts/single-run-examples/run_benchmark_gaia-validation-wrong-0707-74.sh`
- **Custom debugging**: Use `for_debug.yaml` configuration for detailed debugging

## Agent Architecture
- **Orchestrator**: Core agent logic in `src/mirage_agent/core/orchestrator.py`
- **LLM Integration**: Multiple provider support in `src/mirage_agent/llm/providers/`
- **Tool Management**: Tool integration via `mirage.contrib.tools.managers.ToolManager`
- **Logging**: Comprehensive logging and tracing in `src/mirage_agent/logging/`

## Benchmark Integration
- **Available Benchmarks**: GAIA, BBEH, BrowseComp, Frames, HLE, MMLU-Pro, SimpleQA, SuperGPQA, WebWalkerQA
- **Running Benchmarks**: Use scripts in `scripts/qwen3-in-house/` for multiple runs
- **Evaluation**: Results and metrics are automatically collected and processed

