# =============================================================================
# MiroFlow Environment Template
# Copy this file to `.env` and fill in the REQUIRED keys before running agents.
# All other keys are OPTIONAL and are pre-filled with `dummy` so the system
# can start even if you don't use those features.
# But DO NOT delete those keys, as agents won;t run without the full key settings.
#
# Conventions:
# - REQUIRED  → you must replace the value with a real key or URL.
# - OPTIONAL  → you may leave as `dummy` to disable the related tool.
# =============================================================================

# -----------------------------------------------------------------------------
# 1. Core LLM Gateway (REQUIRED for almost all agents)
#    Recommended: use an OpenRouter API key here.
#    How to get an OpenRouter API key: https://openrouter.ai/docs
# -----------------------------------------------------------------------------
OPENROUTER_API_KEY=your_api_key
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL_NAME=anthropic/claude-3.7-sonnet
# For LLM Gateway:
# LLM_GATEWAY_URL=https://api.miromind.site/v1
# LLM_GATEWAY_API_KEY=your-api-key
# OPENROUTER_BASE_URL=${LLM_GATEWAY_URL}
# OPENROUTER_API_KEY=${LLM_GATEWAY_API_KEY}


# -----------------------------------------------------------------------------
# 2. Web Search & Web Scraping (OPTIONAL, but required for GAIA / web benchmarks)
#    SERPER (Google-style search)
#      How to get a Serper API key: https://serper.dev/
#      You need to install node.js and install the Serper node package
#    Jina AI (reader/scraper)
#      How to get a Jina AI key: https://jina.ai/reader
# -----------------------------------------------------------------------------
SERPER_API_KEY=dummy
SERPER_BASE_URL=https://google.serper.dev
# For Miro API:
# MIRO_API_URL=https://miro-api.miromind.site
# MIRO_API_KEY=your-api-key
# SERPER_API_KEY=${MIRO_API_KEY}
# SERPER_BASE_URL=${MIRO_API_URL}/serper

JINA_API_KEY=dummy
JINA_BASE_URL=https://r.jina.ai


# -----------------------------------------------------------------------------
# 3. Direct Provider API Keys (OPTIONAL)
#    Only needed if you call providers directly instead of going through your
#    unified LLM gateway (e.g., some vision / reasoning / query rewrite tools).
# -----------------------------------------------------------------------------

# Docs & API key: https://docs.anthropic.com/en/api/getting-started
ANTHROPIC_API_KEY=dummy
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_MODEL_NAME=claude-3-7-sonnet-20250219

# Docs & API key: https://platform.openai.com/api-keys
OPENAI_API_KEY=dummy
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL_NAME=gpt-5.1-2025-11-13
# For Miro API:
# MIRO_API_URL=https://miro-api.miromind.site
# MIRO_API_KEY=your-api-key
# OPENAI_API_KEY=${LLM_GATEWAY_API_KEY}
# OPENAI_BASE_URL=${LLM_GATEWAY_URL}

# Docs & API key: https://ai.google.dev/gemini-api/docs/api-key
GEMINI_API_KEY=dummy
GEMINI_MODEL_NAME=gemini-2.5-pro


# -----------------------------------------------------------------------------
# 4. MiroThinker Local Deployment (OPTIONAL)
#    Only needed if you run a local MiroThinker SGLang server.
#    Docs: https://github.com/MiroMindAI/mirothinker
# -----------------------------------------------------------------------------
OAI_MIROTHINKER_API_KEY=dummy_key
OAI_MIROTHINKER_BASE_URL=http://localhost:61005/v1


# -----------------------------------------------------------------------------
# 5. Tool Servers: Code, Reasoning, Vision, Audio (OPTIONAL)
# -----------------------------------------------------------------------------
# E2B code execution sandbox - required only for Python tool (`tool-code.yaml`).
# Docs & API key: https://e2b.dev/docs/getting-started/api-key
E2B_API_KEY=dummy


# Open-source reasoning server (tool-reasoning-os).
# Use this if you deploy your own reasoning LLM behind a /v1/chat/completions API.
# You will typically get the key & URL from your own deployment or gateway.
REASONING_API_KEY=dummy
REASONING_BASE_URL=https://your_reasoning_base_url/v1/chat/completions
REASONING_MODEL_NAME=your-reasoning-model-name

# Open-source vision server (tool-image-video-os).
# See docs/mkdocs/docs/tool_vqa_os.md for deployment examples.
VISION_API_KEY=dummy
VISION_BASE_URL=https://your_vision_base_url/v1/chat/completions
VISION_MODEL_NAME=Qwen/Qwen2.5-VL-72B-Instruct
GEMINI_API_KEY=dummy
GEMINI_BASE_URL=dummy
GEMINI_MODEL_NAME=gemini-2.5-pro
ENABLE_CLAUDE_VISION=false
ENABLE_OPENAI_VISION=false

# Open-source Whisper-compatible audio server (tool-audio-os).
# See docs/mkdocs/docs/tool_audio_os.md for deployment examples.
WHISPER_API_KEY=dummy
WHISPER_BASE_URL=https://your_whisper_base_url/v1
WHISPER_MODEL_NAME=openai/whisper-large-v3-turbo
OPENAI_TRANSCRIPTION_MODEL_NAME=gpt-4o-transcribe
OPENAI_AUDIO_MODEL_NAME=gpt-4o-audio-preview


# -----------------------------------------------------------------------------
# 6. Data & Logging Paths (OPTIONAL)
# -----------------------------------------------------------------------------
DATA_DIR=data                     # Optional root directory for benchmark datasets
LOGS_DIR=logs                     # Optional directory for logs written by Python tool
TASK_ID=0                         # Optional: used by logger; usually set automatically
DEFAULT_TEMPLATE_ID=all_pip_apt_pkg  # Optional: E2B sandbox template id
DEFAULT_TIMEOUT=1800              # Optional: max code-execution time (seconds) for Python tool


# -----------------------------------------------------------------------------
# 7. Advanced Feature Toggles (OPTIONAL)
# -----------------------------------------------------------------------------
ENABLE_CLAUDE_VISION=false   # If true, vision_mcp_server will use Anthropic models when ANTHROPIC_API_KEY is set
ENABLE_OPENAI_VISION=false   # If true, vision_mcp_server will use OpenAI vision when OPENAI_API_KEY is set
REMOVE_SNIPPETS=false        # If true, SERPER search results will omit snippet text
REMOVE_KNOWLEDGE_GRAPH=false # If true, SERPER search results will omit knowledge graph
REMOVE_ANSWER_BOX=false      # If true, SERPER search results will omit instant answer box
# To enable debug mode:
# DEBUG=true
# LOGGER_LEVEL=DEBUG

# -----------------------------------------------------------------------------
# 8. Advanced: Separate LLMs for Hinting & Final Answer (OPTIONAL)
#    Used only by some GAIA / MiroThinker configs.
# -----------------------------------------------------------------------------
HINT_LLM_BASE_URL=https://api.openai.com/v1          # Or http://localhost:61005/v1 for local MiroThinker
FINAL_ANSWER_LLM_BASE_URL=https://api.openai.com/v1

# Optional: specialized OpenAI models for evaluation/judging (used by utils)
# Default reasoning model for judges like xBench/HLE (o3 by default)
OPENAI_REASONING_MODEL_NAME=o3
# Smaller reasoning model variant for some judges (o3 by default)
OPENAI_REASONING_SMALL_MODEL_NAME=o3
# Default evaluation model for simple classification-style judges (gpt-4o-mini by default)
OPENAI_EVAL_MODEL_NAME=gpt-4o-mini

# -----------------------------------------------------------------------------
# 9. Quickstart: Self-hosted / Custom LLM Gateway (OPTIONAL)
# -----------------------------------------------------------------------------
# Example: you have your own LLM gateway at https://my.gateway.com that exposes
# an OpenAI/OpenRouter-compatible /v1/chat/completions endpoint with a model
# like my-org/my-best-model.
#
# Minimal REQUIRED settings to use this gateway as your main LLM:
#   OPENROUTER_API_KEY=your_gateway_key_here
#   OPENROUTER_BASE_URL=https://my.gateway.com/v1
#   OPENROUTER_MODEL_NAME=my-org/my-best-model
#
# (If you prefer the LLM_GATEWAY_* naming in the config files, you can also set:
#   LLM_GATEWAY_API_KEY=${OPENROUTER_API_KEY}
#   LLM_GATEWAY_URL=${OPENROUTER_BASE_URL}
#   LLM_GATEWAY_MODEL=${OPENROUTER_MODEL_NAME}
# )
#
# Models used by summarization / judges (GAIA, HLE, xBench, etc.):
#   # Main reasoning model for summary_utils and reasoning_mcp_server
#   OPENAI_MODEL_NAME=${OPENROUTER_MODEL_NAME}
#   # Reasoning models for eval_utils judges
#   OPENAI_REASONING_MODEL_NAME=${OPENROUTER_MODEL_NAME}
#   OPENAI_REASONING_SMALL_MODEL_NAME=${OPENROUTER_MODEL_NAME}
#   OPENAI_EVAL_MODEL_NAME=${OPENROUTER_MODEL_NAME}
#
# Optional: point tool-specific servers at the same gateway if it supports them.
#   # If your gateway also serves a generic reasoning model via chat.completions
#   REASONING_API_KEY=${OPENROUTER_API_KEY}
#   REASONING_BASE_URL=${OPENROUTER_BASE_URL}/chat/completions
#   REASONING_MODEL_NAME=${OPENROUTER_MODEL_NAME}
#
#   # If your gateway serves a vision-capable model
#   VISION_API_KEY=${OPENROUTER_API_KEY}
#   VISION_BASE_URL=${OPENROUTER_BASE_URL}/chat/completions
#   VISION_MODEL_NAME=my-org/my-vision-model
# 
# How to test each component separately (from repo root):
#        uv run python test_openrouter_llm.py (for Mac)
#        python test_openrouter_llm.py (for Windows)
