{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MiroFlow","text":""},{"location":"#what-is-miroflow","title":"\ud83d\ude80 What is MiroFlow?","text":"<p>MiroFlow is a comprehensive agentic foundation platform for building intelligent AI agents that achieve state-of-the-art performance on complex tasks. It provides enhanced conversation management, flexible tool integration, and extensive benchmark evaluations across multiple datasets.</p>"},{"location":"#quick-start","title":"\ud83c\udfaf Quick Start","text":"<p>Ready to get started? Choose your path:</p> <p>Get Started in Minutes</p> Quick SetupEvaluationDevelopment <p>Jump right in with our quickstart guide:</p> <p>Get Started </p> <p>Evaluate existing models on benchmarks:</p> <p>Run Evaluations </p> <p>Build your own AI agents:</p> <p>Core Concepts </p>"},{"location":"#ecosystem","title":"\ud83d\udd17 Ecosystem","text":"<p>Explore the complete MiroMind AI ecosystem:</p> <p>MiroMind AI Products</p> Name Description Link MiroFlow Core framework for AI agent platform Documentation MiroThinker State-of-the-art agent foundation models Hugging Face MiroVerse Curated datasets for model training Dataset MiroTrain Complete training recipes and tools GitHub <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI \u00b7 Version: v0.3</p>"},{"location":"all_about_agents/","title":"\ud83d\udcda All About Agents","text":"<p>Welcome to our comprehensive resource collection for AI agents. This page curates valuable tools, frameworks, research papers, and learning materials to help you understand and build sophisticated agent systems.</p>"},{"location":"all_about_agents/#table-of-contents","title":"Table of Contents","text":"<p>Resource Categories</p> <ol> <li>Agent Frameworks</li> <li>Agent Memory</li> <li>Papers</li> <li>Evaluation</li> </ol>"},{"location":"all_about_agents/#agent-frameworks","title":"Agent Frameworks","text":"<p>Popular Agent Development Frameworks</p> <p>Comprehensive frameworks for building and deploying AI agents across different domains.</p> <ul> <li> <p>MiroFlow: Build, manage, and scale your AI agents with ease</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>OpenManus: No fortress, purely open ground. OpenManus is Coming</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>OpenBB Platform: Financial data platform for analysts, quants and AI agents </p> <ul> <li> Project</li> </ul> </li> </ul>"},{"location":"all_about_agents/#agent-memory","title":"Agent Memory","text":"<p>Memory Systems for Persistent Agent Intelligence</p> <p>Advanced memory solutions for building agents with long-term context and learning capabilities.</p> <ul> <li> <p>Mem0: Building Production- Ready AI Agents with Scalable Long-Term Memory</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>memobase: Profile-Based Long-Term Memory for AI Applications</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>Memento: Fine-tuning LLM Agents without Fine-tuning LLMs</p> <ul> <li> Paper \u00b7  GitHub</li> </ul> </li> </ul>"},{"location":"all_about_agents/#papers","title":"Papers","text":"<p>Research Papers &amp; Publications</p> <p>Latest research in agent systems, methodologies, and theoretical foundations.</p> <ul> <li> <p>Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld </p> <ul> <li> Paper</li> </ul> </li> <li> <p>AFlow: Automating Agentic Workflow Generation </p> <ul> <li> Paper</li> </ul> </li> <li> <p>AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs </p> <ul> <li> Paper</li> </ul> </li> <li> <p>Throttling Web Agents Using Reasoning Gates</p> <ul> <li> Paper</li> </ul> </li> <li> <p>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</p> <ul> <li> Paper</li> </ul> </li> </ul>"},{"location":"all_about_agents/#evaluation","title":"Evaluation","text":"<p>Benchmarks &amp; Evaluation Frameworks</p> <p>Comprehensive evaluation tools and benchmarks for measuring agent performance across various tasks.</p> <ul> <li> <p>LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries </p> <ul> <li> Paper</li> </ul> </li> <li> <p>BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent </p> <ul> <li> Paper</li> </ul> </li> <li> <p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</p> <ul> <li> Paper</li> </ul> </li> <li> <p>GAIA: a benchmark for General AI Assistants </p> <ul> <li> Paper \u00b7  Leaderboard</li> </ul> </li> <li> <p>xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations </p> <ul> <li> Paper</li> </ul> </li> <li> <p>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers </p> <ul> <li> Paper</li> </ul> </li> <li> <p>FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction </p> <ul> <li> Paper</li> </ul> </li> <li> <p>Terminal-Bench: the benchmark for testing AI agents in real terminal environments </p> <ul> <li> GitHub</li> </ul> </li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"applications/","title":"\ud83d\udcf1 Applications","text":"<p>MiroFlow enables building a wide variety of intelligent applications across different domains and use cases.</p>"},{"location":"applications/#available-applications","title":"\ud83c\udfaf Available Applications","text":"<p>Ready-to-Use Applications</p> <p>Experience MiroFlow's capabilities through these available interfaces and demos.</p>"},{"location":"applications/#gradio-demo","title":"Gradio Demo","text":"<p>Local Development Interface</p> <p>Interactive web interface for testing MiroFlow agents locally. Currently available at MiroThinker Gradio Demo.</p>"},{"location":"applications/#live-demo","title":"Live Demo","text":"<p>Online Experience</p> <p>Experience MiroFlow's capabilities through our online demo for deep research tasks.</p>"},{"location":"applications/#development-status","title":"\ud83d\udd04 Development Status","text":"<p>Integration Progress</p> <p>The MiroThinker model workflows are being integrated into the main MiroFlow framework. This will provide a unified experience for all applications and demos.</p> <p>Stay tuned for updates on application availability and new integrations!</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"claude-3.7-sonnet/","title":"Claude 3.7 Sonnet","text":""},{"location":"claude-3.7-sonnet/#what-this-is","title":"What This Is","text":"<p>Anthropic's Claude 3.7 Sonnet model with 200K context, strong reasoning, and tool use capabilities.</p>"},{"location":"claude-3.7-sonnet/#available-clients","title":"Available Clients","text":""},{"location":"claude-3.7-sonnet/#claudeanthropicclient-direct-api","title":"ClaudeAnthropicClient (Direct API)","text":"<p>Environment: <pre><code>export ANTHROPIC_API_KEY=\"your-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.anthropic.com\"  # optional\n</code></pre></p> <p>Config: <pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeAnthropicClient\"\n    model_name: \"claude-3-7-sonnet-20250219\"  # Use actual model name from Anthropic API\n    anthropic_api_key: \"${oc.env:ANTHROPIC_API_KEY,???}\"\n    anthropic_base_url: \"${oc.env:ANTHROPIC_BASE_URL,https://api.anthropic.com}\"\n    ...\n</code></pre></p>"},{"location":"claude-3.7-sonnet/#usage","title":"Usage","text":"<pre><code># Use existing config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_benchmarks/","title":"\ud83e\uddea Adding New Benchmarks to MiroFlow","text":"<p>This guide provides a comprehensive walkthrough for adding new benchmarks to the MiroFlow framework. MiroFlow uses a modular benchmark architecture that allows for easy integration of new evaluation datasets.</p>"},{"location":"contribute_benchmarks/#step-by-step-implementation-guide","title":"\ud83d\ude80 Step-by-Step Implementation Guide","text":""},{"location":"contribute_benchmarks/#step-1-prepare-your-dataset","title":"Step 1: Prepare Your Dataset","text":"<p>Your benchmark dataset should follow this structure:</p> <pre><code>your-benchmark/\n\u251c\u2500\u2500 standardized_data.jsonl    # Metadata file (required)\n\u251c\u2500\u2500 file1.pdf                  # Optional: Binary files referenced by tasks\n\u251c\u2500\u2500 file2.png\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"contribute_benchmarks/#metadata-format-jsonl","title":"Metadata Format (JSONL)","text":"<p>Each line in <code>standardized_data.jsonl</code> should be a JSON object with these fields:</p> <pre><code>{\n  \"task_id\": \"unique_task_identifier\",\n  \"task_question\": \"The question or instruction for the task\",\n  \"ground_truth\": \"The expected answer or solution\",\n  \"file_path\": \"path/to/file.pdf\",  // Optional, can be null\n  \"metadata\": {                     // Optional, can be empty\n    \"difficulty\": \"hard\",\n    \"category\": \"reasoning\",\n    \"source\": \"original_dataset_name\"\n  }\n}\n</code></pre> <p>Example: <pre><code>{\n  \"task_id\": \"math_001\",\n  \"task_question\": \"What is the integral of x^2 from 0 to 2?\",\n  \"ground_truth\": \"8/3\",\n  \"file_path\": null,\n  \"metadata\": {\n    \"difficulty\": \"medium\",\n    \"category\": \"calculus\"\n  }\n}\n</code></pre></p>"},{"location":"contribute_benchmarks/#step-2-create-configuration-file","title":"Step 2: Create Configuration File","text":"<p>Create a new configuration file in <code>config/benchmark/your-benchmark.yaml</code>:</p> <pre><code># config/benchmark/your-benchmark.yaml\ndefaults:\n  - default\n  - _self_\n\nname: \"your-benchmark\"\n\ndata:\n  data_dir: \"${data_dir}/your-benchmark\"  # Path to your dataset\n  metadata_file: \"standardized_data.jsonl\"  # Metadata filename\n  whitelist: []  # Optional: List of specific task_ids to run\n\nexecution:\n  max_tasks: null      # null = no limit, or specify a number\n  max_concurrent: 5    # Number of parallel tasks\n  pass_at_k: 1         # Number of attempts per task\n\nopenai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n</code></pre>"},{"location":"contribute_benchmarks/#step-3-set-up-data-directory","title":"Step 3: Set Up Data Directory","text":"<p>Place your dataset in the appropriate data directory:</p> <pre><code># Create the benchmark data directory\nmkdir -p data/your-benchmark\n\n# Copy your dataset files\ncp your-dataset/* data/your-benchmark/\n</code></pre>"},{"location":"contribute_benchmarks/#step-4-test-your-benchmark","title":"Step 4: Test Your Benchmark","text":"<p>Run your benchmark using the MiroFlow CLI:</p> <pre><code># Test with a small subset \nuv run main.py common-benchmark \\\n  --config_file_name=agent_quickstart_1 \\\n  benchmark=your-benchmark \\\n  benchmark.execution.max_tasks=5 \\\n  output_dir=logs/test-your-benchmark\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_llm_clients/","title":"Contributing New LLM Clients","text":""},{"location":"contribute_llm_clients/#what-this-does","title":"What This Does","text":"<p>Add support for new LLM providers to MiroFlow by creating a provider class.</p>"},{"location":"contribute_llm_clients/#client-structure","title":"Client Structure","text":"<p>Each LLM client inherits from <code>LLMProviderClientBase</code> and implements 4 methods: - <code>_create_client()</code> - Initialize API client - <code>_create_message()</code> - Make API calls - <code>process_llm_response()</code> - Handle responses - <code>extract_tool_calls_info()</code> - Parse tool calls</p>"},{"location":"contribute_llm_clients/#implementation-steps","title":"Implementation Steps","text":""},{"location":"contribute_llm_clients/#1-create-provider-file","title":"1. Create Provider File","text":"<p><code>src/llm/providers/your_provider_client.py</code>:</p> <pre><code>import dataclasses\nfrom src.llm.provider_client_base import LLMProviderClientBase\n\n@dataclasses.dataclass\nclass YourProviderClient(LLMProviderClientBase):\n    def _create_client(self, config):\n        # Initialize your API client\n        pass\n\n    async def _create_message(self, system_prompt, messages, tools_definitions, keep_tool_result=-1):\n        # Make API call\n        pass\n\n    def process_llm_response(self, llm_response, message_history, agent_type=\"main\"):\n        # Extract response text, return (text, should_exit)\n        pass\n\n    def extract_tool_calls_info(self, llm_response, assistant_response_text):\n        # Parse tool calls, return (tool_calls, tool_names)\n        pass\n</code></pre>"},{"location":"contribute_llm_clients/#2-create-config","title":"2. Create Config","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"YourProviderClient\"\n    model_name: \"your-model\"\n    your_api_key: \"${oc.env:YOUR_API_KEY,???}\"\n    your_base_url: \"${oc.env:YOUR_BASE_URL,https://api.yourprovider.com/v1}\"\n</code></pre>"},{"location":"contribute_llm_clients/#3-set-environment","title":"3. Set Environment","text":"<pre><code>export YOUR_API_KEY=\"your-key\"\nexport YOUR_BASE_URL=\"https://api.yourprovider.com/v1\"  # optional if using default\n</code></pre>"},{"location":"contribute_llm_clients/#examples","title":"Examples","text":"<p>See existing providers in <code>src/llm/providers/</code>: - <code>ClaudeAnthropicClient</code> - Direct API - <code>ClaudeOpenRouterClient</code> - Proxy API - <code>GPTOpenAIClient</code> - OpenAI API</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_tools/","title":"Adding New Tools","text":""},{"location":"contribute_tools/#what-this-does","title":"What This Does","text":"<p>Extend the agent\u2019s functionality by introducing a new tool. Each tool is implemented as an MCP server and registered via configuration.</p>"},{"location":"contribute_tools/#implementation-steps","title":"Implementation Steps","text":""},{"location":"contribute_tools/#1-create-mcp-server","title":"1. Create MCP Server","text":"<p>Create a new file <code>src/tool/mcp_servers/new-mcp-server.py</code> that implements the tool\u2019s core logic.  </p> <pre><code>from fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"new-mcp-server\")\n\n@mcp.tool()\nasync def tool_name(param: str) -&gt; str:\n    \"\"\"\n    Explanation of the tool, its parameters, and return value.\n    \"\"\"\n    tool_result = ...  # Your logic here\n    return tool_result\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre> <p>Tool schemas are automatically generated from <code>docstrings</code> and <code>hints</code> via the FastMCP protocol.</p>"},{"location":"contribute_tools/#2-create-tool-config","title":"2. Create Tool Config","text":"<p>Add a new config file at <code>config/tools/new-tool-name.yaml</code>:</p> <pre><code>name: \"new-tool-name\"\ntool_command: \"python\"\nargs:\n  - \"-m\"\n  - \"src.tool.mcp_servers.new-mcp-server\"  # Match the server file created above\n</code></pre>"},{"location":"contribute_tools/#3-register-tool-in-agent-config","title":"3. Register Tool in Agent Config","text":"<p>Enable the new tool inside your agent config (e.g., <code>config/agent-with-new-tool.yaml</code>):</p> <pre><code>main_agent:\n  ...\n  tool_config:\n    - tool-reasoning\n    - new-tool-name   # \ud83d\udc48 Add your new tool here\n  ...\nsub_agents:\n  agent-worker:\n    ...\n    tool_config:\n      - tool-searching\n      - tool-image-video\n      - tool-reading\n      - tool-code\n      - tool-audio\n      - new-tool-name # \ud83d\udc48 Add your new tool here\n    ...\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI  </p>"},{"location":"contributors/","title":"\ud83d\udcdd Contributors","text":"<p>Thank you to all the amazing contributors who have helped make MiroFlow better! \ud83d\ude4f</p>"},{"location":"contributors/#core-team","title":"Core Team","text":"<p>Development Team</p> <p>The MiroFlow framework is developed and maintained by the MiroMind AI team.</p>"},{"location":"contributors/#community-contributors","title":"Community Contributors","text":"<p>Community Appreciation</p> <p>We welcome contributions from the community! Whether you're fixing bugs, adding features, improving documentation, or helping with benchmarks, your contributions are valued.</p> <p></p>"},{"location":"contributors/#how-to-contribute","title":"How to Contribute","text":"<p>Contribution Opportunities</p> <p>There are many ways to contribute to MiroFlow:</p>"},{"location":"contributors/#bug-reports-feature-requests","title":"\ud83d\udc1b Bug Reports &amp; Feature Requests","text":"<p>Issue Reporting</p> <ul> <li>Report bugs or request features via GitHub Issues</li> <li>Use clear, descriptive titles and provide detailed information</li> </ul>"},{"location":"contributors/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<p>Development Workflow</p> <ul> <li>Fork the repository and create a feature branch</li> <li>Follow our coding standards and include tests</li> <li>Submit a pull request with a clear description of your changes</li> </ul>"},{"location":"contributors/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Documentation Help</p> <ul> <li>Help improve our documentation</li> <li>Add examples and tutorials</li> <li>Fix typos and clarify explanations</li> </ul>"},{"location":"contributors/#testing-benchmarks","title":"\ud83e\uddea Testing &amp; Benchmarks","text":"<p>Quality Assurance</p> <ul> <li>Help us test MiroFlow on different platforms</li> <li>Contribute new benchmark datasets</li> <li>Improve existing evaluation scripts</li> </ul>"},{"location":"contributors/#community-support","title":"\ud83d\udcac Community Support","text":"<p>Community Engagement</p> <ul> <li>Answer questions in our Discord community</li> <li>Help other users in GitHub discussions</li> <li>Share your experiences and use cases</li> </ul>"},{"location":"contributors/#recognition","title":"Recognition","text":"<p>Contributor Acknowledgment</p> <p>All contributors are recognized in our:</p> <ul> <li>GitHub contributors graph</li> <li>Release notes for significant contributions</li> <li>Community acknowledgments</li> </ul>"},{"location":"contributors/#getting-started","title":"Getting Started","text":"<p>Quick Start Guide</p> <ol> <li>Check out our GitHub repository</li> <li>Read the contributing guidelines</li> <li>Join our Discord community to connect with other contributors</li> </ol> <p>Thank You</p> <p>Thank you for helping us build the future of AI agents! \ud83d\ude80</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"core_concepts/","title":"\ud83e\udd16 MiroFlow Agentic Foundation Framework","text":"<p>MiroFlow provides a flexible framework for building and deploying intelligent agents capable of complex reasoning and tool use.</p>"},{"location":"core_concepts/#workflow-overview","title":"Workflow Overview","text":"<p>Multi-Stage Agentic Process</p> <p>MiroFlow handles user queries through a multi-stage and agentic process designed for flexibility and depth. The workflow is organized as follows:</p> <ol> <li> <p>Intent Recognition &amp; Query Augmentation    LLMs analyze user input to detect intent and refine the query.</p> </li> <li> <p>Planning &amp; Task Orchestration    The main agent drafts an execution plan, invokes tools, and coordinates sub-agents.</p> </li> <li> <p>Delegation to Sub-Agents    Specialized agents (e.g., agent-browsing) handle complex or domain-specific tasks. Sub-agents independently plan, act, and execute tool calls as needed.</p> </li> <li> <p>Tool Access via MCP Servers    When external capabilities are required, agents leverage specialized tools by connecting to MCP (Model Context Protocol) servers.</p> </li> <li> <p>Result Synthesis &amp; Output Alignment    After task completion, a dedicated summary process synthesizes results, ensuring the output is high-quality and aligned with user instructions (or benchmark formats).</p> </li> </ol>"},{"location":"core_concepts/#architecture-components","title":"Architecture Components","text":"<p>Directory Structure</p> <p>All core components are located in the <code>src/</code> directory.</p> Source Code Structure<pre><code>src/\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 pipeline.py                      # Pipeline: coordinates task execution\n\u2502   \u2514\u2500\u2500 orchestrator.py                  # Orchestrator: manages LLM \u2194 tool flow\n\u2502\n\u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 client.py                        # Unified LLM client interface\n\u2502   \u251c\u2500\u2500 provider_client_base.py          # Base class for LLM providers\n\u2502   \u251c\u2500\u2500 util.py                          # LLM utility functions\n\u2502   \u2514\u2500\u2500 providers/                       # Provider-specific implementations\n\u2502       \u251c\u2500\u2500 claude_anthropic_client.py   # Anthropic Claude client\n\u2502       \u251c\u2500\u2500 claude_newapi_client.py      # Claude via NewAPI\n\u2502       \u251c\u2500\u2500 claude_openrouter_client.py  # Claude via OpenRouter\n\u2502       \u251c\u2500\u2500 deepseek_newapi_client.py    # DeepSeek via NewAPI\n\u2502       \u251c\u2500\u2500 gpt_openai_client.py         # OpenAI GPT client\n\u2502       \u251c\u2500\u2500 gpt_openai_response_client.py # OpenAI response client\n\u2502       \u251c\u2500\u2500 mirothinker_sglang_client.py # MiroThinker via SGLang\n\u2502       \u2514\u2500\u2500 qwen_sglang_client.py        # Qwen via SGLang\n\u2502\n\u251c\u2500\u2500 tool/\n\u2502   \u251c\u2500\u2500 manager.py                       # Tool Manager: MCP server connector\n\u2502   \u2514\u2500\u2500 mcp_servers/                     # Individual MCP tool servers\n\u2502       \u251c\u2500\u2500 python_server.py             # Code execution (E2B integration)\n\u2502       \u251c\u2500\u2500 vision_mcp_server.py         # Visual perception &amp; image analysis\n\u2502       \u251c\u2500\u2500 searching_mcp_server.py      # Web search &amp; retrieval\n\u2502       \u251c\u2500\u2500 audio_mcp_server.py          # Audio transcription &amp; processing\n\u2502       \u251c\u2500\u2500 reasoning_mcp_server.py      # Enhanced reasoning capabilities\n\u2502       \u251c\u2500\u2500 reading_mcp_server.py        # Document processing &amp; file reading\n\u2502       \u251c\u2500\u2500 browser_session.py           # Persistent browser session management\n\u2502       \u2514\u2500\u2500 utils/                       # Tool utilities\n\u2502           \u2514\u2500\u2500 smart_request.py         # Smart request handling\n\u2502\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 io_utils.py                      # Output formatting utilities\n\u2502   \u251c\u2500\u2500 parsing_utils.py                 # Text parsing utilities\n\u2502   \u251c\u2500\u2500 summary_utils.py                 # Summary generation utilities\n\u2502   \u2514\u2500\u2500 tool_utils.py                    # Tool configuration helpers\n\u2502\n\u2514\u2500\u2500 logging/                             # Task logging &amp; metrics\n    \u251c\u2500\u2500 logger.py                        # Main logging interface\n    \u2514\u2500\u2500 task_tracer.py                   # Task execution tracing\n</code></pre>"},{"location":"core_concepts/#core-system","title":"Core System \ud83d\udcbb","text":"<p>Core Components</p> <p>Pipeline (<code>src/core/pipeline.py</code>)</p> <p>Main entry point that coordinates task execution, creates and manages all components, handles error recovery, and returns final results. Initializes LLM clients, tool managers, and orchestrator for complete task processing.</p> <p>Orchestrator (<code>src/core/orchestrator.py</code>)</p> <p>Central coordination hub that manages multi-turn conversations, parses tool calls, executes tools, delegates to sub-agents, and handles the complete agent workflow. Supports features like message ID generation, Chinese context handling, and output formatting.</p> <p>LLM Client (<code>src/llm/client.py</code>)</p> <p>Unified interface supporting multiple language model providers including Anthropic Claude, OpenAI GPT, Google Gemini, Qwen, DeepSeek, and MiroThinker models. Provider-specific implementations in <code>src/llm/providers/</code> handle different API formats and capabilities.</p>"},{"location":"core_concepts/#tool-integration","title":"Tool Integration \ud83d\udd27","text":"<p>Tool System Components</p> <p>Tool Manager (<code>src/tool/manager.py</code>)</p> <p>Comprehensive MCP server connection manager that handles tool discovery, maintains persistent connections, manages tool blacklisting, and provides error handling. Supports both local and remote MCP servers with automatic tool definition retrieval.</p> <p>MCP Servers (<code>src/tool/mcp_servers/</code>)</p> <p>Individual tool implementations built on FastMCP protocol. Provides extensive capabilities including:</p> <ul> <li>Code Execution (<code>python_server.py</code>): E2B-powered Python sandbox for safe code execution with pre-installed packages</li> <li>Visual Perception (<code>vision_mcp_server.py</code>): Image and video analysis capabilities with format detection</li> <li>Web Search (<code>searching_mcp_server.py</code>): Google search integration with content filtering and retrieval</li> <li>Audio Processing (<code>audio_mcp_server.py</code>): Audio transcription and processing capabilities</li> <li>Enhanced Reasoning (<code>reasoning_mcp_server.py</code>): Advanced reasoning tool using high-quality language models</li> <li>Document Processing (<code>reading_mcp_server.py</code>): File reading and document analysis across multiple formats</li> <li>Browser Sessions (<code>browser_session.py</code>): Persistent browser session management for web interaction</li> <li>Smart Utilities (<code>utils/smart_request.py</code>): Intelligent request handling and optimization</li> </ul>"},{"location":"core_concepts/#agent-system","title":"Agent System \ud83d\udc77","text":"<p>Agent Architecture</p> <p>Main Agent</p> <p>The primary agent that receives user tasks and coordinates the overall execution. It can directly use reasoning tools and delegate complex tasks to specialized sub-agents. Main agents support different prompt classes for various benchmarks and use cases.</p> <p>Sub-Agents</p> <p>Specialized agents designed for specific domains and capabilities:</p> <ul> <li><code>agent-worker</code>: General-purpose sub-agent with access to comprehensive tool sets including web search, file processing, code execution, audio/video analysis, and document reading</li> <li>Each sub-agent maintains dedicated tool configurations and custom prompts</li> <li>Sub-agents can operate independently with their own LLM configurations and turn limits</li> <li>Agent definitions and prompts are managed through the configuration system in <code>config/agent_prompts/</code></li> </ul>"},{"location":"core_concepts/#support-systems","title":"Support Systems \u2699\ufe0f","text":"<p>Supporting Infrastructure</p> <p>Configuration System (<code>config/</code>)</p> <p>Hydra-powered YAML configuration for agents, LLMs, and benchmarks</p> <p>Output Formatter (<code>src/utils/io_utils.py</code>)</p> <p>Intelligent response formatting that adapts to various benchmark requirements</p> <p>Parsing Utilities (<code>src/utils/parsing_utils.py</code>)</p> <p>Text parsing and processing utilities</p> <p>Summary Utilities (<code>src/utils/summary_utils.py</code>)</p> <p>Summary generation and processing utilities</p> <p>Task Logger (<code>src/logging/</code>)</p> <p>Comprehensive logging for agent interactions, tool executions, and performance metrics</p>"},{"location":"core_concepts/#configuration","title":"Configuration","text":"<p>Configuration System</p> <p>MiroFlow uses a flat Hydra-based configuration system with agent configurations directly in the <code>config/</code> directory. Each agent configuration combines LLM settings, tool configurations, and agent behavior parameters.</p>"},{"location":"core_concepts/#configuration-structure","title":"Configuration Structure","text":"Configuration Directory<pre><code>config/\n\u251c\u2500\u2500 agent_quickstart_1.yaml       # Quick start agent configuration\n\u251c\u2500\u2500 agent_gaia-validation.yaml    # GAIA validation agent configuration  \n\u251c\u2500\u2500 agent_mirothinker.yaml        # MiroThinker model configuration\n\u251c\u2500\u2500 agent_prompts/                # Agent prompt classes\n\u2502   \u251c\u2500\u2500 base_agent_prompt.py      # Base prompt class\n\u2502   \u251c\u2500\u2500 main_agent_prompt_gaia.py # GAIA-specific prompts\n\u2502   \u251c\u2500\u2500 main_boxed_answer.py      # Boxed answer extraction\n\u2502   \u251c\u2500\u2500 main_gaia.py             # GAIA main agent prompts\n\u2502   \u2514\u2500\u2500 sub_worker.py            # Sub-agent prompts\n\u251c\u2500\u2500 benchmark/                    # Benchmark configurations\n\u2502   \u251c\u2500\u2500 default.yaml              # Default benchmark settings\n\u2502   \u2514\u2500\u2500 gaia-validation.yaml      # GAIA validation benchmark\n\u251c\u2500\u2500 tool/                         # Tool configurations\n\u2502   \u251c\u2500\u2500 tool-code.yaml            # Code execution tool\n\u2502   \u251c\u2500\u2500 tool-searching.yaml       # Web search tool\n\u2502   \u251c\u2500\u2500 tool-reasoning.yaml       # Reasoning tool\n\u2502   \u251c\u2500\u2500 tool-reading.yaml         # Document reading tool\n\u2502   \u251c\u2500\u2500 tool-image-video.yaml     # Image/video processing tool\n\u2502   \u2514\u2500\u2500 tool-audio.yaml          # Audio processing tool\n\u2514\u2500\u2500 no-in-use-*/                  # Archive of legacy configurations\n</code></pre>"},{"location":"core_concepts/#agent-configuration-example","title":"Agent Configuration Example","text":"<p>Basic Agent Configuration - <code>config/agent_quickstart_1.yaml</code></p> agent_quickstart_1.yaml<pre><code>defaults:\n  - benchmark: gaia-validation\n  - override hydra/job_logging: none\n  - _self_\n\nmain_agent:\n  prompt_class: MainAgentPromptBoxedAnswer\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.3\n    max_tokens: 32000\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n    openrouter_base_url: \"${oc.env:OPENROUTER_BASE_URL,https://openrouter.ai/api/v1}\"\n\n  tool_config: []  # Main agent with no tools (basic setup)\n  max_turns: -1\n  max_tool_calls_per_turn: 10\n  add_message_id: true\n  chinese_context: \"${oc.env:CHINESE_CONTEXT,false}\"\n\nsub_agents:\n  agent-worker:\n    prompt_class: SubAgentWorkerPrompt\n    llm: \n      provider_class: \"ClaudeOpenRouterClient\"\n      model_name: \"anthropic/claude-3.7-sonnet\"\n      temperature: 0.3\n      max_tokens: 32000\n      openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n\n    tool_config:\n      - tool-reading  # Document processing capability\n    max_turns: -1\n    max_tool_calls_per_turn: 10\n\noutput_dir: logs/\ndata_dir: \"${oc.env:DATA_DIR,data}\"\n</code></pre>"},{"location":"core_concepts/#advanced-agent-configuration","title":"Advanced Agent Configuration","text":"<p>Advanced Configuration - <code>config/agent_gaia-validation.yaml</code></p> agent_gaia-validation.yaml<pre><code>main_agent:\n  prompt_class: MainAgentPrompt_GAIA\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.3\n    max_tokens: 32000\n\n  tool_config:\n    - tool-reasoning  # Enhanced reasoning capabilities\n\n  input_process:\n    o3_hint: true      # Use O3 hints for better performance\n  output_process:\n    o3_final_answer: true  # Extract final answers using O3\n\nsub_agents:\n  agent-worker:\n    tool_config:\n      - tool-searching     # Web search capabilities\n      - tool-image-video   # Visual content processing\n      - tool-reading       # Document processing\n      - tool-code         # Code execution\n      - tool-audio        # Audio processing\n</code></pre>"},{"location":"core_concepts/#tool-configuration","title":"Tool Configuration","text":"<p>Tool Configuration Example - <code>config/tool/tool-searching.yaml</code></p> tool-searching.yaml<pre><code>name: \"tool-searching\"\ntool_command: \"python\"\nargs:\n  - \"-m\"\n  - \"src.tool.mcp_servers.searching_mcp_server\"\nenv:\n  SERPER_API_KEY: \"${oc.env:SERPER_API_KEY}\"\n  JINA_API_KEY: \"${oc.env:JINA_API_KEY}\"\n  REMOVE_SNIPPETS: \"${oc.env:REMOVE_SNIPPETS,false}\"\n  REMOVE_KNOWLEDGE_GRAPH: \"${oc.env:REMOVE_KNOWLEDGE_GRAPH,false}\"\n  REMOVE_ANSWER_BOX: \"${oc.env:REMOVE_ANSWER_BOX,false}\"\n</code></pre>"},{"location":"core_concepts/#benchmark-configuration","title":"Benchmark Configuration","text":"<p>Benchmark Configuration Example - <code>config/benchmark/gaia-validation.yaml</code></p> gaia-validation.yaml<pre><code>defaults:\n  - default\n  - _self_\n\nname: \"gaia-validation\"\n\ndata:\n  data_dir: \"${oc.env:DATA_DIR,data}/gaia-val\"\n\nexecution:\n  max_tasks: null  # null means no limit\n  max_concurrent: 5\n</code></pre>"},{"location":"core_concepts/#key-features","title":"Key Features","text":"<p>Architecture Highlights</p> <p>Multi-Agent Architecture</p> <ul> <li>Main Agent: Coordinates overall task execution and reasoning</li> <li>Sub-Agents: Specialized agents with dedicated tool sets for specific domains</li> <li>Dynamic Delegation: Intelligent task routing based on capability requirements</li> </ul> <p>Advanced Configuration</p> <ul> <li>Flexible LLM Support: Multiple provider integrations with unified interface</li> <li>Tool Modularity: Mix and match tools based on task requirements  </li> <li>Benchmark Integration: Pre-configured setups for popular AI benchmarks</li> <li>Environment Management: Secure API key and environment variable handling</li> </ul> <p>Production Features</p> <ul> <li>Error Recovery: Robust error handling and graceful degradation</li> <li>Logging &amp; Tracing: Comprehensive task execution monitoring</li> <li>Concurrent Execution: Parallel task processing capabilities</li> <li>Resource Management: Efficient tool connection pooling and cleanup</li> </ul>"},{"location":"core_concepts/#examples","title":"Examples","text":"<p>Check out our example applications to see agents in action.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"data/","title":"\ud83d\udcca Data","text":"<p>The MiroVerse dataset collection provides comprehensive training data for building advanced AI agents with full trajectory coverage.</p>"},{"location":"data/#news-updates","title":"\ud83d\udd25 News &amp; Updates","text":"<p>Latest Releases</p> <ul> <li>The data is released over Huggingface.</li> <li>MiroVerse v0.1 has been released. This dataset can be used with our training framework, MiroTrain. In MiroVerse v0.1, we provide both SFT and DPO data, making it easy to reproduce MiroThinker-v0.1's benchmark performance on Qwen3. Give it a try!</li> </ul>"},{"location":"data/#first-batch-of-miroverse","title":"\ud83d\udd25 First Batch of MiroVerse","text":"<p>What makes this release special</p> <p>\u2728 Special Features:</p> <ul> <li>\ud83d\udcda Diverse Verified Open Source Data \u2014 Carefully curated and validated community datasets</li> <li>\ud83e\udde0 Fresh Large-Scale Deep Research Data \u2014 Generated by our proprietary data engine</li> <li>\ud83d\udd04 Complete Trajectory Coverage \u2014 Every single sample includes full rollout trajectories</li> <li>\u2705 Quality Assurance: \u2014 Each trajectory has been verified, ensuring high-quality training data for your models.</li> <li>\ud83c\udf31 Always Growing, Always Open \u2014 Regular updates, powered by collaboration with the community</li> </ul>"},{"location":"data/#dataset-overview","title":"\ud83d\udce6 Dataset Overview","text":"<p>MiroVerse-v0.1 Statistics</p> <p>MiroVerse-v0.1 is a large-scale agent dataset with 147K+ samples featuring full rollout trajectories across diverse AI agent tasks including multi-hop QA, web navigation, and scientific reasoning. Every single sample includes complete execution traces with 1.9B+ tokens and 602K+ tool interactions, providing comprehensive training data for tool-using and web-browsing AI agents.</p> <p></p> Split #Sample #Main Trace #Browse Trace #Token #Turns #Tools License MiroVerse-Voyager1.0 59097 19115 39982 1129113893 444723 325537 CC-BY-NC-4.0 MiroVerse-MuSiQue 29572 10422 19150 294351053 143080 90486 CC-BY-4.0 MiroVerse-HotpotQA 12942 6553 6389 67352039 46320 20524 CC-BY-SA-4.0 MiroVerse-WebWalkerQA-Silver 10817 4961 5856 107650324 67846 46215 Apache 2.0 MiroVerse-MegaScience 10615 8270 2345 111120264 63594 42443 CC-BY-NC-SA-4.0 MiroVerse-TaskCraft 8890 4277 4613 95518109 35013 17236 MIT MiroVerse-QA-Expert-Multi-Hop-V1.0 6187 2091 4096 63983151 31957 19585 Apache 2.0 MiroVerse-OneGen-TrainDataset-MultiHopQA 3289 1347 1942 33214386 17187 11449 MIT MiroVerse-2WikiMultihopQA 3001 1410 1591 28977451 13982 7981 Apache 2.0 MiroVerse-WikiTables 1606 1288 318 16461870 12089 8877 MIT MiroVerse-WebShaper 1514 486 1028 31240265 12126 9578 MIT MiroVerse-WebDancer 455 192 263 7817689 3170 2268 MIT MiroVerse-v0.1 147985 60412 87573 1993099086 891087 602179 / <p>Dataset Details</p> <p>Every sample includes successful MiroFlow rollout trajactories that reached the verified answer\u2014one JSON line, zero secrets.</p> <p>Licensing Information</p> <p>MiroVerse-v0.1 dataset follows a hybrid licensing model: query and answer data retain their original source licenses, while all trace data is licensed under CC-BY-NC-4.0; for commercial use, please contact us to request a commercial license.</p>"},{"location":"data/#why-were-different","title":"\ud83c\udd9a Why We're Different","text":"<p>Our Philosophy</p> <p>While high-quality data is essential for training advanced models and often kept private, we believe that the path to truly general-purpose agents is still long. That's why we're committed to open-sourcing as much of our data as possible\u2014including raw samples and exploration traces\u2014to support and accelerate progress across the community.</p> Org Work Samples Trace Data Reproducible? OpenAI Deep Research \u2014 \u274c \u274c Gemini Gemini Deep Research \u2014 \u274c \u274c Tencent Cognitive Kernel-Pro 7 k \u274c \u274c Tongyi WebShaper 500 \u274c \u274c MiroMind (ours) this repo 147 k+ \u2705 \u2705"},{"location":"data/#benchmark-performance","title":"\ud83d\udcc8 Benchmark Performance","text":"<p>Training Results</p> <p>MiroVerse-v0.1 is used in the training of our MiroThinker-v0.1 models.</p> <p>By using this dataset, we achieved the following benchmark performance.</p>"},{"location":"data/#gaia-benchmark","title":"GAIA Benchmark","text":"Method Text-103Best Pass@1 Text-103Pass@1 (Avg@8) Val-165Best Pass@1 Val-165Pass@1 (Avg@8) Search-o1-7B 17.5 - - - R1-Searcher-7B 20.4 - - - WebDancer-7B 31.0 - - - WebSailor-7B 37.9 - - - CK-Pro-8B 43.7 - 35.2 - MiroThinker-8B-SFT-v0.1 44.7 40.1 34.6 31.8 + Commercial Tools 46.6 42.1 37.6 33.9 MiroThinker-8B-DPO-v0.1 46.6 44.8 37.0 35.4 + Commercial Tools 50.5 46.7 38.2 35.9 MiroThinker-14B-SFT-v0.1 47.6 44.4 37.0 34.4 + Commercial Tools 49.5 47.5 41.8 39.8 MiroThinker-14B-DPO-v0.1 48.5 46.6 42.4 39.2 + Commercial Tools 52.4 48.5 45.5 42.0 Qwen3-32B 31.1 26.7 29.7 26.4 Search-o1-32B 28.2 - - - WebThinker-32B-RL 48.5 - - - WebDancer-QwQ-32B 51.5 - - - WebSailor-32B 53.2 - - - WebShaper-QwQ-32B 53.3 - - - MiroThinker-32B-SFT-v0.1 55.3 51.3 44.9 42.7 + Commercial Tools 58.3 54.2 48.5 45.8 MiroThinker-32B-DPO-v0.1 57.3 54.1 48.5 45.9 + Commercial Tools 60.2 57.9 50.9 48.9 <ol> <li> <p>Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.</p> </li> <li> <p>For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.</p> </li> <li> <p>By default, we use open-source tools wherever possible, except for the code tool E2B and the Google search tool Serper. We use Whisper, Qwen2.5-VL-72B-Instruct, and Qwen3-235B-A22B-Thinking-2507 in our implementation. The framework can be easily extended to other open-source tools of your choice.</p> </li> <li> <p>Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our models.</p> </li> </ol>"},{"location":"data/#more-benchmarks","title":"More Benchmarks","text":"Method HLEPass@1 FramesPass@1 BrowseCompPass@1 BrowseComp-ZHPass@1 WebWalkerQAPass@1 OpenAI Deep Research 26.6 - 51.5 42.9 - Gemini Deep Research 26.9 - - - - Kimi-Researcher 26.9 78.8 - - - WebDancer-7B - - - - 36.0 WebSailor-7B - - 6.7 14.2 - MiroThinker-8B-SFT-v0.1 - 58.0 5.5 9.3 41.3 MiroThinker-8B-DPO-v0.1 - 64.4 8.7 13.5 45.7 WebThinker-32B-RL - - - - 46.5 WebDancer-QwQ-32B - - 3.8 18.0 47.9 WebSailor-32B - - 10.5 25.5 - WebShaper-32B - - - - 51.4 MiroThinker-32B-SFT-v0.1 10.2 70.4 10.6 13.8 45.7 MiroThinker-32B-DPO-v0.1 11.8 71.7 13.0 17.0 49.3 <ol> <li> <p>MiroThinker\u2019s performance was tested with this repository and open-source tools; other models\u2019 results are from their papers and official sites.</p> </li> <li> <p>As MiroVerse-v0.1 mainly contains English data, the model's Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.</p> </li> </ol>"},{"location":"data/#examples","title":"\ud83e\udde9 Examples","text":"<p>Sample QA Examples</p> <p>Below are two QA examples synthesized by our data engine (MiroVerse-Voyager1.0).</p> <p>Case 1</p> <p>Q: A female lead actress received her first major annual Hindi film performance award for best actress for her role in a late-2000s comedy-drama, directed by the filmmaker who later created a sports-themed drama released in 2023 starring an actress known for completing an athletic triathlon event in Berlin. What is the title of the film for which this actress first won that award?</p> <p>A: Paa</p> <p>Case 2</p> <p>Q: Identify the agricultural practice, unique to a mountain range that forms a border including an independent principality and known for spectacular geologic landforms, that was one of the key reasons for part of the range's inscription as a UNESCO World Heritage Site in the decade before the 21st century. This region's history features a brief early-1800s reorganization of provincial boundaries after a liberal revolution in the southern country, and the northern country is globally recognized as the leading tourist destination with the fourth-largest number of heritage sites. What is this traditional agricultural system called?</p> <p>A: transhumance</p>"},{"location":"data/#free-trace-rollout-let-us-help-you-train","title":"\ud83d\udee0\ufe0f Free Trace Rollout: Let Us Help You Train","text":"<p>Community Support</p> <p>Generating high-quality training trajectories is expensive \u2014 on average, $1.50 per sample using top-tier commercial models.</p> <p>To empower the community, we're offering free rollout services for qualifying seed data:</p>"},{"location":"data/#how-it-works","title":"How It Works","text":"<p>Process Steps</p> <p>1. Submit a Request</p> <p>Open a ticket via this template and provide the basic info, rollout requirements, and up to 100 sample rows in one go.</p> <p>2. Review &amp; Rollout</p> <p>We'll review your submission within 48 hours. Once approved, we'll reach out to you for the full dataset and then launch the complete trace rollout using top-tier commercial models.</p> <p>3. Delivery &amp; Recognition</p> <p>Upon completion, we'll send the augmented dataset to you via email.</p> <p>With your explicit consent, we'll also publish it publicly and credit you as a Community Contributor \u2014 with a permanent badge in this README.</p>"},{"location":"data/#license","title":"\ud83e\udd1d License","text":"<p>License Terms</p> <p>This project is released under the CC BY-NC 4.0. Parts of this project contain code and models from other sources, which are subject to their respective licenses. For commercial use cases, please contact us at: service@miromind.ai.</p>"},{"location":"data/#citation","title":"\ud83d\udcdc Citation","text":"<p>Academic Citation</p> <p>If you find this project useful in your research, please consider cite:</p> BibTeX Citation<pre><code>@misc{miromind2024opendata,\n  title={MiroVerse V0.1: A Reproducible, Full-Trajectory, Ever-Growing Deep Research Dataset},\n  author={MiroMind Data Team},\n  year={2025},\n  url={https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1}\n}\n</code></pre>"},{"location":"data/#contact-us","title":"Contact Us","text":"<p>Get in Touch</p> <p>MiroVerse is developed by the MiroMind Data Team. If you would like to leave us a message, feel free to get in touch.  In addition to GitHub,  Discord,  WeChat,  and RedNote,  you can also reach us via email at service@miromind.ai.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"download_datasets/","title":"Dataset Download Instructions","text":"<p>This guide walks you through downloading and preparing benchmark datasets for MiroFlow evaluation.</p>"},{"location":"download_datasets/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>Before downloading datasets, ensure you have completed both access requests and environment setup below.</p>"},{"location":"download_datasets/#1-request-dataset-access","title":"1. Request Dataset Access","text":"<p>You must request access to the following Hugging Face datasets:</p> <p>Required Datasets</p> <ul> <li>GAIA Dataset: https://huggingface.co/datasets/gaia-benchmark/GAIA</li> <li>HLE Dataset: https://huggingface.co/datasets/cais/hle</li> </ul> <p>Visit the links above and request access to both datasets.</p>"},{"location":"download_datasets/#2-configure-environment-variables","title":"2. Configure Environment Variables","text":"<p>Copy the template file and create your environment configuration:</p> <pre><code>cp .env.template .env\n</code></pre> <p>Edit the <code>.env</code> file and configure these essential variables:</p> .env<pre><code># Required: Your Hugging Face token for dataset access\nHF_TOKEN=\"your-actual-huggingface-token-here\"\n\n# Data directory path \nDATA_DIR=\"data/\"\n</code></pre> <p>Getting Your Hugging Face Token</p> <ol> <li>Go to https://huggingface.co/settings/tokens</li> <li>Create a new token with at least \"Read\" permissions</li> <li>Replace <code>your-actual-huggingface-token-here</code> in the <code>.env</code> file with your actual token</li> </ol>"},{"location":"download_datasets/#download-and-prepare-datasets","title":"Download and Prepare Datasets","text":"<p>Once you have been granted access to the required datasets, run the preparation script to download all benchmark datasets.</p>"},{"location":"download_datasets/#running-the-download-script","title":"Running the Download Script","text":"<p>Execute the following command to start the download process for all datasets, if a single dataset is needed, you could run the specific command:</p> <pre><code>bash scripts/run_prepare_benchmark.sh\n</code></pre> <p>Script Contents</p> <p>The script contains the following logic and dataset downloads. You can comment out any unwanted datasets by adding <code>#</code> at the start of the line.</p> scripts/run_prepare_benchmark.sh<pre><code>#!/bin/bash\necho \"Please grant access to these datasets:\"\necho \"- https://huggingface.co/datasets/gaia-benchmark/GAIA\"\necho \"- https://huggingface.co/datasets/cais/hle\"\necho\n\nread -p \"Have you granted access? [Y/n]: \" answer\nanswer=${answer:-Y}\nif [[ ! $answer =~ ^[Yy] ]]; then\n    echo \"Please grant access to the datasets first\"\n    exit 1\nfi\necho \"Access confirmed\"\n\n# Comment out any unwanted datasets by adding # at the start of the line\nuv run main.py prepare-benchmark get gaia-val\nuv run main.py prepare-benchmark get gaia-val-text-only\nuv run main.py prepare-benchmark get frames-test\nuv run main.py prepare-benchmark get webwalkerqa\nuv run main.py prepare-benchmark get browsecomp-test\nuv run main.py prepare-benchmark get browsecomp-zh-test\nuv run main.py prepare-benchmark get hle\nuv run main.py prepare-benchmark get xbench-ds\nuv run main.py prepare-benchmark get futurex\n</code></pre>"},{"location":"download_datasets/#what-this-script-does","title":"What This Script Does","text":"<p>Script Actions</p> <ol> <li>Confirms dataset access - Verifies you have requested access to required datasets</li> <li>Downloads benchmark datasets - Retrieves the following datasets:<ul> <li><code>gaia-val</code> - GAIA validation set</li> <li><code>gaia-val-text-only</code> - Text-only GAIA validation</li> <li><code>frames-test</code> - Frames test dataset</li> <li><code>webwalkerqa</code> - Web Walker QA dataset</li> <li><code>browsecomp-test</code> - English BrowseComp test set</li> <li><code>browsecomp-zh-test</code> - Chinese BrowseComp test set</li> <li><code>hle</code> - HLE dataset</li> <li><code>xbench-ds</code> - xbench-DeepSearch dataset</li> <li><code>futurex</code> - Futurex-Online dataset</li> </ul> </li> </ol>"},{"location":"download_datasets/#customizing-dataset-selection","title":"Customizing Dataset Selection","text":"<p>To download only specific datasets, edit the script and comment out unwanted lines:</p> <pre><code># Comment out unwanted datasets like this:\n# uv run main.py prepare-benchmark get gaia-val\nuv run main.py prepare-benchmark get gaia-val-text-only\n# uv run main.py prepare-benchmark get frames-test\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"e2b_advanced_features/","title":"E2B Extension","text":"<p>We provide an option for local E2B Sandbox.</p>"},{"location":"e2b_advanced_features/#local-e2b-sandbox-deployment","title":"Local E2B Sandbox Deployment","text":"<p>To achieve our best benchmark results, we recommend using a pre-defined sandbox template that includes the most commonly used Python and apt packages. </p> <p>If you prefer not to use a sandbox template, you can disable it by commenting out the line <code>template=DEFAULT_TEMPLATE_ID,</code> in <code>libs/miroflow-tool/src/miroflow/tool/mcp_servers/python_server.py</code> (line 145).</p>"},{"location":"e2b_advanced_features/#prepare-e2b-sandbox-optional","title":"Prepare E2B Sandbox (Optional)","text":"<p>[!TIP] We provide a public E2B sandbox template. Follow this step if you want to reproduce the best scores.</p> <p>For the E2B sandbox service, we recommend setting up a Linux Docker image with a comprehensive set of apt and Python packages pre-installed. Without these pre-installed packages, the agent will need to spend extra steps and context installing them, resulting in reduced token efficiency.</p> <p>you need to have <code>npm</code> install and <code>docker</code> running locally.</p> <ol> <li>Install <code>e2b</code> command line and login:</li> </ol> <pre><code>## install e2b\nnpm install -g @e2b/cli\n## check that it is available\nwhich e2b \n</code></pre> <ol> <li>Download our pre-configured Dockerfile: e2b.Dockerfile.</li> </ol> <pre><code>wget https://github.com/MiroMindAI/MiroFlow/blob/main/docs/e2b.Dockerfile\n</code></pre> <ol> <li>Run <code>e2b template build</code> command check official doc here, use <code>all_pip_apt_pkg</code> as the name of template.</li> </ol> <pre><code>## build the template with `docker build` locally\nE2B_ACCESS_TOKEN=${your-token}\ne2b template build -c \"/root/.jupyter/start-up.sh\" -n \"all_pip_apt_pkg\" -d ./e2b.Dockerfile\n## check that template is built successfully\nE2B_ACCESS_TOKEN=${your-token} e2b template list\n</code></pre> <p>You can also create your own custom sandbox template for specific use cases by following similar steps. For more information, please refer to the E2B Docker documentation.</p>"},{"location":"e2b_advanced_features/#e2b-docker","title":"E2B Docker","text":"<p>This document describes the custom E2B Docker environment used by MiroFlow for code execution. The E2B extension provides a sandboxed environment with pre-installed scientific computing libraries, data analysis tools, and other dependencies commonly needed for AI agent tasks.</p> <pre><code># You can use most Debian-based base images\nFROM e2bdev/code-interpreter\n\n# Update package list and install Python 3.10 and pip\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    portaudio19-dev \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel\n\n# Install dependencies and customize sandbox\nRUN python3 -m pip install --no-cache-dir \\\n    Flask \\\n    IPython \\\n    Pillow \\\n    PyGithub \\\n    PyMuPDF \\\n    PyPDF2 \\\n    arch \\\n    arm-pyart \\\n    arxiv \\\n    ase \\\n    astropy \\\n    astroquery \\\n    awscli \\\n    beautifulsoup4 \\\n    biopython \\\n    boto3 \\\n    brian2 \\\n    cairosvg \\\n    cgt \\\n    chardet \\\n    chess \\\n    cinemagoer \\\n    clifford \\\n    contextily \\\n    control \\\n    cryptography \\\n    cvxpy \\\n    datasets \\\n    descarteslabs \\\n    duckduckgo-search \\\n    edalize \\\n    english_words \\\n    ephem \\\n    esp-docs \\\n    flask \\\n    folium \\\n    geopandas \\\n    geopy \\\n    google-search-results \\\n    googlesearch-python \\\n    googletrans \\\n    habanero \\\n    helics \\\n    hijri_converter \\\n    imbalanced-learn \\\n    inflect \\\n    isbnlib \\\n    kaggle \\\n    lifelines \\\n    lxml \\\n    lxml_html_clean \\\n    mapclassify \\\n    markdown \\\n    'matplotlib&gt;=3.8' \\\n    mendeleev \\\n    metpy \\\n    music21 \\\n    networkx \\\n    nipype \\\n    numba \\\n    'numpy&gt;=2' \\\n    opencv-python \\\n    openpyxl \\\n    'pandas&gt;=2' \\\n    pandas_datareader \\\n    parsl \\\n    pdf2image \\\n    pdfminer \\\n    pdfplumber \\\n    periodictable \\\n    plotly \\\n    polars \\\n    psycopg2-binary \\\n    pulp \\\n    pyXSteam \\\n    pybel \\\n    pycryptodome \\\n    pydot \\\n    pygplates \\\n    pymatgen \\\n    pymupdf \\\n    pypdf2 \\\n    pypinyin \\\n    pyscf \\\n    pytesseract \\\n    python-docx \\\n    pytube \\\n    pywavelets \\\n    rdflib \\\n    reportlab \\\n    requests \\\n    requests-html \\\n    scanpy \\\n    scikit-image \\\n    scikit-learn \\\n    scipy \\\n    scvelo \\\n    seaborn \\\n    selenium \\\n    semanticscholar \\\n    shap \\\n    shapely \\\n    siphon \\\n    skyfield \\\n    smbus2 \\\n    snappy \\\n    spglib \\\n    sphinx \\\n    splink \\\n    statsmodels \\\n    stockfish \\\n    sympy \\\n    tabulate \\\n    torch \\\n    torchvision \\\n    transformers \\\n    uncertainpy \\\n    us \\\n    virtualenv \\\n    wbdata \\\n    webdriver-manager \\\n    wikipedia-api \\\n    wolframalpha \\\n    wordfreq \\\n    yfinance \\\n    yt-dlp \\\n    docx2txt \\\n    rdkit \\\n    stockfish \\\n    yfinance \\\n    seaborn \\\n    python-pptx \\\n    pyaudio \\\n    pyshp \\\n    SpeechRecognition \\\n    waybackpy\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    # \u2500\u2500 Basic build &amp; Python \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    build-essential gfortran cmake pkg-config git curl wget ca-certificates \\\n    # \u2500\u2500 scientific computing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libopenblas-dev liblapack-dev libatlas-base-dev \\\n    libssl-dev libffi-dev zlib1g-dev \\\n    # \u2500\u2500 image / OpenCV / Pillow \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libgl1 libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender1 \\\n    libjpeg-dev libpng-dev libwebp-dev libfreetype6-dev libopenjp2-7 liblcms2-dev \\\n    # \u2500\u2500 video / audio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ffmpeg libsndfile1 sox portaudio19-dev \\\n    # \u2500\u2500 PDF / doc / OCR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    poppler-utils pdfgrep ghostscript \\\n    tesseract-ocr tesseract-ocr-deu \\\n    libxml2-dev libxslt1-dev \\\n    # \u2500\u2500 other tools \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    imagemagick unlambda stockfish \\\n    unzip zip tar nano &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"evaluation_overview/","title":"\ud83d\udcca Performance Benchmarks","text":"<p>MiroFlow achieves state-of-the-art performance across multiple agentic benchmarks, demonstrating its effectiveness in complex reasoning and tool-use tasks.</p>"},{"location":"evaluation_overview/#performance-on-future-prediction","title":"Performance on Future Prediction","text":"<p>Future X Benchmark Results</p> <p>MiroFlow demonstrates exceptional performance in future prediction tasks.</p> <p></p>"},{"location":"evaluation_overview/#performance-on-benchmarks","title":"\u2728 Performance on Benchmarks","text":"<p>Comprehensive Benchmark Analysis</p> <p>We benchmark MiroFlow on a series of benchmarks including GAIA, HLE, BrowseComp and xBench-DeepSearch.</p> <p></p>"},{"location":"evaluation_overview/#other-benchmark-results","title":"Other Benchmark Results","text":"<p>Detailed Performance Comparison</p> <p>Comprehensive comparison across multiple benchmark categories and competing frameworks.</p>"},{"location":"evaluation_overview/#reasoning-language-understanding","title":"Reasoning &amp; Language Understanding","text":"Model/Framework GAIA Val HLE HLE-Text MiroFlow 82.4% 27.2% 29.5% OpenAI Deep Research 67.4% 26.6% - Gemini Deep Research - 26.9% - Kimi Researcher - - 26.9% WebSailor-72B 55.4% - - Manus 73.3% - - DeepSeek v3.1 - - 29.8%"},{"location":"evaluation_overview/#web-browsing-search-tasks","title":"Web Browsing &amp; Search Tasks","text":"Model/Framework BrowserComp-EN BrowserComp-ZH xBench-DeepSearch MiroFlow 33.2% 47.1% 72.0% OpenAI Deep Research 51.5% 42.9% - Gemini Deep Research - - 50+% Kimi Researcher - - 69.0% WebSailor-72B - 30.1% 55.0% DeepSeek v3.1 - - 71.2% <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"faq_and_known_issues/","title":"Faq and known issues","text":""},{"location":"faq_and_known_issues/#faq","title":"FAQ","text":"<p>Q: What is the estimated cost of running the GAIA validation set for a single run? A: The cost is approximately $250 USD for a run with cache.</p> <p>Q: How long does it take to run the GAIA validation set for a single run? A: With the <code>max_concurrent</code> parameter set to 20, a full run takes about 2 hours to complete.</p> <p>Q: Are all the specified APIs required? A: Yes. To fully reproduce our published results, access to all the listed APIs in corresponding benchmark is necessary.</p> <p>Q: What is the difference between MiroFlow and MiroThinker? A:  MiroFlow is primarily focused on interacting with proprietary models; MiroThinker is designed for our own open-source models.</p> <p>We plan to merge these two projects in the future to create a single, unified platform.</p>"},{"location":"faq_and_known_issues/#known-issues-roadmap","title":"Known Issues &amp; Roadmap","text":""},{"location":"faq_and_known_issues/#currently-in-development","title":"\ud83d\udd04 Currently in Development","text":"<ul> <li>FutureX Benchmark: Adding support for FutureX benchmark evaluation</li> <li>Token Usage &amp; Cost Tracking: Implementing detailed usage analytics and cost calculation features</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"faqs/","title":"\ud83d\udc1b FAQ and Known Issues","text":"<p>Common questions and development roadmap for MiroFlow framework.</p>"},{"location":"faqs/#faq","title":"FAQ","text":"<p>Frequently Asked Questions</p> <p>Common questions about MiroFlow usage, costs, and platform differences.</p> <p>Q: What is the estimated cost of running the GAIA validation set for a single run?</p> <p>A: The cost is approximately $250 USD for a run with cache.</p> <p>Q: How long does it take to run the GAIA validation set for a single run?</p> <p>A: With the <code>max_concurrent</code> parameter set to 20, a full run takes about 2 hours to complete.</p> <p>Q: Are all the specified APIs required?</p> <p>A: Yes. To fully reproduce our published results, access to all the listed APIs in corresponding benchmark is necessary.</p> <p>Q: What is the difference between MiroFlow and MiroThinker?</p> <p>A: MiroFlow is primarily focused on interacting with proprietary models; MiroThinker is designed for our own open-source models.</p> <p>We plan to merge these two projects in the future to create a single, unified platform.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"futurex/","title":"Futurex-Online","text":"<p>MiroFlow's evaluation on the Futurex-Online benchmark demonstrates capabilities in future event prediction tasks.</p>"},{"location":"futurex/#dataset-overview","title":"Dataset Overview","text":"<p>Futurex-Online Dataset</p> <p>The Futurex-Online dataset consists of 61 prediction tasks covering various future events including:</p> <ul> <li>Political events (referendums, elections)</li> <li>Sports outcomes (football matches)</li> <li>Legal proceedings</li> <li>Economic indicators</li> </ul> <p>Key Dataset Characteristics</p> <ul> <li>Total Tasks: 61</li> <li>Task Type: Future event prediction</li> <li>Answer Format: Boxed answers (\\boxed{Yes/No} or \\boxed{A/B/C})</li> <li>Ground Truth: Not available (prediction tasks)</li> <li>Resolution Date: Around 2025-09-21 (GMT+8)</li> </ul>"},{"location":"futurex/#quick-start-guide","title":"Quick Start Guide","text":"<p>Quick Start Instructions</p> <p>This section provides step-by-step instructions to run the Futurex-Online benchmark and prepare submission results. Since this is a prediction dataset without ground truth, we focus on execution traces and response generation. Note: This is a quick start guide for running the benchmark, not for reproducing exact submitted results.</p>"},{"location":"futurex/#step-1-prepare-the-futurex-online-dataset","title":"Step 1: Prepare the Futurex-Online Dataset","text":"<p>Dataset Setup</p> <p>Use the integrated prepare-benchmark command to download and process the dataset:</p> Download Futurex-Online Dataset<pre><code>uv run main.py prepare-benchmark get futurex\n</code></pre> <p>This will create the standardized dataset at <code>data/futurex/standardized_data.jsonl</code>.</p>"},{"location":"futurex/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>API Key Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># For searching and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use Claude-3.7-Sonnet with OpenRouter backend to initialize the LLM\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n\n# Use for llm judge, reasoning, o3 hints, etc.\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"futurex/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Evaluation Execution</p> <p>Execute the following command to run evaluation on the Futurex-Online dataset. This uses the basic <code>agent_quickstart_1</code> configuration for quick start purposes.</p> Run Futurex-Online Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_quickstart_1 benchmark=futurex output_dir=\"logs/futurex/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Progress Monitoring and Resume</p> <p>To check the progress while running:</p> Check Progress<pre><code>uv run utils/progress_check/check_futurex_progress.py $PATH_TO_LOG\n</code></pre> <p>If you need to resume an interrupted evaluation, specify the same output directory to continue from where you left off.</p> Resume Evaluation, e.g.<pre><code>uv run main.py common-benchmark --config_file_name=agent_quickstart_1 benchmark=futurex output_dir=\"logs/futurex/20250918_1010\"\n</code></pre>"},{"location":"futurex/#step-4-extract-results","title":"Step 4: Extract Results","text":"<p>Result Extraction</p> <p>After evaluation completion, extract the results using the provided utility:</p> Extract Results<pre><code>uv run utils/extract_futurex_results.py logs/futurex/$(date +\"%Y%m%d_%H%M\")\n</code></pre> <p>This will generate:</p> <ul> <li><code>futurex_results.json</code>: Detailed results for each task</li> <li><code>futurex_summary.json</code>: Summary statistics</li> <li><code>futurex_predictions.csv</code>: Predictions in CSV format</li> </ul>"},{"location":"futurex/#sample-task-examples","title":"Sample Task Examples","text":""},{"location":"futurex/#political-prediction","title":"Political Prediction","text":"<pre><code>Task: \"Will the 2025 Guinea referendum pass? (resolved around 2025-09-21 (GMT+8))\"\nExpected Format: \\boxed{Yes} or \\boxed{No}\n</code></pre>"},{"location":"futurex/#sports-prediction","title":"Sports Prediction","text":"<pre><code>Task: \"Brighton vs. Tottenham (resolved around 2025-09-21 (GMT+8))\nA. Brighton win on 2025-09-20\nB. Brighton vs. Tottenham end in a draw  \nC. Tottenham win on 2025-09-20\"\nExpected Format: \\boxed{A}, \\boxed{B}, or \\boxed{C}\n</code></pre>"},{"location":"futurex/#multiple-runs-and-voting","title":"Multiple Runs and Voting","text":"<p>Improving Prediction Accuracy</p> <p>For better prediction accuracy, you can run multiple evaluations and use voting mechanisms to aggregate results. This approach helps reduce randomness and improve the reliability of predictions. Note: This is a quick start approach; production submissions may use more sophisticated configurations.</p>"},{"location":"futurex/#step-1-run-multiple-evaluations","title":"Step 1: Run Multiple Evaluations","text":"<p>Use the multiple runs script to execute several independent evaluations:</p> Run Multiple Evaluations<pre><code>./scripts/run_evaluate_multiple_runs_futurex.sh\n</code></pre> <p>This script will:</p> <ul> <li>Run 3 independent evaluations by default (configurable with <code>NUM_RUNS</code>)</li> <li>Execute all tasks in parallel for efficiency</li> <li>Generate separate result files for each run in <code>run_1/</code>, <code>run_2/</code>, etc.</li> <li>Create a consolidated <code>futurex_submission.jsonl</code> file with voting results</li> </ul>"},{"location":"futurex/#step-2-customize-multiple-runs","title":"Step 2: Customize Multiple Runs","text":"<p>You can customize the evaluation parameters:</p> Custom Multiple Runs<pre><code># Run 5 evaluations with limited tasks for testing\nNUM_RUNS=5 MAX_TASKS=10 ./scripts/run_evaluate_multiple_runs_futurex.sh\n\n# Use different agent configuration\nAGENT_SET=agent_gaia-validation ./scripts/run_evaluate_multiple_runs_futurex.sh\n\n# Adjust concurrency for resource management\nMAX_CONCURRENT=3 ./scripts/run_evaluate_multiple_runs_futurex.sh\n</code></pre>"},{"location":"futurex/#step-3-voting-and-aggregation","title":"Step 3: Voting and Aggregation","text":"<p>After multiple runs, the system automatically:</p> <ol> <li>Extracts predictions from all runs using <code>utils/extract_futurex_results.py</code></li> <li>Applies majority voting to aggregate predictions across runs</li> <li>Generates submission file in the format required by FutureX platform</li> <li>Provides voting statistics showing prediction distribution across runs</li> </ol> <p>The voting process works as follows:</p> <ul> <li>Majority Vote: Most common prediction across all runs wins</li> <li>Tie-breaking: If tied, chooses the prediction that appeared earliest across all runs</li> <li>Vote Counts: Tracks how many runs predicted each option</li> <li>Confidence Indicators: High agreement indicates more reliable predictions</li> </ul>"},{"location":"futurex/#step-4-analyze-voting-results","title":"Step 4: Analyze Voting Results","text":"<p>Check the generated files for voting analysis:</p> Check Voting Results<pre><code># View submission file with voting results\ncat logs/futurex/agent_quickstart_1_*/futurex_submission.jsonl\n\n# Check individual run results\nls logs/futurex/agent_quickstart_1_*/run_*/\n\n# Check progress and voting statistics\nuv run python utils/progress_check/check_futurex_progress.py logs/futurex/agent_quickstart_1_*\n</code></pre>"},{"location":"futurex/#manual-voting-aggregation","title":"Manual Voting Aggregation","text":"<p>You can also manually run the voting aggregation:</p> Manual Voting Aggregation<pre><code># Aggregate multiple runs with majority voting\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_1_* --aggregate\n\n# Force single run mode (if needed)\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_1_*/run_1 --single\n\n# Specify custom output file\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_1_* -o my_voted_predictions.jsonl\n</code></pre>"},{"location":"futurex/#voting-output-format","title":"Voting Output Format","text":"<p>The voting aggregation generates a submission file with the following format:</p> <pre><code>{\"id\": \"687104310a994c0060ef87a9\", \"prediction\": \"No\", \"vote_counts\": {\"No\": 2}}\n{\"id\": \"68a9b46e961bd3003c8f006b\", \"prediction\": \"Yes\", \"vote_counts\": {\"Yes\": 2}}\n</code></pre> <p>The output includes:</p> <ul> <li><code>id</code>: Task identifier</li> <li><code>prediction</code>: Final voted prediction (without <code>\\boxed{}</code> wrapper)</li> <li><code>vote_counts</code>: Dictionary showing how many runs predicted each option</li> </ul> <p>For example, <code>\"vote_counts\": {\"No\": 2}</code> means 2 out of 2 runs predicted \"No\", indicating high confidence.</p>"},{"location":"futurex/#evaluation-notes","title":"Evaluation Notes","text":"<p>No Ground Truth Available</p> <p>Since Futurex-Online is a prediction dataset, there are no ground truth answers available for evaluation. The focus is on:</p> <ul> <li>Response generation quality</li> <li>Reasoning process documentation</li> <li>Prediction confidence and methodology</li> </ul> <p>Output Analysis</p> <p>The evaluation generates detailed execution traces showing:</p> <ul> <li>Research process for each prediction</li> <li>Information gathering from web sources</li> <li>Reasoning chains leading to predictions</li> <li>Final boxed answers in required format</li> </ul>"},{"location":"futurex/#directory-structure","title":"Directory Structure","text":"<p>After running multiple evaluations, you'll find the following structure:</p> <pre><code>logs/futurex/agent_quickstart_1_YYYYMMDD_HHMM/\n\u251c\u2500\u2500 futurex_submission.jsonl          # Final voted predictions\n\u251c\u2500\u2500 run_1/                            # First run results\n\u2502   \u251c\u2500\u2500 benchmark_results.jsonl       # Individual task results\n\u2502   \u251c\u2500\u2500 benchmark_results_pass_at_1_accuracy.txt\n\u2502   \u2514\u2500\u2500 task_*_attempt_1.json        # Detailed execution traces\n\u251c\u2500\u2500 run_2/                            # Second run results\n\u2502   \u2514\u2500\u2500 ... (same structure as run_1)\n\u251c\u2500\u2500 run_1_output.log                  # Run 1 execution log\n\u2514\u2500\u2500 run_2_output.log                  # Run 2 execution log\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_test/","title":"GAIA Test","text":"<p>This document provides step-by-step instructions for evaluating the GAIA test benchmark.</p>"},{"location":"gaia_test/#step-1-prepare-the-gaia-test-dataset","title":"Step 1: Prepare the GAIA Test Dataset","text":"<p>First, download and prepare the GAIA test dataset: <pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-test.zip\nunzip gaia-test.zip\n# The unzip passcode is: `pf4*`\n</code></pre></p>"},{"location":"gaia_test/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> <pre><code># For searching and scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use Claude-3.7-Sonnet with OpenRouter backend to initialize the LLM. The main reason is that OpenRouter provides better response rates\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n\n# Use for llm judge, reasoning, o3 hints, etc.\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"gaia_test/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the following command to run a single evaluation pass on the GAIA test dataset:</p> <pre><code>uv run main.py common-benchmark --config_file_name=agent_gaia-test output_dir=\"logs/gaia-test/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Index @ MiroMind AI</p>"},{"location":"gaia_validation/","title":"GAIA Validation","text":"<p>MiroFlow's performance on the GAIA validation benchmark demonstrates state-of-the-art capabilities in complex reasoning tasks.</p>"},{"location":"gaia_validation/#performance-comparison","title":"Performance Comparison","text":"<p>State-of-the-Art Performance</p> <p>MiroFlow achieves state-of-the-art (SOTA) performance among open-source agent frameworks on the GAIA validation set.</p> <p></p> <p>Key Performance Metrics</p> <ul> <li>Pass@3: 81.8%</li> <li>Majority Vote: 82.4%</li> <li>Pass@1 (best@3): 74.5%</li> <li>Pass@1 (avg@3): 72.2%</li> </ul> <p>Reproducibility Guarantee</p> <p>Unlike other frameworks with unclear evaluation methods, MiroFlow's results are fully reproducible. Note that Hugging Face access was disabled during inference to prevent direct answer retrieval.</p>"},{"location":"gaia_validation/#reproduction-guide","title":"Reproduction Guide","text":"<p>Reproducibility Instructions</p> <p>This section provides step-by-step instructions to reproduce our GAIA validation benchmark results. All results are fully reproducible using our open-source framework.</p>"},{"location":"gaia_validation/#step-1-prepare-the-gaia-validation-dataset","title":"Step 1: Prepare the GAIA Validation Dataset","text":"<p>Dataset Setup</p> <p>Please follow the Dataset Download Instructions from previous section.</p> <p>Alternatively, you can manually download and set up the dataset as follows:</p> Manual Dataset Download<pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-val.zip\nunzip gaia-val.zip\n# The unzip passcode is: `pf4*`\n</code></pre>"},{"location":"gaia_validation/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>API Key Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># For searching and scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use Claude-3.7-Sonnet with OpenRouter backend to initialize the LLM. The main reason is that OpenRouter provides better response rates\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n\n# Use for llm judge, reasoning, o3 hints, etc.\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"gaia_validation/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Evaluation Execution</p> <p>Execute the following command to run a single evaluation pass on the GAIA validation dataset:</p> Run GAIA Validation<pre><code>uv run main.py common-benchmark --config_file_name=agent_gaia-validation output_dir=\"logs/gaia-validation/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Progress Monitoring and Resume</p> <p>To check the progress while running:</p> <p>Check Progress<pre><code>uv run uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> If you need to resume an interrupted evaluation, specify the same output directory to continue from where you left off.</p> Resume Evaluation, e.g.<pre><code>uv run main.py common-benchmark --config_file_name=agent_gaia-validation --output_dir=\"logs/gaia-validation/20251225_1430\"\n</code></pre>"},{"location":"gaia_validation/#traces","title":"Traces","text":"<p>Complete Execution Traces</p> <p>We have released our complete execution traces for the <code>gaia-validation</code> dataset on Hugging Face. This comprehensive collection includes a full run of 165 tasks with an overall accuracy of 73.94%.</p> <p>You can download them using the following command:</p> Download Execution Traces<pre><code>wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia_validation_miroflow_trace_public_20250825.zip\nunzip gaia_validation_miroflow_trace_public_20250825.zip\n# The unzip passcode is: `pf4*`.\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#overview","title":"Overview","text":"<p>MiroFlow is released under the Apache License 2.0, which is a permissive open-source license that allows for both commercial and non-commercial use.</p> <p>License Summary</p> <ul> <li>\u2705 Commercial use - Use MiroFlow in commercial projects</li> <li>\u2705 Modification - Modify and adapt the code for your needs  </li> <li>\u2705 Distribution - Distribute original or modified versions</li> <li>\u2705 Private use - Use MiroFlow in private projects</li> <li>\u26a0\ufe0f Trademark - You cannot use MiroMind AI trademarks</li> <li>\u26a0\ufe0f Liability - No warranty or liability from the authors</li> </ul>"},{"location":"license/#apache-license-20","title":"Apache License 2.0","text":"<p>The full text of the Apache License 2.0 can be found at: https://www.apache.org/licenses/LICENSE-2.0</p>"},{"location":"license/#component-licenses","title":"Component Licenses","text":"<p>Some components within MiroFlow may have different licenses:</p> <p>Third-Party Components</p> <p>Individual components, dependencies, or integrated tools may have their own license terms. Please check the respective file headers, <code>LICENSE</code> files, or documentation for specific licensing information.</p>"},{"location":"license/#attribution","title":"Attribution","text":"<p>When using MiroFlow in your projects, attribution is appreciated but not required. You may include:</p> <pre><code>Powered by MiroFlow - https://github.com/MiroMindAI/MiroFlow\n</code></pre>"},{"location":"license/#questions","title":"Questions","text":"<p>For licensing questions or clarifications, please:</p> <ul> <li>Review the full Apache License 2.0 text</li> <li>Check individual component licenses</li> <li>Open an issue on our GitHub repository</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"llm_clients_overview/","title":"LLM Clients Overview","text":""},{"location":"llm_clients_overview/#available-clients","title":"Available Clients","text":"Client Provider Model Environment Variables <code>ClaudeAnthropicClient</code> Anthropic Direct claude-3-7-sonnet <code>ANTHROPIC_API_KEY</code>, <code>ANTHROPIC_BASE_URL</code> <code>ClaudeOpenRouterClient</code> OpenRouter anthropic/claude-3.7-sonnet, and other supported models <code>OPENROUTER_API_KEY</code>, <code>OPENROUTER_BASE_URL</code> <code>GPTOpenAIClient</code> OpenAI gpt-4, gpt-3.5 <code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code> <code>MiroThinkerSGLangClient</code> SGLang MiroThinker series <code>OAI_MIROTHINKER_API_KEY</code>, <code>OAI_MIROTHINKER_BASE_URL</code>"},{"location":"llm_clients_overview/#basic-configuration","title":"Basic Configuration","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"ClientName\"\n    model_name: \"model-name\"\n    api_key_param: \"${oc.env:API_KEY,???}\"\n    base_url_param: \"${oc.env:BASE_URL,default-url}\"\n    ...\n</code></pre>"},{"location":"llm_clients_overview/#quick-start","title":"Quick Start","text":"<ol> <li>Set relevant environment variable for your chosen provider</li> <li>Update your yaml config file</li> <li>Run: <code>uv run main.py trace --config_file_name=your_config_file --task=\"task\" --task_file_name=\"file\"</code></li> </ol> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"mirothinker/","title":"MiroThinker","text":""},{"location":"mirothinker/#mirothinker","title":"\ud83c\udf1f MiroThinker","text":"<p>MiroThinker (4B/7B/14B/32B) is our suite of open-source agentic models, designed to work seamlessly with the MiroFlow framework. Our models are specifically built to handle complex, multi-tool tasks, leveraging the reproducible and robust foundation that MiroFlow provides.</p> <p>By combining MiroFlow's reliable orchestration with MiroThinker's advanced reasoning capabilities, we offer a powerful, end-to-end solution for building high-performing, reproducible AI agents. These models are a direct result of our extensive data collection efforts, utilizing MiroFlow to generate high-quality, post-training agent trace data. This unique approach enables MiroThinker to excel in planning, executing, and reasoning through complex multi-step tasks.</p>"},{"location":"mirothinker/#deploying-mirothinker-32b-with-miroflow","title":"Deploying MiroThinker-32B with MiroFlow","text":"<p>This guide explains how to deploy the MiroThinker-32B-DPO-v0.2 model from Hugging Face and integrate it with MiroFlow.</p>"},{"location":"mirothinker/#prerequisites","title":"Prerequisites","text":"<ul> <li>SGLang installed</li> <li>Sufficient GPU memory for the model</li> <li>MiroFlow repository set up</li> </ul>"},{"location":"mirothinker/#step-1-deploy-model-with-sglang","title":"Step 1: Deploy Model with SGLang","text":"<p>Deploy the MiroThinker-32B model using SGLang with the following command:</p> <pre><code>python3 -m sglang.launch_server \\\n    --model-path miromind-ai/MiroThinker-32B-DPO-v0.2 \\\n    --tp 8 \\\n    --dp 1 \\\n    --host 0.0.0.0 \\\n    --port 61005 \\\n    --trust-remote-code \\\n    --chat-template qwen3_nonthinking.jinja\n</code></pre> <p>Important Notes: - Adjust the <code>--tp</code> (tensor parallelism) parameter to match your number of GPUs - Download the chat template from: qwen3_nonthinking.jinja - Ensure the port you used (in this case 61005) is available on your system</p>"},{"location":"mirothinker/#step-2-configure-miroflow","title":"Step 2: Configure MiroFlow","text":"<p>Once the SGLang server is running, configure MiroFlow by adding the following to your <code>.env</code> file:</p> <pre><code>OAI_MIROTHINKER_API_KEY=\"dummy_key\"\nOAI_MIROTHINKER_BASE_URL=\"http://localhost:61005/v1\"\n</code></pre> <p>Note:  - If your model requires authentication, replace <code>dummy_key</code> with your actual API key - Replace <code>localhost</code> with the appropriate hostname if deploying on a remote server</p>"},{"location":"mirothinker/#step-3-test-the-integration","title":"Step 3: Test the Integration","text":"<p>Test your setup with the following command:</p> <pre><code>uv run main.py trace --config_file_name=agent_mirothinker \\\n    --task=\"What is the first country listed in the XLSX file that have names starting with Co?\" \\\n    --task_file_name=\"data/FSI-2023-DOWNLOAD.xlsx\"\n</code></pre> <p>This command will: - Use the <code>agent_mirothinker</code> configuration with the dedicated MiroThinkerSGLangClient - Process the specified Excel file - Query the model to find countries starting with \"Co\"</p>"},{"location":"mirothinker/#configuration-details","title":"Configuration Details","text":"<p>The <code>./config/agent_mirothinker.yaml</code> configuration file uses: - <code>provider_class: \"MiroThinkerSGLangClient\"</code> - A dedicated client for MiroThinker models deployed with SGLang - Model path and generation parameters (temperature, top_p, max_tokens, etc.) - Environment variables for API endpoint configuration</p> <p>Last Updated: Sep 2025 Doc Contributor: Xalp @MiroMind AI</p>"},{"location":"openai-gpt/","title":"OpenAI GPT Models","text":""},{"location":"openai-gpt/#what-this-is","title":"What This Is","text":"<p>OpenAI's latest models including GPT-4o and O3 reasoning models with strong coding, vision, and reasoning capabilities.</p>"},{"location":"openai-gpt/#client-used","title":"Client Used","text":"<p><code>GPTOpenAIClient</code></p>"},{"location":"openai-gpt/#environment-setup","title":"Environment Setup","text":"<pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # optional\n</code></pre>"},{"location":"openai-gpt/#configuration","title":"Configuration","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"GPTOpenAIClient\"\n    model_name: \"gpt-4o\"  # or o3, etc.\n    openai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n    openai_base_url: \"${oc.env:OPENAI_BASE_URL,https://api.openai.com/v1}\"\n    ...\n</code></pre>"},{"location":"openai-gpt/#usage","title":"Usage","text":"<pre><code># Create custom OpenAI config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"openrouter-claude-3.7-sonnet/","title":"OpenRouter Claude 3.7 Sonnet (Recommended)","text":""},{"location":"openrouter-claude-3.7-sonnet/#what-this-is","title":"What This Is","text":"<p>Access multiple models via OpenRouter using unified OpenAI chat format. Supports Claude, GPT, and other models with higher rate limits.</p>"},{"location":"openrouter-claude-3.7-sonnet/#client-used","title":"Client Used","text":"<p><code>ClaudeOpenRouterClient</code></p>"},{"location":"openrouter-claude-3.7-sonnet/#environment-setup","title":"Environment Setup","text":"<pre><code>export OPENROUTER_API_KEY=\"your-openrouter-key\"\nexport OPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"  # optional\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#configuration","title":"Configuration","text":"<pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"  # or openai/gpt-4, etc.\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n    openrouter_base_url: \"${oc.env:OPENROUTER_BASE_URL,https://openrouter.ai/api/v1}\"\n    openrouter_provider: \"anthropic\"  # Force provider, or \"\" for auto\n    ...\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#other-supported-models","title":"Other Supported Models","text":"<ul> <li><code>openai/gpt-4</code></li> <li><code>openai/gpt-3.5-turbo</code></li> <li><code>anthropic/claude-3-opus</code></li> <li><code>google/gemini-pro</code></li> <li>Many others via unified OpenAI format</li> </ul>"},{"location":"openrouter-claude-3.7-sonnet/#usage","title":"Usage","text":"<pre><code># Use existing OpenRouter config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#benefits-vs-direct-api","title":"Benefits vs Direct API","text":"<ul> <li>Unified chat format</li> <li>Higher rate limits</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"quickstart/","title":"\ud83d\ude80 Get Started in Under 5 Minutes","text":"<p>Clone the repository, configure your API key, and run your first intelligent agent. You'll just need one <code>OPENROUTER_API_KEY</code>.</p>"},{"location":"quickstart/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>System Requirements</p> <ul> <li>Python: 3.12 or higher</li> <li>Package Manager: <code>uv</code>, https://docs.astral.sh/uv/</li> <li>Operating System: Linux, macOS</li> </ul>"},{"location":"quickstart/#quick-setup","title":"\u26a1 Quick Setup","text":""},{"location":"quickstart/#example-1-intelligent-document-analysis-with-file-processing-capabilities","title":"Example 1: Intelligent document analysis with file processing capabilities","text":"<p>File Processing Demo</p> <p>This example demonstrates MiroFlow's document analysis capabilities.</p> Setup Commands<pre><code># 1. Clone and setup\ngit clone https://github.com/MiroMindAI/MiroFlow &amp;&amp; cd MiroFlow\nuv sync\n\n# 2. Configure API key\ncp .env.template .env\n# Edit .env and add your OPENROUTER_API_KEY\n\n# 3. Run your first agent\nuv run main.py trace --config_file_name=agent_quickstart_1 --task=\"What is the first country listed in the XLSX file that have names starting with Co?\" --task_file_name=\"data/FSI-2023-DOWNLOAD.xlsx\"\n</code></pre> <p>Expected Output</p> <p>\ud83c\udf89 Expected Output: Your agent should return \\boxed{Congo Democratic Republic} \ud83d\ude0a</p> <p>Troubleshooting</p> <p>\ud83d\udca1 Tip: If you encounter issues, check that your API key is correctly set in the <code>.env</code> file and that all dependencies are installed.</p> <p>Coming Soon</p> <p>Coming Soon: We will add a video demo for this example</p>"},{"location":"quickstart/#example-2-web-research-and-multi-agent-orchestration","title":"Example 2: Web research and multi-agent orchestration","text":"<p>Work in Progress</p> <p>The example is not complete yet, to be completed</p> Web Research Command<pre><code>uv run main.py trace --config_file_name=agent_quickstart_2 --task=\"What is the Nasdaq Composite Index at today?\"\n</code></pre> <p>Coming Soon</p> <p>Coming Soon: Web research and multi-agent orchestration example</p> <p>Documentation Info</p> <p>Last Updated: Sep 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_overview/","title":"\ud83d\udd27 Tools","text":""},{"location":"tool_overview/#-coming-soon-","title":"- Coming Soon -","text":"<p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_python/","title":"Python Tools (<code>python_server.py</code>)","text":"<p>The Python Execution Server provides a secure sandboxed environment for running Python code and shell commands using E2B server.</p>"},{"location":"tool_python/#create_sandbox","title":"<code>create_sandbox()</code>","text":"<p>Creates a Linux sandbox for safely executing commands and running Python code.</p> <p>Returns: - <code>str</code>: The <code>sandbox_id</code> of the newly created sandbox</p> <p>Usage Notes: - This tool must be called before using other tools within this MCP server file - The sandbox may timeout and automatically shut down - The sandbox comes pre-installed with common packages for data science and document processing. For a detailed list and advanced usage information, see E2B Extention</p>"},{"location":"tool_python/#run_commandsandbox_id-str-command-str","title":"<code>run_command(sandbox_id: str, command: str)</code>","text":"<p>Execute shell commands in the Linux sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox (must be created first) - <code>command</code>: Shell command to execute</p> <p>Returns: - <code>str</code>: Command execution result (stderr, stdout, exit_code, error)</p> <p>Features: - Automatic retry mechanism - Permission hints for sudo commands</p>"},{"location":"tool_python/#run_python_codesandbox_id-str-code_block-str","title":"<code>run_python_code(sandbox_id: str, code_block: str)</code>","text":"<p>Run Python code in the sandbox and return execution results.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>code_block</code>: Python code to execute</p> <p>Returns: - <code>str</code>: Code execution result (stderr, stdout, exit_code, error)</p> <p>Features: - Automatic retry mechanism</p>"},{"location":"tool_python/#upload_file_from_local_to_sandboxsandbox_id-str-local_file_path-str-sandbox_file_path-str-homeuser","title":"<code>upload_file_from_local_to_sandbox(sandbox_id: str, local_file_path: str, sandbox_file_path: str = \"/home/user\")</code>","text":"<p>Upload local files to the sandbox environment.</p> <p>When a local file is provided to the agent, the agent needs to call this tool to copy the file from local storage to the sandbox for further file processing.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>local_file_path</code>: Local path of the file to upload - <code>sandbox_file_path</code>: Target directory in sandbox (default: <code>/home/user</code>)</p> <p>Returns: - <code>str</code>: Path of uploaded file in sandbox or error message</p>"},{"location":"tool_python/#download_file_from_internet_to_sandboxsandbox_id-str-url-str-sandbox_file_path-str-homeuser","title":"<code>download_file_from_internet_to_sandbox(sandbox_id: str, url: str, sandbox_file_path: str = \"/home/user\")</code>","text":"<p>Download files from the internet directly to the sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>url</code>: URL of the file to download - <code>sandbox_file_path</code>: Target directory in sandbox (default: <code>/home/user</code>)</p> <p>Returns: - <code>str</code>: Path of downloaded file in sandbox or error message</p> <p>Features: - Automatic retry mechanism</p>"},{"location":"tool_python/#download_file_from_sandbox_to_localsandbox_id-str-sandbox_file_path-str-local_filename-str-none","title":"<code>download_file_from_sandbox_to_local(sandbox_id: str, sandbox_file_path: str, local_filename: str = None)</code>","text":"<p>Download files from sandbox to local system for processing by other tools.</p> <p>Other MCP tools (such as visual question answering) cannot access files in a sandbox. Therefore, this tool should be called when the agent wants other tools to analyze files in the sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the sandbox - <code>sandbox_file_path</code>: Path of file in sandbox - <code>local_filename</code>: Optional local filename (uses original if not provided)</p> <p>Returns: - <code>str</code>: Local path of downloaded file or error message</p> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_reasoning/","title":"Reasoning Tools (<code>reasoning_mcp_server.py</code>)","text":"<p>The Reasoning MCP Server provides a pure text-based reasoning engine. It supports logical analysis, problem solving, and planning, using LLM backends (OpenAI or Anthropic) with retry and exponential backoff for robustness.</p>"},{"location":"tool_reasoning/#environment-variables","title":"Environment Variables","text":"<p>Where to Modify</p> <p>The <code>reasoning_mcp_server.py</code> reads environment variables that are passed through the <code>tool-reasoning.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <ul> <li> <p>OpenAI Configuration:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>OPENAI_BASE_URL</code> : default = <code>https://api.openai.com/v1</code></li> <li><code>OPENAI_MODEL_NAME</code> : default = <code>o3</code></li> </ul> </li> <li> <p>Anthropic Configuration:</p> <ul> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>ANTHROPIC_BASE_URL</code> : default = <code>https://api.anthropic.com</code></li> <li><code>ANTHROPIC_MODEL_NAME</code> : default = <code>claude-3-7-sonnet-20250219</code></li> </ul> </li> </ul>"},{"location":"tool_reasoning/#reasoningquestion-str","title":"<code>reasoning(question: str)</code>","text":"<p>Perform step-by-step reasoning, analysis, and planning over a text-only input. This tool is specialized for complex thinking tasks.</p> <p>Parameters</p> <ul> <li><code>question</code>:  A detailed, complex question or problem statement that includes all necessary information. The tool will not fetch external data or context.</li> </ul> <p>Returns</p> <ul> <li><code>str</code>: A structured, step-by-step reasoned answer.</li> </ul> <p>Features</p> <ul> <li>Runs on OpenAI or Anthropic models, depending on available API keys.</li> <li>Exponential backoff retry logic (up to 5 attempts).</li> <li>For Anthropic, uses Thinking mode with token budget (21k max, 19k thinking).</li> <li>Ensures non-empty responses with fallback error reporting.</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_searching/","title":"Searching Tools (<code>searching_mcp_server.py</code>)","text":"<p>The Searching MCP Server provides comprehensive search capabilities including Google search, Wikipedia content retrieval, archive searching, and web scraping functionality.</p>"},{"location":"tool_searching/#environment-variables-used-in-tools","title":"Environment Variables Used in Tools","text":"<ul> <li><code>SERPER_API_KEY</code>: Required API key for Serper service, Used by <code>google_search</code> and as a fallback for <code>scrape_website</code></li> <li><code>JINA_API_KEY</code>: Required API key for JINA service. Default choice for scraping websites in <code>scrape_website</code></li> <li><code>REMOVE_SNIPPETS</code>: Set to \"true\" to filter out snippets from results. Used in <code>google_search</code> to filter the search results returned by Serper.</li> <li><code>REMOVE_KNOWLEDGE_GRAPH</code>: Set to \"true\" to remove knowledge graph data. Used in <code>google_search</code> to filter the search results returned by Serper.</li> <li><code>REMOVE_ANSWER_BOX</code>: Set to \"true\" to remove answer box content. Used in <code>google_search</code> to filter the search results returned by Serper.</li> </ul>"},{"location":"tool_searching/#google_searchq-str-gl-str-us-hl-str-en-location-str-none-num-int-10-tbs-str-none-page-int-1","title":"<code>google_search(q: str, gl: str = \"us\", hl: str = \"en\", location: str = None, num: int = 10, tbs: str = None, page: int = 1)</code>","text":"<p>Perform Google searches via Serper API and retrieve rich search results including organic results, people also ask, related searches, and knowledge graph.</p> <p>Parameters:</p> <ul> <li><code>q</code>: Search query string</li> <li><code>gl</code>: Country context for search (e.g., 'us' for United States, 'cn' for China, 'uk' for United Kingdom). Default: 'us'</li> <li><code>hl</code>: Google interface language (e.g., 'en' for English, 'zh' for Chinese, 'es' for Spanish). Default: 'en'</li> <li><code>location</code>: City-level location for search results (e.g., 'SoHo, New York, United States', 'California, United States')</li> <li><code>num</code>: Number of results to return. Default: 10</li> <li><code>tbs</code>: Time-based search filter ('qdr:h' for past hour, 'qdr:d' for past day, 'qdr:w' for past week, 'qdr:m' for past month, 'qdr:y' for past year)</li> <li><code>page</code>: Page number of results to return. Default: 1</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: JSON formatted search results with organic results and related information</li> </ul> <p>Features:</p> <ul> <li>Automatic retry mechanism (up to 5 attempts)</li> <li>Configurable result filtering via environment variables</li> <li>Support for regional and language-specific searches</li> </ul>"},{"location":"tool_searching/#wiki_get_page_contententity-str-first_sentences-int-10","title":"<code>wiki_get_page_content(entity: str, first_sentences: int = 10)</code>","text":"<p>Get specific Wikipedia page content for entities (people, places, concepts, events) and return structured information.</p> <p>Parameters:</p> <ul> <li><code>entity</code>: The entity to search for in Wikipedia</li> <li><code>first_sentences</code>: Number of first sentences to return from the page. Set to 0 to return full content. Default: 10</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted content containing page title, introduction/full content, and URL</li> </ul> <p>Features:</p> <ul> <li>Handles disambiguation pages automatically</li> <li>Provides clean, structured output</li> <li>Fallback search suggestions when page not found</li> <li>Automatic content truncation for manageable output</li> </ul>"},{"location":"tool_searching/#search_wiki_revisionentity-str-year-int-month-int-max_revisions-int-50","title":"<code>search_wiki_revision(entity: str, year: int, month: int, max_revisions: int = 50)</code>","text":"<p>Search for an entity in Wikipedia and return the revision history for a specific month.</p> <p>Parameters:</p> <ul> <li><code>entity</code>: The entity to search for in Wikipedia</li> <li><code>year</code>: The year of the revision (e.g., 2024)</li> <li><code>month</code>: The month of the revision (1-12)</li> <li><code>max_revisions</code>: Maximum number of revisions to return. Default: 50</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted revision history with timestamps, revision IDs, and URLs</li> </ul> <p>Features:</p> <ul> <li>Automatic date validation and adjustment</li> <li>Support for date range from 2000 to current year</li> <li>Detailed revision metadata including timestamps and direct links</li> <li>Clear error handling for invalid dates or missing pages</li> </ul>"},{"location":"tool_searching/#search_archived_webpageurl-str-year-int-month-int-day-int","title":"<code>search_archived_webpage(url: str, year: int, month: int, day: int)</code>","text":"<p>Search the Wayback Machine (archive.org) for archived versions of a webpage for a specific date.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL to search for in the Wayback Machine</li> <li><code>year</code>: The target year (e.g., 2023)</li> <li><code>month</code>: The target month (1-12)</li> <li><code>day</code>: The target day (1-31)</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted archive information including archived URL, timestamp, and availability status</li> </ul> <p>Features:</p> <ul> <li>Automatic URL protocol detection and correction</li> <li>Date validation and adjustment (1995 to present)</li> <li>Fallback to most recent archive if specific date not found</li> <li>Special handling for Wikipedia URLs with tool suggestions</li> <li>Automatic retry mechanism for reliable results</li> </ul>"},{"location":"tool_searching/#scrape_websiteurl-str","title":"<code>scrape_website(url: str)</code>","text":"<p>Scrape website content including support for regular websites and YouTube video information.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL of the website to scrape</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Scraped website content including text, metadata, and structured information</li> </ul> <p>Features:</p> <ul> <li>Support for various website types</li> <li>YouTube video information extraction (subtitles, titles, descriptions, key moments)</li> <li>Automatic content parsing and cleaning</li> <li>Integration with Jina API for enhanced scraping capabilities</li> </ul> <p>Usage Notes:</p> <ul> <li>Search engines are not supported by this tool</li> <li>For YouTube videos, provides non-visual information only</li> <li>Content may be incomplete for some complex websites</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_vqa/","title":"Vision Tools (<code>vision_mcp_server.py</code>)","text":"<p>The Vision MCP Server enables OCR + Visual Question Answering (VQA) over images and multimodal understanding of YouTube videos, with pluggable backends (Anthropic, OpenAI, Google Gemini).</p>"},{"location":"tool_vqa/#environment-variables","title":"Environment Variables","text":"<p>Where to Modify</p> <p>The <code>vision_mcp_server.py</code> reads environment variables that are passed through the <code>tool-image-video.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <ul> <li>Vision Backend Control:<ul> <li><code>ENABLE_CLAUDE_VISION</code>: <code>\"true\"</code> to allow Anthropic Vision backend.</li> <li><code>ENABLE_OPENAI_VISION</code>: <code>\"true\"</code> to allow OpenAI Vision backend.</li> </ul> </li> <li>Anthropic Configuration:<ul> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>ANTHROPIC_BASE_URL</code> : default = <code>https://api.anthropic.com</code></li> <li><code>ANTHROPIC_MODEL_NAME</code> : default = <code>claude-3-7-sonnet-20250219</code></li> </ul> </li> <li>OpenAI Configuration:<ul> <li><code>OPENAI_API_KEY</code></li> <li><code>OPENAI_BASE_URL</code> : default = <code>https://api.openai.com/v1</code></li> <li><code>OPENAI_MODEL_NAME</code> : default = <code>gpt-4o</code></li> </ul> </li> <li>Gemini Configuration:<ul> <li><code>GEMINI_API_KEY</code></li> <li><code>GEMINI_MODEL_NAME</code> : default = <code>gemini-2.5-pro</code></li> </ul> </li> </ul>"},{"location":"tool_vqa/#visual_question_answeringimage_path_or_url-str-question-str","title":"<code>visual_question_answering(image_path_or_url: str, question: str)</code>","text":"<p>Ask questions about an image. Runs two passes:</p> <ol> <li> <p>OCR pass using the selected vision backend with a meticulous extraction prompt.</p> </li> <li> <p>VQA pass that analyzes the image and cross-checks against OCR text.</p> </li> </ol> <p>Parameters</p> <ul> <li><code>image_path_or_url</code>: Local path (accessible to server) or web URL. HTTP URLs are auto-upgraded/validated to HTTPS for some backends.</li> <li><code>question</code>: The user\u2019s question about the image.</li> </ul> <p>Returns</p> <ul> <li><code>str</code>: Concatenated text with:<ul> <li><code>OCR results: ...</code></li> <li><code>VQA result: ...</code></li> </ul> </li> </ul> <p>Features</p> <ul> <li>Automatic MIME detection, reads magic bytes, falls back to extension, final default is <code>image/jpeg</code>.</li> </ul>"},{"location":"tool_vqa/#visual_audio_youtube_analyzingurl-str-question-str-provide_transcribe-bool-false","title":"<code>visual_audio_youtube_analyzing(url: str, question: str = \"\", provide_transcribe: bool = False)</code>","text":"<p>Analyze public YouTube videos (audio + visual). Supports watch pages, Shorts, and Live VODs.</p> <ul> <li>Accepted URL patterns: <code>youtube.com/watch</code>, <code>youtube.com/shorts</code>, <code>youtube.com/live</code>.</li> </ul> <p>Parameters</p> <ul> <li><code>url</code>: YouTube video URL (publicly accessible).</li> <li><code>question</code> (optional): A specific question about the video. You can scope by time using <code>MM:SS</code> or <code>MM:SS-MM:SS</code> (e.g., <code>01:45</code>, <code>03:20-03:45</code>).</li> <li><code>provide_transcribe</code> (optional, default <code>False</code>): If <code>True</code>, returns a timestamped transcription including salient events and brief visual descriptions.</li> </ul> <p>Returns</p> <ul> <li><code>str</code>: transcription of the video (if asked) and answer to the question.</li> </ul> <p>Features</p> <ul> <li>Gemini-powered video analysis (requires <code>GEMINI_API_KEY</code>).</li> <li>Dual mode: full transcript, targeted Q&amp;A, or both.</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"yaml_config/","title":"\ud83d\udccb YAML Configuration Guide","text":"<p>MiroFlow uses a flexible Hydra-based configuration system that allows you to customize every aspect of your AI agents. This guide explains all configuration features and how to use them effectively.</p>"},{"location":"yaml_config/#configuration-structure","title":"\ud83c\udfd7\ufe0f Configuration Structure","text":"<p>Current File Organization</p> <p>The configuration system is organized hierarchically for easy management.</p> Configuration Directory Structure<pre><code>config/\n\u251c\u2500\u2500 agent_quickstart_1.yaml       # Quick start configuration\n\u251c\u2500\u2500 agent_gaia-validation.yaml    # GAIA benchmark configuration  \n\u251c\u2500\u2500 agent_mirothinker.yaml        # MiroThinker model configuration\n\u251c\u2500\u2500 agent_prompts/                # Agent prompt classes\n\u2502   \u251c\u2500\u2500 main_agent_prompt_gaia.py # GAIA-specific prompts\n\u2502   \u251c\u2500\u2500 main_boxed_answer.py      # Boxed answer extraction\n\u2502   \u2514\u2500\u2500 sub_worker.py            # Sub-agent prompts\n\u251c\u2500\u2500 benchmark/                    # Benchmark configurations\n\u2502   \u251c\u2500\u2500 default.yaml              # Default benchmark settings\n\u2502   \u2514\u2500\u2500 gaia-validation.yaml      # GAIA validation benchmark\n\u251c\u2500\u2500 tool/                         # Tool configurations\n\u2502   \u251c\u2500\u2500 tool-reasoning.yaml       # Enhanced reasoning tool\n\u2502   \u251c\u2500\u2500 tool-searching.yaml       # Web search capabilities\n\u2502   \u251c\u2500\u2500 tool-reading.yaml         # Document processing\n\u2502   \u251c\u2500\u2500 tool-code.yaml           # Code execution\n\u2502   \u251c\u2500\u2500 tool-image-video.yaml    # Visual content analysis\n\u2502   \u2514\u2500\u2500 tool-audio.yaml          # Audio processing\n\u2514\u2500\u2500 no-in-use-*/                  # Archive of legacy configurations\n</code></pre>"},{"location":"yaml_config/#quick-start-usage","title":"\ud83d\ude80 Quick Start Usage","text":"<p>Basic Usage Examples</p> <p>Simple Agent Execution</p> Basic Command<pre><code># Run with a specific agent configuration\nuv run main.py trace --config_file_name=agent_quickstart_1 --task=\"Your task here\"\n\n# Run with file input\nuv run main.py trace --config_file_name=agent_quickstart_1 --task=\"Analyze this file\" --task_file_name=\"data/file.xlsx\"\n</code></pre> <p>Parameter Override</p> Dynamic Configuration<pre><code># Override specific parameters on the fly\nuv run main.py trace --config_file_name=agent_gaia-validation \\\n    main_agent.llm.temperature=0.1 \\\n    main_agent.max_turns=30 \\\n    --task=\"Your task\"\n</code></pre>"},{"location":"yaml_config/#configuration-features","title":"\u2699\ufe0f Configuration Features","text":""},{"location":"yaml_config/#agent-configuration","title":"\ud83e\udd16 Agent Configuration","text":"<p>Main Agent Settings</p> <p>Core Configuration Options</p> <ul> <li><code>prompt_class</code>: Defines the prompt template and behavior</li> <li><code>MainAgentPromptBoxedAnswer</code>: Basic boxed answer extraction</li> <li><code>MainAgentPrompt_GAIA</code>: GAIA benchmark optimized prompts</li> <li><code>llm</code>: Language model configuration (inline or reference)</li> <li><code>tool_config</code>: List of tools available to the main agent</li> <li><code>max_turns</code>: Maximum conversation turns (-1 = unlimited)</li> <li><code>max_tool_calls_per_turn</code>: Limit tool calls per turn (default: 10)</li> </ul> <p>Sub-Agent Configuration</p> <ul> <li><code>agent-worker</code>: General-purpose sub-agent with comprehensive tools</li> <li>Individual LLM settings: Each sub-agent can use different models</li> <li>Specialized tool sets: Customize tools per sub-agent role</li> </ul> <p>Advanced Features</p> <ul> <li><code>add_message_id</code>: Add unique IDs to messages for tracking</li> <li><code>chinese_context</code>: Enable Chinese language optimization</li> <li><code>keep_tool_result</code>: Control tool result retention (-1 = keep all)</li> </ul>"},{"location":"yaml_config/#llm-configuration","title":"\ud83d\udd27 LLM Configuration","text":"<p>LLM Provider Settings</p> <p>Provider Configuration Example</p> LLM Configuration<pre><code>llm:\n  provider_class: \"ClaudeOpenRouterClient\"  # or \"MiroThinkerSGLangClient\"\n  model_name: \"anthropic/claude-3.7-sonnet\"\n  temperature: 0.3\n  max_tokens: 32000\n  async_client: true\n</code></pre> <p>Available Providers</p> <ul> <li>Claude (OpenRouter): <code>ClaudeOpenRouterClient</code></li> <li>MiroThinker: <code>MiroThinkerSGLangClient</code></li> <li>OpenAI: <code>GPTOpenAIClient</code></li> <li>DeepSeek: <code>DeepSeekNewAPIClient</code></li> </ul> <p>Model Parameters</p> <ul> <li><code>temperature</code>: Creativity/randomness (0.0-1.0)</li> <li><code>top_p</code>: Nucleus sampling parameter</li> <li><code>max_tokens</code>: Maximum response length</li> <li><code>top_k</code>: Top-k sampling (-1 = disabled)</li> </ul>"},{"location":"yaml_config/#tool-configuration","title":"\ud83d\udee0\ufe0f Tool Configuration","text":"<p>Available Tools &amp; Environment Setup</p> <p>Available Tools</p> <ul> <li><code>tool-reasoning</code>: Enhanced reasoning with high-quality models</li> <li><code>tool-searching</code>: Web search with Google/Serper integration</li> <li><code>tool-reading</code>: Document processing (PDF, DOCX, TXT, etc.)</li> <li><code>tool-code</code>: Python code execution in E2B sandbox</li> <li><code>tool-image-video</code>: Visual content analysis</li> <li><code>tool-audio</code>: Audio transcription and processing</li> </ul> <p>Tool Environment Variables</p> tool-searching.yaml<pre><code># tool-searching.yaml\nenv:\n  SERPER_API_KEY: \"${oc.env:SERPER_API_KEY}\"\n  JINA_API_KEY: \"${oc.env:JINA_API_KEY}\"\n  REMOVE_SNIPPETS: \"${oc.env:REMOVE_SNIPPETS,false}\"\n</code></pre>"},{"location":"yaml_config/#processing-features","title":"\ud83c\udfaf Processing Features","text":"<p>Advanced Processing Options</p> <p>Input Processing</p> <ul> <li><code>o3_hint</code>: Use O3 model for task hint generation</li> <li>Advanced prompt engineering for better task understanding</li> </ul> <p>Output Processing</p> <ul> <li><code>o3_final_answer</code>: Use O3 model for final answer extraction</li> <li>Boxed answer format for benchmark compliance</li> <li>Intelligent result synthesis</li> </ul>"},{"location":"yaml_config/#creating-custom-configurations","title":"\ud83d\udd27 Creating Custom Configurations","text":"<p>Configuration Examples</p> Basic Custom AgentMulti-Tool ConfigurationEnvironment-Specific Settings Custom Agent Configuration<pre><code>defaults:\n  - benchmark: gaia-validation\n  - override hydra/job_logging: none\n  - _self_\n\nmain_agent:\n  prompt_class: MainAgentPromptBoxedAnswer\n  llm:\n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.5  # Custom temperature\n  tool_config:\n    - tool-reasoning  # Add reasoning capability\n  max_turns: 15      # Custom turn limit\n\nsub_agents:\n  agent-worker:\n    prompt_class: SubAgentWorkerPrompt\n    tool_config:\n      - tool-reading   # Document processing only\n    max_turns: 10\n</code></pre> Advanced Tool Setup<pre><code>main_agent:\n  tool_config:\n    - tool-reasoning    # Enhanced reasoning\n    - tool-searching    # Web search\n\nsub_agents:\n  agent-worker:\n    tool_config:\n      - tool-reading    # Document processing\n      - tool-code       # Code execution\n      - tool-image-video # Visual analysis\n      - tool-audio      # Audio processing\n</code></pre> Environment Configurations<pre><code># Development configuration\nmain_agent:\n  llm:\n    temperature: 0.7     # Higher creativity for exploration\n  max_turns: -1          # Unlimited turns\n  add_message_id: true   # Enable debugging\n\n# Production configuration  \nmain_agent:\n  llm:\n    temperature: 0.3     # Lower temperature for consistency\n  max_turns: 20          # Controlled turn limit\n  add_message_id: false  # Disable for performance\n</code></pre>"},{"location":"yaml_config/#benchmark-integration","title":"\ud83d\udcca Benchmark Integration","text":"<p>Benchmark Configuration Examples</p> GAIA ValidationCustom Benchmark GAIA Benchmark Configuration<pre><code>defaults:\n  - benchmark: gaia-validation\n\nmain_agent:\n  prompt_class: MainAgentPrompt_GAIA\n  input_process:\n    o3_hint: true          # Use O3 hints for better performance\n  output_process:\n    o3_final_answer: true  # Extract answers with O3\n</code></pre> config/benchmark/my-benchmark.yaml<pre><code># config/benchmark/my-benchmark.yaml\nname: \"my-benchmark\"\ndata:\n  data_dir: \"${oc.env:DATA_DIR,data}/my-data\"\nexecution:\n  max_tasks: 100         # Limit task count\n  max_concurrent: 5      # Parallel execution\n  pass_at_k: 1          # Success criteria\n</code></pre>"},{"location":"yaml_config/#script-based-execution","title":"\ud83d\udcdc Script-Based Execution","text":"<p>Script-Based Approach</p> <p>We recommend using the scripts provided under <code>./scripts/</code>, as script files are much easier to read, maintain, and customize compared to single-line commands.</p> <p>Benchmark Evaluation Script</p> scripts/benchmark_evaluation.sh<pre><code>#!/bin/bash\n\n# Configuration\nNUM_RUNS=3                              # Number of parallel runs\nMAX_CONCURRENT=10                       # Concurrent tasks per run\nBENCHMARK_NAME=\"gaia-validation\"        # Benchmark configuration\nAGENT_CONFIG=\"agent_gaia-validation\"    # Agent configuration\nADD_MESSAGE_ID=\"true\"                   # Enable message tracking\nMAX_TURNS=-1                           # Unlimited turns (-1)\n\n# Auto-detect Chinese benchmarks\nif [[ $BENCHMARK_NAME == \"xbench-ds\" ]] || [[ $BENCHMARK_NAME == \"browsecomp-zh\" ]]; then\n    export CHINESE_CONTEXT=\"true\"\n    echo \"Detected Chinese benchmark, enabling Chinese context\"\nfi\n\n# Google search filtering (optional)\n# export REMOVE_SNIPPETS=\"true\"\n# export REMOVE_KNOWLEDGE_GRAPH=\"true\" \n# export REMOVE_ANSWER_BOX=\"true\"\n\nRESULTS_DIR=\"logs/${BENCHMARK_NAME}/${AGENT_CONFIG}\"\necho \"Starting evaluation with $NUM_RUNS parallel runs...\"\n\n# Launch parallel experiments\nfor i in $(seq 1 $NUM_RUNS); do\n    RUN_ID=\"run_$i\"\n    (\n        uv run main.py trace \\\n            --config_file_name=$AGENT_CONFIG \\\n            --benchmark_name=$BENCHMARK_NAME \\\n            --max_concurrent=$MAX_CONCURRENT \\\n            --output_dir=\"$RESULTS_DIR/$RUN_ID\" \\\n            &gt; \"$RESULTS_DIR/${RUN_ID}_output.log\" 2&gt;&amp;1\n    ) &amp;\n    sleep 2\ndone\n\nwait\necho \"All runs completed! Check results in: $RESULTS_DIR\"\n</code></pre>"},{"location":"yaml_config/#advanced-configuration-tips","title":"\ud83d\udd27 Advanced Configuration Tips","text":"<p>Environment Variable Management</p> .env file example<pre><code># .env file example\nOPENROUTER_API_KEY=your_key_here\nSERPER_API_KEY=your_serper_key\nJINA_API_KEY=your_jina_key\nE2B_API_KEY=your_e2b_key\n\n# Optional: Customize search behavior\nREMOVE_SNIPPETS=false\nREMOVE_KNOWLEDGE_GRAPH=false\nREMOVE_ANSWER_BOX=false\nCHINESE_CONTEXT=false\n</code></pre> <p>Configuration Categories</p> Configuration ValidationPerformance OptimizationDebugging Configuration Validation Guidelines<pre><code># Add validation to your configs\nmain_agent:\n  llm:\n    temperature: 0.3        # Must be 0.0-1.0\n    max_tokens: 32000       # Model-specific limits\n  max_turns: 20             # Positive integer or -1\n  tool_config:              # Must be valid tool names\n    - tool-reasoning\n</code></pre> High-Performance Settings<pre><code># High-performance configuration\nmain_agent:\n  llm:\n    async_client: true      # Enable async processing\n    max_tokens: 8192        # Reasonable token limit\n  max_tool_calls_per_turn: 5  # Limit tool calls for speed\n\nbenchmark:\n  execution:\n    max_concurrent: 15      # Parallel execution\n    pass_at_k: 1           # Single attempt per task\n</code></pre> Debug-Friendly Settings<pre><code># Debug-friendly settings\nmain_agent:\n  add_message_id: true      # Track message flow\n  keep_tool_result: -1      # Keep all tool results\n  max_turns: -1             # Unlimited exploration\n\nsub_agents:\n  agent-worker:\n    max_turns: -1           # Unlimited sub-agent turns\n</code></pre>"},{"location":"yaml_config/#configuration-best-practices","title":"\ud83c\udfaf Configuration Best Practices","text":"<p>Best Practices Guide</p> <p>1. Start Simple</p> <p>Begin with basic configurations like <code>agent_quickstart_1.yaml</code> and add complexity gradually.</p> <p>2. Environment Separation</p> <ul> <li>Development: Higher temperature, unlimited turns, debugging enabled</li> <li>Testing: Moderate settings with turn limits</li> <li>Production: Conservative settings, optimized for performance</li> </ul> <p>3. Tool Selection</p> <p>Choose tools based on your use case:</p> <ul> <li>Research Tasks: <code>tool-searching</code> + <code>tool-reasoning</code></li> <li>Document Analysis: <code>tool-reading</code> + <code>tool-reasoning</code></li> <li>Code Tasks: <code>tool-code</code> + <code>tool-reasoning</code></li> <li>Multimedia: <code>tool-image-video</code> + <code>tool-audio</code></li> </ul> <p>4. Resource Management</p> <ul> <li>Monitor <code>max_concurrent</code> to avoid API rate limits</li> <li>Set reasonable <code>max_tokens</code> for cost control</li> <li>Use <code>max_turns</code> to prevent infinite loops</li> </ul> <p>5. API Key Security</p> <ul> <li>Always use environment variables for API keys</li> <li>Never commit API keys to version control</li> <li>Use different keys for development and production</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"}]}