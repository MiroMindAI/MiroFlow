{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MiroFlow","text":""},{"location":"#what-is-miroflow","title":"\ud83d\ude80 What is MiroFlow?","text":"<p>MiroFlow is a comprehensive agentic foundation platform for building intelligent AI agents that achieve state-of-the-art performance on complex tasks. It provides enhanced conversation management, flexible tool integration, and extensive benchmark evaluations across multiple datasets.</p>"},{"location":"#quick-start","title":"\ud83c\udfaf Quick Start","text":"<p>Ready to get started? Choose your path:</p> <p>Get Started in Minutes</p> Quick SetupEvaluationDevelopment <p>Jump right in with our quickstart guide:</p> <p>Get Started </p> <p>Evaluate existing models on benchmarks:</p> <p>Run Evaluations </p> <p>Build your own AI agents:</p> <p>Core Concepts </p>"},{"location":"#ecosystem","title":"\ud83d\udd17 Ecosystem","text":"<p>Explore the complete MiroMind AI ecosystem:</p> <p>MiroMind AI Products</p> Name Description Link MiroFlow Core framework for AI agent platform Documentation MiroThinker State-of-the-art agent foundation models Hugging Face MiroVerse Curated datasets for model training Dataset MiroTrain Complete training recipes and tools GitHub <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI \u00b7 Version: v0.3</p>"},{"location":"all_about_agents/","title":"\ud83d\udcda All About Agents","text":"<p>Welcome to our comprehensive resource collection for AI agents. This page curates valuable tools, frameworks, research papers, and learning materials to help you understand and build sophisticated agent systems.</p>"},{"location":"all_about_agents/#table-of-contents","title":"Table of Contents","text":"<p>Resource Categories</p> <ol> <li>Agent Frameworks</li> <li>Agent Memory</li> <li>Papers</li> <li>Evaluation</li> </ol>"},{"location":"all_about_agents/#agent-frameworks","title":"Agent Frameworks","text":"<p>Popular Agent Development Frameworks</p> <p>Comprehensive frameworks for building and deploying AI agents across different domains.</p> <ul> <li> <p>MiroFlow: Build, manage, and scale your AI agents with ease</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>OpenManus: No fortress, purely open ground. OpenManus is Coming</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>OpenBB Platform: Financial data platform for analysts, quants and AI agents </p> <ul> <li> Project</li> </ul> </li> </ul>"},{"location":"all_about_agents/#agent-memory","title":"Agent Memory","text":"<p>Memory Systems for Persistent Agent Intelligence</p> <p>Advanced memory solutions for building agents with long-term context and learning capabilities.</p> <ul> <li> <p>Mem0: Building Production- Ready AI Agents with Scalable Long-Term Memory</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>memobase: Profile-Based Long-Term Memory for AI Applications</p> <ul> <li> GitHub</li> </ul> </li> <li> <p>Memento: Fine-tuning LLM Agents without Fine-tuning LLMs</p> <ul> <li> Paper \u00b7  GitHub</li> </ul> </li> </ul>"},{"location":"all_about_agents/#papers","title":"Papers","text":"<p>Research Papers &amp; Publications</p> <p>Latest research in agent systems, methodologies, and theoretical foundations.</p> <ul> <li> <p>Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld </p> <ul> <li> Paper</li> </ul> </li> <li> <p>AFlow: Automating Agentic Workflow Generation </p> <ul> <li> Paper</li> </ul> </li> <li> <p>AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs </p> <ul> <li> Paper</li> </ul> </li> <li> <p>Throttling Web Agents Using Reasoning Gates</p> <ul> <li> Paper</li> </ul> </li> <li> <p>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</p> <ul> <li> Paper</li> </ul> </li> </ul>"},{"location":"all_about_agents/#evaluation","title":"Evaluation","text":"<p>Benchmarks &amp; Evaluation Frameworks</p> <p>Comprehensive evaluation tools and benchmarks for measuring agent performance across various tasks.</p> <ul> <li> <p>LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries </p> <ul> <li> Paper</li> </ul> </li> <li> <p>BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent </p> <ul> <li> Paper</li> </ul> </li> <li> <p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</p> <ul> <li> Paper</li> </ul> </li> <li> <p>GAIA: a benchmark for General AI Assistants </p> <ul> <li> Paper \u00b7  Leaderboard</li> </ul> </li> <li> <p>xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations </p> <ul> <li> Paper</li> </ul> </li> <li> <p>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers </p> <ul> <li> Paper</li> </ul> </li> <li> <p>FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction </p> <ul> <li> Paper</li> </ul> </li> <li> <p>Terminal-Bench: the benchmark for testing AI agents in real terminal environments </p> <ul> <li> GitHub</li> </ul> </li> <li> <p>Gaia2 and ARE: Empowering the Community to Evaluate Agents</p> <ul> <li> Blog Post</li> </ul> </li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"applications/","title":"\ud83d\udcf1 Applications","text":"<p>MiroFlow enables building a wide variety of intelligent applications across different domains and use cases.</p>"},{"location":"applications/#available-applications","title":"\ud83c\udfaf Available Applications","text":"<p>Ready-to-Use Applications</p> <p>Experience MiroFlow's capabilities through these available interfaces and demos.</p>"},{"location":"applications/#gradio-demo","title":"Gradio Demo","text":"<p>Local Development Interface</p> <p>Interactive web interface for testing MiroFlow agents locally. Currently available at MiroThinker Gradio Demo.</p>"},{"location":"applications/#live-demo","title":"Live Demo","text":"<p>Online Experience</p> <p>Experience MiroFlow's capabilities through our online demo for deep research tasks.</p>"},{"location":"applications/#development-status","title":"\ud83d\udd04 Development Status","text":"<p>Integration Progress</p> <p>The MiroThinker model workflows are being integrated into the main MiroFlow framework. This will provide a unified experience for all applications and demos.</p> <p>Stay tuned for updates on application availability and new integrations!</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"claude-3.7-sonnet/","title":"Claude 3.7 Sonnet","text":"<p>Anthropic's Claude 3.7 Sonnet model with 200K context, strong reasoning, and tool use capabilities.</p>"},{"location":"claude-3.7-sonnet/#available-clients","title":"Available Clients","text":""},{"location":"claude-3.7-sonnet/#claudeanthropicclient-direct-api","title":"ClaudeAnthropicClient (Direct API)","text":"<p>Environment Setup:</p> Environment Variables<pre><code>export ANTHROPIC_API_KEY=\"your-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.anthropic.com\"  # optional\n</code></pre> <p>Configuration:</p> Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeAnthropicClient\"\n    model_name: \"claude-3-7-sonnet-20250219\"  # Use actual model name from Anthropic API\n    anthropic_api_key: \"${oc.env:ANTHROPIC_API_KEY,???}\"\n    anthropic_base_url: \"${oc.env:ANTHROPIC_BASE_URL,https://api.anthropic.com}\"\n</code></pre>"},{"location":"claude-3.7-sonnet/#usage","title":"Usage","text":"Example Command<pre><code># Use existing config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_benchmarks/","title":"Contributing New Benchmarks to MiroFlow","text":"<p>This comprehensive guide walks you through adding new evaluation benchmarks to the MiroFlow framework. MiroFlow's modular architecture makes it easy to integrate diverse evaluation datasets while maintaining consistency and reproducibility.</p>"},{"location":"contribute_benchmarks/#overview","title":"Overview","text":"<p>Why Add New Benchmarks?</p> <p>Adding new benchmarks serves multiple purposes:</p> <ul> <li>Internal Testing: Validate your agent's performance on custom tasks and domains specific to your use case</li> <li>Development Iteration: Create targeted test sets to debug and improve specific agent capabilities</li> <li>Domain-Specific Evaluation: Test agents on proprietary or specialized datasets relevant to your application</li> <li>Research Contributions: Expand MiroFlow's benchmark coverage to advance the field with new evaluation paradigms</li> <li>Comparative Analysis: Benchmark your agent against custom baselines or competitors</li> </ul>"},{"location":"contribute_benchmarks/#step-by-step-implementation-guide","title":"Step-by-Step Implementation Guide","text":""},{"location":"contribute_benchmarks/#step-1-prepare-your-dataset","title":"Step 1: Prepare Your Dataset","text":"<p>Your benchmark dataset must follow MiroFlow's standardized structure for seamless integration.</p>"},{"location":"contribute_benchmarks/#required-directory-structure","title":"Required Directory Structure","text":"<pre><code>your-benchmark/\n\u251c\u2500\u2500 standardized_data.jsonl    # Metadata file (required)\n\u251c\u2500\u2500 file1.pdf                  # Optional: Binary files referenced by tasks\n\u251c\u2500\u2500 file2.png                  # Optional: Images, documents, etc.\n\u251c\u2500\u2500 data.csv                   # Optional: Additional data files\n\u2514\u2500\u2500 ...                        # Any other supporting files\n</code></pre>"},{"location":"contribute_benchmarks/#metadata-format-specification","title":"Metadata Format Specification","text":"<p>Each line in <code>standardized_data.jsonl</code> must be a valid JSON object with the following schema:</p> <p>Required Fields</p> <pre><code>{\n  \"task_id\": \"unique_task_identifier\",\n  \"task_question\": \"The question or instruction for the task\",\n  \"ground_truth\": \"The expected answer or solution\",\n  \"file_path\": \"path/to/file.pdf\",  // Optional, can be null\n  \"metadata\": {                     // Optional, can be empty object or other structure\n    \"difficulty\": \"hard\",\n    \"category\": \"reasoning\",\n    \"source\": \"original_dataset_name\"\n  }\n}\n</code></pre>"},{"location":"contribute_benchmarks/#example-tasks","title":"Example Tasks","text":"<p>Simple Text-Only Task: <pre><code>{\n  \"task_id\": \"math_001\",\n  \"task_question\": \"What is the integral of x^2 from 0 to 2?\",\n  \"ground_truth\": \"8/3\",\n  \"file_path\": null,\n  \"metadata\": {\n    \"difficulty\": \"medium\",\n    \"category\": \"calculus\",\n    \"source\": \"custom_math_problems\"\n  }\n}\n</code></pre></p> <p>File-Based Task: <pre><code>{\n  \"task_id\": \"doc_analysis_001\",\n  \"task_question\": \"Based on the provided financial report, what was the company's revenue growth rate?\",\n  \"ground_truth\": \"12.5%\",\n  \"file_path\": \"reports/financial_q3_2023.pdf\",\n  \"metadata\": {\n    \"difficulty\": \"hard\",\n    \"category\": \"document_analysis\",\n    \"file_type\": \"pdf\"\n  }\n}\n</code></pre></p>"},{"location":"contribute_benchmarks/#step-2-create-benchmark-configuration","title":"Step 2: Create Benchmark Configuration","text":"<p>Create a configuration file to define how MiroFlow should handle your benchmark.</p>"},{"location":"contribute_benchmarks/#configuration-file-location","title":"Configuration File Location","text":"<p>Create: <code>config/benchmark/your-benchmark.yaml</code></p>"},{"location":"contribute_benchmarks/#configuration-template","title":"Configuration Template","text":"config/benchmark/your-benchmark.yaml<pre><code># Benchmark configuration for your custom dataset\ndefaults:\n  - default          # Use default benchmark settings\n  - _self_           # Allow overrides in this file\n\nname: \"your-benchmark\"\n\ndata:\n  data_dir: \"${data_dir}/your-benchmark\"        # Dataset location\n  metadata_file: \"standardized_data.jsonl\"     # Metadata filename\n\nexecution:\n  max_tasks: null          # null = no limit, number = max tasks to run\n  max_concurrent: 5        # Number of parallel task executions\n  pass_at_k: 1             # Number of attempts per task for pass@k evaluation\n\n# LLM judge configuration for evaluation\nopenai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n</code></pre>"},{"location":"contribute_benchmarks/#configuration-options","title":"Configuration Options","text":"<p>Execution Parameters</p> <ul> <li>max_tasks: Control dataset size during development (use small numbers for testing)</li> <li>max_concurrent: Balance speed vs. resource usage</li> <li>pass_at_k: Enable multiple attempts for better success measurement</li> </ul>"},{"location":"contribute_benchmarks/#step-3-set-up-data-directory","title":"Step 3: Set Up Data Directory","text":"<p>Organize your dataset files in the MiroFlow data structure.</p> Data Directory Setup<pre><code># Create the benchmark data directory\nmkdir -p data/your-benchmark\n\n# Copy your dataset files\ncp your-dataset/* data/your-benchmark/\n\n# Verify the structure\nls -la data/your-benchmark/\n</code></pre> <p>File Path Consistency</p> <p>Ensure that all <code>file_path</code> entries in your JSONL metadata correctly reference files in your data directory.</p>"},{"location":"contribute_benchmarks/#step-4-test-your-benchmark","title":"Step 4: Test Your Benchmark","text":"<p>Validate your benchmark integration with comprehensive testing.</p>"},{"location":"contribute_benchmarks/#initial-testing","title":"Initial Testing","text":"<p>Start with a small subset to verify everything works correctly:</p> Test Benchmark Integration<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_quickstart_1 \\\n  benchmark=your-benchmark \\\n  benchmark.execution.max_tasks=3 \\\n  output_dir=\"logs/test-your-benchmark/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"contribute_benchmarks/#full-evaluation","title":"Full Evaluation","text":"<p>Once testing passes, run the complete benchmark:</p> Run Full Benchmark<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_quickstart_1 \\\n  benchmark=your-benchmark \\\n  output_dir=\"logs/your-benchmark/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"contribute_benchmarks/#step-5-validate-results","title":"Step 5: Validate Results","text":"<p>Review the evaluation outputs to ensure proper integration:</p>"},{"location":"contribute_benchmarks/#check-output-files","title":"Check Output Files","text":"Verify Results<pre><code># List generated files\nls -la logs/your-benchmark/\n\n# Review a sample task log\ncat logs/your-benchmark/task_*_attempt_1.json | head -50\n</code></pre>"},{"location":"contribute_benchmarks/#expected-output-structure","title":"Expected Output Structure","text":"<p>Your benchmark should generate:</p> <ul> <li>Individual task execution logs</li> <li>Aggregate benchmark results (<code>benchmark_results.jsonl</code>)</li> <li>Accuracy summary files</li> <li>Hydra configuration logs</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_llm_clients/","title":"Contributing New LLM Clients","text":"<p>Add support for new LLM providers to MiroFlow by creating a provider class that integrates with the existing client infrastructure.</p>"},{"location":"contribute_llm_clients/#client-structure","title":"Client Structure","text":"<p>Each LLM client inherits from <code>LLMProviderClientBase</code> and implements 4 required methods:</p> <ul> <li><code>_create_client()</code> - Initialize API client</li> <li><code>_create_message()</code> - Make API calls  </li> <li><code>process_llm_response()</code> - Handle responses</li> <li><code>extract_tool_calls_info()</code> - Parse tool calls</li> </ul>"},{"location":"contribute_llm_clients/#implementation-steps","title":"Implementation Steps","text":""},{"location":"contribute_llm_clients/#step-1-create-provider-file","title":"Step 1: Create Provider File","text":"<p>Create <code>src/llm/providers/your_provider_client.py</code>:</p> Provider Implementation<pre><code>import dataclasses\nfrom src.llm.provider_client_base import LLMProviderClientBase\n\n@dataclasses.dataclass\nclass YourProviderClient(LLMProviderClientBase):\n    def _create_client(self, config):\n        # Initialize your API client\n        pass\n\n    async def _create_message(self, system_prompt, messages, tools_definitions, keep_tool_result=-1):\n        # Make API call\n        pass\n\n    def process_llm_response(self, llm_response, message_history, agent_type=\"main\"):\n        # Extract response text, return (text, should_exit)\n        pass\n\n    def extract_tool_calls_info(self, llm_response, assistant_response_text):\n        # Parse tool calls, return (tool_calls, tool_names)\n        pass\n</code></pre>"},{"location":"contribute_llm_clients/#step-2-create-configuration","title":"Step 2: Create Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"YourProviderClient\"\n    model_name: \"your-model\"\n    your_api_key: \"${oc.env:YOUR_API_KEY,???}\"\n    your_base_url: \"${oc.env:YOUR_BASE_URL,https://api.yourprovider.com/v1}\"\n</code></pre>"},{"location":"contribute_llm_clients/#step-3-set-environment-variables","title":"Step 3: Set Environment Variables","text":"Environment Setup<pre><code>export YOUR_API_KEY=\"your-key\"\nexport YOUR_BASE_URL=\"https://api.yourprovider.com/v1\"  # optional if using default\n</code></pre>"},{"location":"contribute_llm_clients/#examples","title":"Examples","text":"<p>See existing providers in <code>src/llm/providers/</code>:</p> <ul> <li><code>ClaudeAnthropicClient</code> - Direct API</li> <li><code>ClaudeOpenRouterClient</code> - Proxy API  </li> <li><code>GPTOpenAIClient</code> - OpenAI API</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_tools/","title":"Contributing New Tools","text":"<p>MiroFlow's extensible tool system allows you to add custom functionality by implementing new MCP (Model Context Protocol) servers. Each tool extends the agent's capabilities and can be easily integrated into the framework.</p>"},{"location":"contribute_tools/#overview","title":"Overview","text":"<p>What This Does</p> <p>Extend the agent's functionality by introducing a new tool. Each tool is implemented as an MCP server and registered via configuration, enabling agents to access new capabilities seamlessly.</p>"},{"location":"contribute_tools/#implementation-steps","title":"Implementation Steps","text":""},{"location":"contribute_tools/#step-1-create-mcp-server","title":"Step 1: Create MCP Server","text":"<p>Create a new file <code>src/tool/mcp_servers/new-mcp-server.py</code> that implements the tool's core logic.</p> src/tool/mcp_servers/new-mcp-server.py<pre><code>from fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"new-mcp-server\")\n\n@mcp.tool()\nasync def tool_name(param: str) -&gt; str:\n    \"\"\"\n    Explanation of the tool, its parameters, and return value.\n    \"\"\"\n    tool_result = ...  # Your logic here\n    return tool_result\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre> <p>Automatic Schema Generation</p> <p>Tool schemas are automatically generated from <code>docstrings</code> and <code>type hints</code> via the FastMCP protocol.</p>"},{"location":"contribute_tools/#step-2-create-tool-configuration","title":"Step 2: Create Tool Configuration","text":"<p>Add a new configuration file at <code>config/tool/new-tool-name.yaml</code>:</p> config/tool/new-tool-name.yaml<pre><code>name: \"new-tool-name\"\ntool_command: \"python\"\nargs:\n  - \"-m\"\n  - \"src.tool.mcp_servers.new-mcp-server\"  # Match the server file created above\n</code></pre>"},{"location":"contribute_tools/#step-3-register-tool-in-agent-configuration","title":"Step 3: Register Tool in Agent Configuration","text":"<p>Enable the new tool inside your agent configuration (e.g., <code>config/agent-with-new-tool.yaml</code>):</p> config/agent-with-new-tool.yaml<pre><code>main_agent:\n  # ... other configuration ...\n  tool_config:\n    - tool-reasoning\n    - new-tool-name   # \ud83d\udc48 Add your new tool here\n  # ... other configuration ...\n\nsub_agents:\n  agent-worker:\n    # ... other configuration ...\n    tool_config:\n      - tool-searching\n      - tool-image-video\n      - tool-reading\n      - tool-code\n      - tool-audio\n      - new-tool-name # \ud83d\udc48 Add your new tool here\n    # ... other configuration ...\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contributors/","title":"\ud83d\udcdd Contributors","text":"<p>Thank you to all the amazing contributors who have helped make MiroFlow better! \ud83d\ude4f</p>"},{"location":"contributors/#core-team","title":"Core Team","text":"<p>Development Team</p> <p>The MiroFlow framework is developed and maintained by the MiroMind AI team.</p>"},{"location":"contributors/#community-contributors","title":"Community Contributors","text":"<p>Community Appreciation</p> <p>We welcome contributions from the community! Whether you're fixing bugs, adding features, improving documentation, or helping with benchmarks, your contributions are valued.</p> <p></p>"},{"location":"contributors/#how-to-contribute","title":"How to Contribute","text":"<p>Contribution Opportunities</p> <p>There are many ways to contribute to MiroFlow:</p>"},{"location":"contributors/#bug-reports-feature-requests","title":"\ud83d\udc1b Bug Reports &amp; Feature Requests","text":"<p>Issue Reporting</p> <ul> <li>Report bugs or request features via GitHub Issues</li> <li>Use clear, descriptive titles and provide detailed information</li> </ul>"},{"location":"contributors/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<p>Development Workflow</p> <ul> <li>Fork the repository and create a feature branch</li> <li>Follow our coding standards and include tests</li> <li>Submit a pull request with a clear description of your changes</li> </ul>"},{"location":"contributors/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Documentation Help</p> <ul> <li>Help improve our documentation</li> <li>Add examples and tutorials</li> <li>Fix typos and clarify explanations</li> </ul>"},{"location":"contributors/#testing-benchmarks","title":"\ud83e\uddea Testing &amp; Benchmarks","text":"<p>Quality Assurance</p> <ul> <li>Help us test MiroFlow on different platforms</li> <li>Contribute new benchmark datasets</li> <li>Improve existing evaluation scripts</li> </ul>"},{"location":"contributors/#community-support","title":"\ud83d\udcac Community Support","text":"<p>Community Engagement</p> <ul> <li>Answer questions in our Discord community</li> <li>Help other users in GitHub discussions</li> <li>Share your experiences and use cases</li> </ul>"},{"location":"contributors/#recognition","title":"Recognition","text":"<p>Contributor Acknowledgment</p> <p>All contributors are recognized in our:</p> <ul> <li>GitHub contributors graph</li> <li>Release notes for significant contributions</li> <li>Community acknowledgments</li> </ul>"},{"location":"contributors/#getting-started","title":"Getting Started","text":"<p>Quick Start Guide</p> <ol> <li>Check out our GitHub repository</li> <li>Read the contributing guidelines</li> <li>Join our Discord community to connect with other contributors</li> </ol> <p>Thank You</p> <p>Thank you for helping us build the future of AI agents! \ud83d\ude80</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"core_concepts/","title":"Core Concepts","text":"<p>MiroFlow is a flexible framework for building and deploying intelligent agents capable of complex reasoning and tool use.</p>"},{"location":"core_concepts/#architecture-overview","title":"Architecture Overview","text":"<p>Multi-Stage Agentic Process</p> <p>MiroFlow processes user queries through a structured workflow:</p> <ol> <li>Intent Recognition &amp; Query Augmentation - LLMs analyze and refine user input</li> <li>Planning &amp; Task Orchestration - Main agent creates execution plans and coordinates sub-agents</li> <li>Delegation to Sub-Agents - Specialized agents handle domain-specific tasks</li> <li>Tool Access via MCP Servers - Agents leverage external capabilities through MCP protocol</li> <li>Result Synthesis &amp; Output Alignment - Final results are synthesized and formatted</li> </ol>"},{"location":"core_concepts/#core-components","title":"Core Components","text":""},{"location":"core_concepts/#agent-system","title":"Agent System","text":"<p>Agent Architecture</p> <p>Main Agent: Primary coordinator that receives tasks, creates plans, and manages overall execution. Can use reasoning tools and delegate to sub-agents.</p> <p>Sub-Agents: Specialized agents for specific domains:</p> <ul> <li><code>agent-worker</code>: General-purpose agent with comprehensive tool access (search, files, code, media)</li> <li>Each sub-agent has dedicated configurations and can operate independently</li> </ul>"},{"location":"core_concepts/#tool-integration","title":"Tool Integration","text":"<p>Tool System</p> <p>Tool Manager: Connects to MCP servers and manages tool availability</p> <p>Available Tools:</p> <ul> <li>Code Execution: Python sandbox via E2B integration</li> <li>Web Search: Google search with content retrieval</li> <li>Document Processing: Multi-format file reading and analysis</li> <li>Visual Processing: Image and video analysis</li> <li>Audio Processing: Transcription and audio analysis</li> <li>Enhanced Reasoning: Advanced reasoning via high-quality LLMs</li> </ul> <p>See Tool Overview for detailed tool configurations and capabilities.</p>"},{"location":"core_concepts/#llm-support","title":"LLM Support","text":"<p>Multi-Provider Support</p> <p>Unified interface supporting:</p> <ul> <li>Anthropic Claude (via Anthropic API, OpenRouter)</li> <li>OpenAI GPT (via OpenAI API)</li> <li>Qwen (via SGLang)</li> <li>MiroThinker (via SGLang)</li> <li>see LLM Clients Overview for details*</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"data/","title":"\ud83d\udcca Data","text":"<p>The MiroVerse dataset collection provides comprehensive training data for building advanced AI agents with full trajectory coverage.</p>"},{"location":"data/#news-updates","title":"\ud83d\udd25 News &amp; Updates","text":"<p>Latest Releases</p> <ul> <li>The data is released over Huggingface.</li> <li>MiroVerse v0.1 has been released. This dataset can be used with our training framework, MiroTrain. In MiroVerse v0.1, we provide both SFT and DPO data, making it easy to reproduce MiroThinker-v0.1's benchmark performance on Qwen3. Give it a try!</li> </ul>"},{"location":"data/#first-batch-of-miroverse","title":"\ud83d\udd25 First Batch of MiroVerse","text":"<p>What makes this release special</p> <p>\u2728 Special Features:</p> <ul> <li>\ud83d\udcda Diverse Verified Open Source Data \u2014 Carefully curated and validated community datasets</li> <li>\ud83e\udde0 Fresh Large-Scale Deep Research Data \u2014 Generated by our proprietary data engine</li> <li>\ud83d\udd04 Complete Trajectory Coverage \u2014 Every single sample includes full rollout trajectories</li> <li>\u2705 Quality Assurance: \u2014 Each trajectory has been verified, ensuring high-quality training data for your models.</li> <li>\ud83c\udf31 Always Growing, Always Open \u2014 Regular updates, powered by collaboration with the community</li> </ul>"},{"location":"data/#dataset-overview","title":"\ud83d\udce6 Dataset Overview","text":"<p>MiroVerse-v0.1 Statistics</p> <p>MiroVerse-v0.1 is a large-scale agent dataset with 147K+ samples featuring full rollout trajectories across diverse AI agent tasks including multi-hop QA, web navigation, and scientific reasoning. Every single sample includes complete execution traces with 1.9B+ tokens and 602K+ tool interactions, providing comprehensive training data for tool-using and web-browsing AI agents.</p> <p></p> Split #Sample #Main Trace #Browse Trace #Token #Turns #Tools License MiroVerse-Voyager1.0 59097 19115 39982 1129113893 444723 325537 CC-BY-NC-4.0 MiroVerse-MuSiQue 29572 10422 19150 294351053 143080 90486 CC-BY-4.0 MiroVerse-HotpotQA 12942 6553 6389 67352039 46320 20524 CC-BY-SA-4.0 MiroVerse-WebWalkerQA-Silver 10817 4961 5856 107650324 67846 46215 Apache 2.0 MiroVerse-MegaScience 10615 8270 2345 111120264 63594 42443 CC-BY-NC-SA-4.0 MiroVerse-TaskCraft 8890 4277 4613 95518109 35013 17236 MIT MiroVerse-QA-Expert-Multi-Hop-V1.0 6187 2091 4096 63983151 31957 19585 Apache 2.0 MiroVerse-OneGen-TrainDataset-MultiHopQA 3289 1347 1942 33214386 17187 11449 MIT MiroVerse-2WikiMultihopQA 3001 1410 1591 28977451 13982 7981 Apache 2.0 MiroVerse-WikiTables 1606 1288 318 16461870 12089 8877 MIT MiroVerse-WebShaper 1514 486 1028 31240265 12126 9578 MIT MiroVerse-WebDancer 455 192 263 7817689 3170 2268 MIT MiroVerse-v0.1 147985 60412 87573 1993099086 891087 602179 / <p>Dataset Details</p> <p>Every sample includes successful MiroFlow rollout trajactories that reached the verified answer\u2014one JSON line, zero secrets.</p> <p>Licensing Information</p> <p>MiroVerse-v0.1 dataset follows a hybrid licensing model: query and answer data retain their original source licenses, while all trace data is licensed under CC-BY-NC-4.0; for commercial use, please contact us to request a commercial license.</p>"},{"location":"data/#why-were-different","title":"\ud83c\udd9a Why We're Different","text":"<p>Our Philosophy</p> <p>While high-quality data is essential for training advanced models and often kept private, we believe that the path to truly general-purpose agents is still long. That's why we're committed to open-sourcing as much of our data as possible\u2014including raw samples and exploration traces\u2014to support and accelerate progress across the community.</p> Org Work Samples Trace Data Reproducible? OpenAI Deep Research \u2014 \u274c \u274c Gemini Gemini Deep Research \u2014 \u274c \u274c Tencent Cognitive Kernel-Pro 7 k \u274c \u274c Tongyi WebShaper 500 \u274c \u274c MiroMind (ours) this repo 147 k+ \u2705 \u2705"},{"location":"data/#benchmark-performance","title":"\ud83d\udcc8 Benchmark Performance","text":"<p>Training Results</p> <p>MiroVerse-v0.1 is used in the training of our MiroThinker-v0.1 models.</p> <p>By using this dataset, we achieved the following benchmark performance.</p>"},{"location":"data/#gaia-benchmark","title":"GAIA Benchmark","text":"Method Text-103Best Pass@1 Text-103Pass@1 (Avg@8) Val-165Best Pass@1 Val-165Pass@1 (Avg@8) Search-o1-7B 17.5 - - - R1-Searcher-7B 20.4 - - - WebDancer-7B 31.0 - - - WebSailor-7B 37.9 - - - CK-Pro-8B 43.7 - 35.2 - MiroThinker-8B-SFT-v0.1 44.7 40.1 34.6 31.8 + Commercial Tools 46.6 42.1 37.6 33.9 MiroThinker-8B-DPO-v0.1 46.6 44.8 37.0 35.4 + Commercial Tools 50.5 46.7 38.2 35.9 MiroThinker-14B-SFT-v0.1 47.6 44.4 37.0 34.4 + Commercial Tools 49.5 47.5 41.8 39.8 MiroThinker-14B-DPO-v0.1 48.5 46.6 42.4 39.2 + Commercial Tools 52.4 48.5 45.5 42.0 Qwen3-32B 31.1 26.7 29.7 26.4 Search-o1-32B 28.2 - - - WebThinker-32B-RL 48.5 - - - WebDancer-QwQ-32B 51.5 - - - WebSailor-32B 53.2 - - - WebShaper-QwQ-32B 53.3 - - - MiroThinker-32B-SFT-v0.1 55.3 51.3 44.9 42.7 + Commercial Tools 58.3 54.2 48.5 45.8 MiroThinker-32B-DPO-v0.1 57.3 54.1 48.5 45.9 + Commercial Tools 60.2 57.9 50.9 48.9 <ol> <li> <p>Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.</p> </li> <li> <p>For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.</p> </li> <li> <p>By default, we use open-source tools wherever possible, except for the code tool E2B and the Google search tool Serper. We use Whisper, Qwen2.5-VL-72B-Instruct, and Qwen3-235B-A22B-Thinking-2507 in our implementation. The framework can be easily extended to other open-source tools of your choice.</p> </li> <li> <p>Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our models.</p> </li> </ol>"},{"location":"data/#more-benchmarks","title":"More Benchmarks","text":"Method HLEPass@1 FramesPass@1 BrowseCompPass@1 BrowseComp-ZHPass@1 WebWalkerQAPass@1 OpenAI Deep Research 26.6 - 51.5 42.9 - Gemini Deep Research 26.9 - - - - Kimi-Researcher 26.9 78.8 - - - WebDancer-7B - - - - 36.0 WebSailor-7B - - 6.7 14.2 - MiroThinker-8B-SFT-v0.1 - 58.0 5.5 9.3 41.3 MiroThinker-8B-DPO-v0.1 - 64.4 8.7 13.5 45.7 WebThinker-32B-RL - - - - 46.5 WebDancer-QwQ-32B - - 3.8 18.0 47.9 WebSailor-32B - - 10.5 25.5 - WebShaper-32B - - - - 51.4 MiroThinker-32B-SFT-v0.1 10.2 70.4 10.6 13.8 45.7 MiroThinker-32B-DPO-v0.1 11.8 71.7 13.0 17.0 49.3 <ol> <li> <p>MiroThinker\u2019s performance was tested with this repository and open-source tools; other models\u2019 results are from their papers and official sites.</p> </li> <li> <p>As MiroVerse-v0.1 mainly contains English data, the model's Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.</p> </li> </ol>"},{"location":"data/#examples","title":"\ud83e\udde9 Examples","text":"<p>Sample QA Examples</p> <p>Below are two QA examples synthesized by our data engine (MiroVerse-Voyager1.0).</p> <p>Case 1</p> <p>Q: A female lead actress received her first major annual Hindi film performance award for best actress for her role in a late-2000s comedy-drama, directed by the filmmaker who later created a sports-themed drama released in 2023 starring an actress known for completing an athletic triathlon event in Berlin. What is the title of the film for which this actress first won that award?</p> <p>A: Paa</p> <p>Case 2</p> <p>Q: Identify the agricultural practice, unique to a mountain range that forms a border including an independent principality and known for spectacular geologic landforms, that was one of the key reasons for part of the range's inscription as a UNESCO World Heritage Site in the decade before the 21st century. This region's history features a brief early-1800s reorganization of provincial boundaries after a liberal revolution in the southern country, and the northern country is globally recognized as the leading tourist destination with the fourth-largest number of heritage sites. What is this traditional agricultural system called?</p> <p>A: transhumance</p>"},{"location":"data/#free-trace-rollout-let-us-help-you-train","title":"\ud83d\udee0\ufe0f Free Trace Rollout: Let Us Help You Train","text":"<p>Community Support</p> <p>Generating high-quality training trajectories is expensive \u2014 on average, $1.50 per sample using top-tier commercial models.</p> <p>To empower the community, we're offering free rollout services for qualifying seed data:</p>"},{"location":"data/#how-it-works","title":"How It Works","text":"<p>Process Steps</p> <p>1. Submit a Request</p> <p>Open a ticket via this template and provide the basic info, rollout requirements, and up to 100 sample rows in one go.</p> <p>2. Review &amp; Rollout</p> <p>We'll review your submission within 48 hours. Once approved, we'll reach out to you for the full dataset and then launch the complete trace rollout using top-tier commercial models.</p> <p>3. Delivery &amp; Recognition</p> <p>Upon completion, we'll send the augmented dataset to you via email.</p> <p>With your explicit consent, we'll also publish it publicly and credit you as a Community Contributor \u2014 with a permanent badge in this README.</p>"},{"location":"data/#license","title":"\ud83e\udd1d License","text":"<p>License Terms</p> <p>This project is released under the CC BY-NC 4.0. Parts of this project contain code and models from other sources, which are subject to their respective licenses. For commercial use cases, please contact us at: service@miromind.ai.</p>"},{"location":"data/#citation","title":"\ud83d\udcdc Citation","text":"<p>Academic Citation</p> <p>If you find this project useful in your research, please consider cite:</p> BibTeX Citation<pre><code>@misc{miromind2024opendata,\n  title={MiroVerse V0.1: A Reproducible, Full-Trajectory, Ever-Growing Deep Research Dataset},\n  author={MiroMind Data Team},\n  year={2025},\n  url={https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1}\n}\n</code></pre>"},{"location":"data/#contact-us","title":"Contact Us","text":"<p>Get in Touch</p> <p>MiroVerse is developed by the MiroMind Data Team. If you would like to leave us a message, feel free to get in touch.  In addition to GitHub,  Discord,  WeChat,  and RedNote,  you can also reach us via email at service@miromind.ai.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"download_datasets/","title":"Dataset Download Instructions","text":"<p>This guide walks you through downloading and preparing benchmark datasets for MiroFlow evaluation.</p>"},{"location":"download_datasets/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>Before downloading datasets, ensure you have completed both access requests and environment setup below.</p>"},{"location":"download_datasets/#1-request-dataset-access","title":"1. Request Dataset Access","text":"<p>You must request access to the following Hugging Face datasets:</p> <p>Required Datasets</p> <ul> <li>GAIA Dataset: https://huggingface.co/datasets/gaia-benchmark/GAIA</li> <li>HLE Dataset: https://huggingface.co/datasets/cais/hle</li> </ul> <p>Visit the links above and request access to both datasets.</p>"},{"location":"download_datasets/#2-configure-environment-variables","title":"2. Configure Environment Variables","text":"<p>Copy the template file and create your environment configuration:</p> <pre><code>cp .env.template .env\n</code></pre> <p>Edit the <code>.env</code> file and configure these essential variables:</p> .env<pre><code># Required: Your Hugging Face token for dataset access\nHF_TOKEN=\"your-actual-huggingface-token-here\"\n\n# Data directory path \nDATA_DIR=\"data/\"\n</code></pre> <p>Getting Your Hugging Face Token</p> <ol> <li>Go to https://huggingface.co/settings/tokens</li> <li>Create a new token with at least \"Read\" permissions</li> <li>Replace <code>your-actual-huggingface-token-here</code> in the <code>.env</code> file with your actual token</li> </ol>"},{"location":"download_datasets/#download-and-prepare-datasets","title":"Download and Prepare Datasets","text":"<p>Once you have been granted access to the required datasets, run the preparation script to download all benchmark datasets.</p>"},{"location":"download_datasets/#running-the-download-script","title":"Running the Download Script","text":"<p>Execute the following command to start the download process for all datasets, if a single dataset is needed, you could run the specific command:</p> <pre><code>bash scripts/run_prepare_benchmark.sh\n</code></pre> <p>Script Contents</p> <p>The script contains the following logic and dataset downloads. You can comment out any unwanted datasets by adding <code>#</code> at the start of the line.</p> scripts/run_prepare_benchmark.sh<pre><code>#!/bin/bash\necho \"Please grant access to these datasets:\"\necho \"- https://huggingface.co/datasets/gaia-benchmark/GAIA\"\necho \"- https://huggingface.co/datasets/cais/hle\"\necho\n\nread -p \"Have you granted access? [Y/n]: \" answer\nanswer=${answer:-Y}\nif [[ ! $answer =~ ^[Yy] ]]; then\n    echo \"Please grant access to the datasets first\"\n    exit 1\nfi\necho \"Access confirmed\"\n\n# Comment out any unwanted datasets by adding # at the start of the line\nuv run main.py prepare-benchmark get gaia-val\nuv run main.py prepare-benchmark get gaia-val-text-only\nuv run main.py prepare-benchmark get frames-test\nuv run main.py prepare-benchmark get webwalkerqa\nuv run main.py prepare-benchmark get browsecomp-test\nuv run main.py prepare-benchmark get browsecomp-zh-test\nuv run main.py prepare-benchmark get hle\nuv run main.py prepare-benchmark get xbench-ds\nuv run main.py prepare-benchmark get futurex\n</code></pre>"},{"location":"download_datasets/#what-this-script-does","title":"What This Script Does","text":"<p>Script Actions</p> <ol> <li>Confirms dataset access - Verifies you have requested access to required datasets</li> <li>Downloads benchmark datasets - Retrieves the following datasets:<ul> <li><code>gaia-val</code> - GAIA validation set</li> <li><code>gaia-val-text-only</code> - Text-only GAIA validation</li> <li><code>frames-test</code> - Frames test dataset</li> <li><code>webwalkerqa</code> - Web Walker QA dataset</li> <li><code>browsecomp-test</code> - English BrowseComp test set</li> <li><code>browsecomp-zh-test</code> - Chinese BrowseComp test set</li> <li><code>hle</code> - HLE dataset</li> <li><code>xbench-ds</code> - xbench-DeepSearch dataset</li> <li><code>futurex</code> - Futurex-Online dataset</li> </ul> </li> </ol>"},{"location":"download_datasets/#customizing-dataset-selection","title":"Customizing Dataset Selection","text":"<p>To download only specific datasets, edit the script and comment out unwanted lines:</p> <pre><code># Comment out unwanted datasets like this:\n# uv run main.py prepare-benchmark get gaia-val\nuv run main.py prepare-benchmark get gaia-val-text-only\n# uv run main.py prepare-benchmark get frames-test\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"e2b_advanced_features/","title":"E2B Advanced Features","text":"<p>Preview Documentation</p> <p>This section is in preview and not fully ready. The features and instructions may change in future releases.</p> <p>MiroFlow provides advanced E2B (Execute to Build) sandbox capabilities for enhanced code execution environments with pre-installed packages and custom configurations.</p>"},{"location":"e2b_advanced_features/#local-e2b-sandbox-deployment","title":"Local E2B Sandbox Deployment","text":"<p>Recommended Setup</p> <p>To achieve our best benchmark results, we recommend using a pre-defined sandbox template that includes the most commonly used Python and apt packages.</p> <p>If you prefer not to use a sandbox template, you can disable it by commenting out the line <code>template=DEFAULT_TEMPLATE_ID,</code> in <code>libs/miroflow-tool/src/miroflow/tool/mcp_servers/python_server.py</code> (line 145).</p>"},{"location":"e2b_advanced_features/#sandbox-setup-guide","title":"Sandbox Setup Guide","text":"<p>Prerequisites</p> <ul> <li>npm installed locally</li> <li>Docker running locally</li> <li>E2B API key configured</li> </ul>"},{"location":"e2b_advanced_features/#step-1-install-e2b-cli","title":"Step 1: Install E2B CLI","text":"Install E2B Command Line<pre><code># Install e2b\nnpm install -g @e2b/cli\n\n# Verify installation\nwhich e2b \n</code></pre>"},{"location":"e2b_advanced_features/#step-2-download-pre-configured-dockerfile","title":"Step 2: Download Pre-configured Dockerfile","text":"<p>Download our pre-configured Dockerfile from the repository:</p> Download Dockerfile<pre><code>wget https://github.com/MiroMindAI/MiroFlow/blob/main/docs/e2b.Dockerfile\n</code></pre>"},{"location":"e2b_advanced_features/#step-3-build-template","title":"Step 3: Build Template","text":"<p>Run the <code>e2b template build</code> command to create your custom template:</p> Build E2B Template<pre><code># Set your E2B access token\nE2B_ACCESS_TOKEN=${your-token}\n\n# Build the template with docker build locally\ne2b template build -c \"/root/.jupyter/start-up.sh\" -n \"all_pip_apt_pkg\" -d ./e2b.Dockerfile\n\n# Verify template was built successfully\nE2B_ACCESS_TOKEN=${your-token} e2b template list\n</code></pre> <p>Custom Templates</p> <p>You can create your own custom sandbox template for specific use cases by following similar steps. For more information, refer to the E2B Docker documentation.</p>"},{"location":"e2b_advanced_features/#e2b-docker-configuration","title":"E2B Docker Configuration","text":"<p>This custom E2B Docker environment provides a sandboxed environment with pre-installed scientific computing libraries, data analysis tools, and dependencies commonly needed for AI agent tasks.</p> e2b.Dockerfile<pre><code># You can use most Debian-based base images\nFROM e2bdev/code-interpreter\n\n# Update package list and install Python 3.10 and pip\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    portaudio19-dev \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel\n\n# Install dependencies and customize sandbox\nRUN python3 -m pip install --no-cache-dir \\\n    Flask \\\n    IPython \\\n    Pillow \\\n    PyGithub \\\n    PyMuPDF \\\n    PyPDF2 \\\n    arch \\\n    arm-pyart \\\n    arxiv \\\n    ase \\\n    astropy \\\n    astroquery \\\n    awscli \\\n    beautifulsoup4 \\\n    biopython \\\n    boto3 \\\n    brian2 \\\n    cairosvg \\\n    cgt \\\n    chardet \\\n    chess \\\n    cinemagoer \\\n    clifford \\\n    contextily \\\n    control \\\n    cryptography \\\n    cvxpy \\\n    datasets \\\n    descarteslabs \\\n    duckduckgo-search \\\n    edalize \\\n    english_words \\\n    ephem \\\n    esp-docs \\\n    flask \\\n    folium \\\n    geopandas \\\n    geopy \\\n    google-search-results \\\n    googlesearch-python \\\n    googletrans \\\n    habanero \\\n    helics \\\n    hijri_converter \\\n    imbalanced-learn \\\n    inflect \\\n    isbnlib \\\n    kaggle \\\n    lifelines \\\n    lxml \\\n    lxml_html_clean \\\n    mapclassify \\\n    markdown \\\n    'matplotlib&gt;=3.8' \\\n    mendeleev \\\n    metpy \\\n    music21 \\\n    networkx \\\n    nipype \\\n    numba \\\n    'numpy&gt;=2' \\\n    opencv-python \\\n    openpyxl \\\n    'pandas&gt;=2' \\\n    pandas_datareader \\\n    parsl \\\n    pdf2image \\\n    pdfminer \\\n    pdfplumber \\\n    periodictable \\\n    plotly \\\n    polars \\\n    psycopg2-binary \\\n    pulp \\\n    pyXSteam \\\n    pybel \\\n    pycryptodome \\\n    pydot \\\n    pygplates \\\n    pymatgen \\\n    pymupdf \\\n    pypdf2 \\\n    pypinyin \\\n    pyscf \\\n    pytesseract \\\n    python-docx \\\n    pytube \\\n    pywavelets \\\n    rdflib \\\n    reportlab \\\n    requests \\\n    requests-html \\\n    scanpy \\\n    scikit-image \\\n    scikit-learn \\\n    scipy \\\n    scvelo \\\n    seaborn \\\n    selenium \\\n    semanticscholar \\\n    shap \\\n    shapely \\\n    siphon \\\n    skyfield \\\n    smbus2 \\\n    snappy \\\n    spglib \\\n    sphinx \\\n    splink \\\n    statsmodels \\\n    stockfish \\\n    sympy \\\n    tabulate \\\n    torch \\\n    torchvision \\\n    transformers \\\n    uncertainpy \\\n    us \\\n    virtualenv \\\n    wbdata \\\n    webdriver-manager \\\n    wikipedia-api \\\n    wolframalpha \\\n    wordfreq \\\n    yfinance \\\n    yt-dlp \\\n    docx2txt \\\n    rdkit \\\n    stockfish \\\n    yfinance \\\n    seaborn \\\n    python-pptx \\\n    pyaudio \\\n    pyshp \\\n    SpeechRecognition \\\n    waybackpy\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    # \u2500\u2500 Basic build &amp; Python \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    build-essential gfortran cmake pkg-config git curl wget ca-certificates \\\n    # \u2500\u2500 scientific computing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libopenblas-dev liblapack-dev libatlas-base-dev \\\n    libssl-dev libffi-dev zlib1g-dev \\\n    # \u2500\u2500 image / OpenCV / Pillow \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libgl1 libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender1 \\\n    libjpeg-dev libpng-dev libwebp-dev libfreetype6-dev libopenjp2-7 liblcms2-dev \\\n    # \u2500\u2500 video / audio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ffmpeg libsndfile1 sox portaudio19-dev \\\n    # \u2500\u2500 PDF / doc / OCR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    poppler-utils pdfgrep ghostscript \\\n    tesseract-ocr tesseract-ocr-deu \\\n    libxml2-dev libxslt1-dev \\\n    # \u2500\u2500 other tools \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    imagemagick unlambda stockfish \\\n    unzip zip tar nano &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"evaluation_overview/","title":"\ud83d\udcca Performance Benchmarks","text":"<p>MiroFlow achieves state-of-the-art performance across multiple agentic benchmarks, demonstrating its effectiveness in complex reasoning and tool-use tasks.</p>"},{"location":"evaluation_overview/#performance-on-future-prediction","title":"Performance on Future Prediction","text":"<p>Future X Benchmark Results</p> <p>MiroFlow demonstrates exceptional performance in future prediction tasks.</p> <p></p>"},{"location":"evaluation_overview/#performance-on-benchmarks","title":"\u2728 Performance on Benchmarks","text":"<p>Comprehensive Benchmark Analysis</p> <p>We benchmark MiroFlow on a series of benchmarks including GAIA, HLE, BrowseComp and xBench-DeepSearch.</p> <p></p>"},{"location":"evaluation_overview/#other-benchmark-results","title":"Other Benchmark Results","text":"<p>Detailed Performance Comparison</p> <p>Comprehensive comparison across multiple benchmark categories and competing frameworks.</p>"},{"location":"evaluation_overview/#reasoning-language-understanding","title":"Reasoning &amp; Language Understanding","text":"Model/Framework GAIA Val HLE HLE-Text MiroFlow 82.4% 27.2% 29.5% OpenAI Deep Research 67.4% 26.6% - Gemini Deep Research - 26.9% - Kimi Researcher - - 26.9% WebSailor-72B 55.4% - - Manus 73.3% - - DeepSeek v3.1 - - 29.8%"},{"location":"evaluation_overview/#web-browsing-search-tasks","title":"Web Browsing &amp; Search Tasks","text":"Model/Framework BrowserComp-EN BrowserComp-ZH xBench-DeepSearch MiroFlow 33.2% 47.1% 72.0% OpenAI Deep Research 51.5% 42.9% - Gemini Deep Research - - 50+% Kimi Researcher - - 69.0% WebSailor-72B - 30.1% 55.0% DeepSeek v3.1 - - 71.2% <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"faq_and_known_issues/","title":"Faq and known issues","text":""},{"location":"faq_and_known_issues/#faq","title":"FAQ","text":"<p>Q: What is the estimated cost of running the GAIA validation set for a single run? A: The cost is approximately $250 USD for a run with cache.</p> <p>Q: How long does it take to run the GAIA validation set for a single run? A: With the <code>max_concurrent</code> parameter set to 20, a full run takes about 2 hours to complete.</p> <p>Q: Are all the specified APIs required? A: Yes. To fully reproduce our published results, access to all the listed APIs in corresponding benchmark is necessary.</p> <p>Q: What is the difference between MiroFlow and MiroThinker? A:  MiroFlow is primarily focused on interacting with proprietary models; MiroThinker is designed for our own open-source models.</p> <p>We plan to merge these two projects in the future to create a single, unified platform.</p>"},{"location":"faq_and_known_issues/#known-issues-roadmap","title":"Known Issues &amp; Roadmap","text":""},{"location":"faq_and_known_issues/#currently-in-development","title":"\ud83d\udd04 Currently in Development","text":"<ul> <li>FutureX Benchmark: Adding support for FutureX benchmark evaluation</li> <li>Token Usage &amp; Cost Tracking: Implementing detailed usage analytics and cost calculation features</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"faqs/","title":"\ud83d\udc1b FAQ and Known Issues","text":"<p>Common questions and development roadmap for MiroFlow framework.</p>"},{"location":"faqs/#faq","title":"FAQ","text":"<p>Frequently Asked Questions</p> <p>Common questions about MiroFlow usage, costs, and platform differences.</p> <p>Q: What is the estimated cost of running the GAIA validation set for a single run?</p> <p>A: The cost is approximately $250 USD for a run with cache.</p> <p>Q: How long does it take to run the GAIA validation set for a single run?</p> <p>A: With the <code>max_concurrent</code> parameter set to 20, a full run takes about 2 hours to complete.</p> <p>Q: Are all the specified APIs required?</p> <p>A: Yes. To fully reproduce our published results, access to all the listed APIs in corresponding benchmark is necessary.</p> <p>Q: What is the difference between MiroFlow and MiroThinker?</p> <p>A: MiroFlow is primarily focused on interacting with proprietary models; MiroThinker is designed for our own open-source models.</p> <p>We plan to merge these two projects in the future to create a single, unified platform.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"finsearchcomp/","title":"FinSearchComp","text":"<p>MiroFlow's evaluation on the FinSearchComp benchmark demonstrates capabilities in financial information search and analysis tasks, showcasing advanced reasoning abilities in complex financial research scenarios.</p> <p>More details: FinSearchComp Dataset</p>"},{"location":"finsearchcomp/#dataset-overview","title":"Dataset Overview","text":"<p>FinSearchComp Dataset</p> <p>The FinSearchComp dataset consists of financial search and analysis tasks that require comprehensive research capabilities including:</p> <ul> <li>Financial data retrieval and analysis</li> <li>Market research and company analysis</li> <li>Investment decision support</li> <li>Financial news and report interpretation</li> <li>Time-sensitive financial information gathering</li> </ul> <p>Key Dataset Characteristics</p> <ul> <li>Total Tasks: 635 (across T1, T2, T3 categories)</li> <li>Task Types: <ul> <li>T1: Time-Sensitive Data Fetching</li> <li>T2: Financial Analysis and Research</li> <li>T3: Complex Historical Investigation</li> </ul> </li> <li>Answer Format: Detailed financial analysis and research reports</li> <li>Ground Truth: Available for T2 and T3 tasks, changes dynamically for T1 tasks</li> <li>Evaluation: Judge-based evaluation with correctness assessment</li> </ul>"},{"location":"finsearchcomp/#quick-start-guide","title":"Quick Start Guide","text":"<p>Quick Start Instructions</p> <p>This section provides step-by-step instructions to run the FinSearchComp benchmark and prepare submission results. Note: This is a quick start guide for running the benchmark, not for reproducing exact submitted results.</p>"},{"location":"finsearchcomp/#step-1-prepare-the-finsearchcomp-dataset","title":"Step 1: Prepare the FinSearchComp Dataset","text":"<p>Dataset Setup</p> <p>Use the integrated prepare-benchmark command to download and process the dataset:</p> Download FinSearchComp Dataset<pre><code>uv run main.py prepare-benchmark get finsearchcomp\n</code></pre> <p>This will create the standardized dataset at <code>data/finsearchcomp/standardized_data.jsonl</code>.</p>"},{"location":"finsearchcomp/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>API Key Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># For searching and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use MiroThinker model for financial analysis\nOAI_MIROTHINKER_API_KEY=\"xxx\"\nOAI_MIROTHINKER_BASE_URL=\"http://localhost:61005/v1\"\n\n# Used for o3 hints and final answer extraction\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n</code></pre>"},{"location":"finsearchcomp/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Evaluation Execution</p> <p>Execute the following command to run evaluation on the FinSearchComp dataset:</p> Run FinSearchComp Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp benchmark=finsearchcomp output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Progress Monitoring and Resume</p> <p>To check the progress while running:</p> Check Progress<pre><code>uv run utils/progress_check/check_finsearchcomp_progress.py $PATH_TO_LOG\n</code></pre> <p>If you need to resume an interrupted evaluation, specify the same output directory to continue from where you left off.</p> Resume Evaluation, e.g.<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp benchmark=finsearchcomp output_dir=${PATH_TO_LOG}\n</code></pre>"},{"location":"finsearchcomp/#step-4-extract-results","title":"Step 4: Extract Results","text":"<p>Result Extraction</p> <p>After evaluation completion, the results are automatically generated in the output directory:</p> <ul> <li><code>benchmark_results.jsonl</code>: Detailed results for each task</li> <li><code>benchmark_results_pass_at_1_accuracy.txt</code>: Summary accuracy statistics</li> <li><code>task_*_attempt_1.json</code>: Individual task execution traces</li> </ul>"},{"location":"finsearchcomp/#evaluation-notes","title":"Evaluation Notes","text":"<p>Task Type Considerations</p> <p>The FinSearchComp dataset includes different task types with varying evaluation criteria:</p> <ul> <li>T1 Tasks: Time-Sensitive Data Fetching tasks are excluded from correctness evaluation due to outdated ground truth, but completion is still tracked</li> <li>T2 Tasks: Financial Analysis tasks are evaluated for correctness and quality</li> <li>T3 Tasks: Complex Historical Investigation tasks require comprehensive research and analysis</li> </ul> <p>Output Analysis</p> <p>The evaluation generates detailed execution traces showing:</p> <ul> <li>Research process for each financial task</li> <li>Information gathering from multiple sources</li> <li>Financial calculations and analysis</li> <li>Comprehensive reports with insights and recommendations</li> </ul>"},{"location":"finsearchcomp/#directory-structure","title":"Directory Structure","text":"<p>After running evaluations, you'll find the following structure:</p> <pre><code>logs/finsearchcomp/agent_finsearchcomp_YYYYMMDD_HHMM/\n\u251c\u2500\u2500 benchmark_results.jsonl              # Task results summary\n\u251c\u2500\u2500 benchmark_results_pass_at_1_accuracy.txt  # Accuracy statistics\n\u251c\u2500\u2500 task_(T1)Time_Sensitive_Data_Fetching_*.json  # T1 task traces\n\u251c\u2500\u2500 task_(T2)Financial_Analysis_*.json   # T2 task traces\n\u251c\u2500\u2500 task_(T3)Complex_Historical_Investigation_*.json  # T3 task traces\n\u2514\u2500\u2500 output.log                           # Execution log\n</code></pre>"},{"location":"finsearchcomp/#task-categories-breakdown","title":"Task Categories Breakdown","text":"<p>The progress checker provides detailed statistics:</p> <ul> <li>Total Tasks: Complete count across all categories</li> <li>Completed Tasks: Successfully finished tasks</li> <li>Correct Tasks: Tasks with judge_result \"CORRECT\" (T2 and T3 only)</li> <li>Category Breakdown: Separate counts for T1, T2, and T3 tasks</li> <li>Accuracy Metrics: Pass@1 accuracy for evaluable tasks</li> </ul>"},{"location":"finsearchcomp/#usage-examples","title":"Usage Examples","text":""},{"location":"finsearchcomp/#single-run-evaluation","title":"Single Run Evaluation","text":"Basic Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp benchmark=finsearchcomp output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"finsearchcomp/#limited-task-testing","title":"Limited Task Testing","text":"Test with Limited Tasks<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp benchmark=finsearchcomp benchmark.execution.max_tasks=5 output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"finsearchcomp/#custom-agent-configuration","title":"Custom Agent Configuration","text":"Different Agent Setup<pre><code>uv run main.py common-benchmark --config_file_name=agent_gaia-validation benchmark=finsearchcomp output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"finsearchcomp/#multiple-runs-for-reliability","title":"Multiple Runs for Reliability","text":"Multiple Runs<pre><code>NUM_RUNS=5 ./scripts/run_evaluate_multiple_runs_finsearchcomp.sh\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"futurex/","title":"Futurex-Online","text":"<p>MiroFlow's evaluation on the Futurex-Online benchmark demonstrates capabilities in future event prediction tasks.</p>"},{"location":"futurex/#dataset-overview","title":"Dataset Overview","text":"<p>Futurex-Online Dataset</p> <p>The Futurex-Online dataset consists of 61 prediction tasks covering various future events including:</p> <ul> <li>Political events (referendums, elections)</li> <li>Sports outcomes (football matches)</li> <li>Legal proceedings</li> <li>Economic indicators</li> </ul> <p>Key Dataset Characteristics</p> <ul> <li>Total Tasks: 61</li> <li>Task Type: Future event prediction</li> <li>Answer Format: Boxed answers (\\boxed{Yes/No} or \\boxed{A/B/C})</li> <li>Ground Truth: Not available (prediction tasks)</li> <li>Resolution Date: Around 2025-09-21 (GMT+8)</li> </ul>"},{"location":"futurex/#quick-start-guide","title":"Quick Start Guide","text":"<p>Quick Start Instructions</p> <p>This section provides step-by-step instructions to run the Futurex-Online benchmark and prepare submission results. Since this is a prediction dataset without ground truth, we focus on execution traces and response generation. Note: This is a quick start guide for running the benchmark, not for reproducing exact submitted results.</p>"},{"location":"futurex/#step-1-prepare-the-futurex-online-dataset","title":"Step 1: Prepare the Futurex-Online Dataset","text":"<p>Dataset Setup</p> <p>Use the integrated prepare-benchmark command to download and process the dataset:</p> Download Futurex-Online Dataset<pre><code>uv run main.py prepare-benchmark get futurex\n</code></pre> <p>This will create the standardized dataset at <code>data/futurex/standardized_data.jsonl</code>.</p>"},{"location":"futurex/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>API Key Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># For searching and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use Claude-3.7-Sonnet with OpenRouter backend to initialize the LLM\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n\n# Use for llm judge, reasoning, o3 hints, etc.\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"futurex/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Evaluation Execution</p> <p>Execute the following command to run evaluation on the Futurex-Online dataset. This uses the basic <code>agent_quickstart_1</code> configuration for quick start purposes.</p> Run Futurex-Online Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_quickstart_1 benchmark=futurex output_dir=\"logs/futurex/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Progress Monitoring and Resume</p> <p>To check the progress while running:</p> Check Progress<pre><code>uv run utils/progress_check/check_futurex_progress.py $PATH_TO_LOG\n</code></pre> <p>If you need to resume an interrupted evaluation, specify the same output directory to continue from where you left off.</p> Resume Evaluation, e.g.<pre><code>uv run main.py common-benchmark --config_file_name=agent_quickstart_1 benchmark=futurex output_dir=\"logs/futurex/20250918_1010\"\n</code></pre>"},{"location":"futurex/#step-4-extract-results","title":"Step 4: Extract Results","text":"<p>Result Extraction</p> <p>After evaluation completion, extract the results using the provided utility:</p> Extract Results<pre><code>uv run utils/extract_futurex_results.py logs/futurex/$(date +\"%Y%m%d_%H%M\")\n</code></pre> <p>This will generate:</p> <ul> <li><code>futurex_results.json</code>: Detailed results for each task</li> <li><code>futurex_summary.json</code>: Summary statistics</li> <li><code>futurex_predictions.csv</code>: Predictions in CSV format</li> </ul>"},{"location":"futurex/#sample-task-examples","title":"Sample Task Examples","text":""},{"location":"futurex/#political-prediction","title":"Political Prediction","text":"<pre><code>Task: \"Will the 2025 Guinea referendum pass? (resolved around 2025-09-21 (GMT+8))\"\nExpected Format: \\boxed{Yes} or \\boxed{No}\n</code></pre>"},{"location":"futurex/#sports-prediction","title":"Sports Prediction","text":"<pre><code>Task: \"Brighton vs. Tottenham (resolved around 2025-09-21 (GMT+8))\nA. Brighton win on 2025-09-20\nB. Brighton vs. Tottenham end in a draw  \nC. Tottenham win on 2025-09-20\"\nExpected Format: \\boxed{A}, \\boxed{B}, or \\boxed{C}\n</code></pre>"},{"location":"futurex/#multiple-runs-and-voting","title":"Multiple Runs and Voting","text":"<p>Improving Prediction Accuracy</p> <p>For better prediction accuracy, you can run multiple evaluations and use voting mechanisms to aggregate results. This approach helps reduce randomness and improve the reliability of predictions. Note: This is a quick start approach; production submissions may use more sophisticated configurations.</p>"},{"location":"futurex/#step-1-run-multiple-evaluations","title":"Step 1: Run Multiple Evaluations","text":"<p>Use the multiple runs script to execute several independent evaluations:</p> Run Multiple Evaluations<pre><code>./scripts/run_evaluate_multiple_runs_futurex.sh\n</code></pre> <p>This script will:</p> <ul> <li>Run 3 independent evaluations by default (configurable with <code>NUM_RUNS</code>)</li> <li>Execute all tasks in parallel for efficiency</li> <li>Generate separate result files for each run in <code>run_1/</code>, <code>run_2/</code>, etc.</li> <li>Create a consolidated <code>futurex_submission.jsonl</code> file with voting results</li> </ul>"},{"location":"futurex/#step-2-customize-multiple-runs","title":"Step 2: Customize Multiple Runs","text":"<p>You can customize the evaluation parameters:</p> Custom Multiple Runs<pre><code># Run 5 evaluations with limited tasks for testing\nNUM_RUNS=5 MAX_TASKS=10 ./scripts/run_evaluate_multiple_runs_futurex.sh\n\n# Use different agent configuration\nAGENT_SET=agent_gaia-validation ./scripts/run_evaluate_multiple_runs_futurex.sh\n\n# Adjust concurrency for resource management\nMAX_CONCURRENT=3 ./scripts/run_evaluate_multiple_runs_futurex.sh\n</code></pre>"},{"location":"futurex/#step-3-voting-and-aggregation","title":"Step 3: Voting and Aggregation","text":"<p>After multiple runs, the system automatically:</p> <ol> <li>Extracts predictions from all runs using <code>utils/extract_futurex_results.py</code></li> <li>Applies majority voting to aggregate predictions across runs</li> <li>Generates submission file in the format required by FutureX platform</li> <li>Provides voting statistics showing prediction distribution across runs</li> </ol> <p>The voting process works as follows:</p> <ul> <li>Majority Vote: Most common prediction across all runs wins</li> <li>Tie-breaking: If tied, chooses the prediction that appeared earliest across all runs</li> <li>Vote Counts: Tracks how many runs predicted each option</li> <li>Confidence Indicators: High agreement indicates more reliable predictions</li> </ul>"},{"location":"futurex/#step-4-analyze-voting-results","title":"Step 4: Analyze Voting Results","text":"<p>Check the generated files for voting analysis:</p> Check Voting Results<pre><code># View submission file with voting results\ncat logs/futurex/agent_quickstart_1_*/futurex_submission.jsonl\n\n# Check individual run results\nls logs/futurex/agent_quickstart_1_*/run_*/\n\n# Check progress and voting statistics\nuv run python utils/progress_check/check_futurex_progress.py logs/futurex/agent_quickstart_1_*\n</code></pre>"},{"location":"futurex/#manual-voting-aggregation","title":"Manual Voting Aggregation","text":"<p>You can also manually run the voting aggregation:</p> Manual Voting Aggregation<pre><code># Aggregate multiple runs with majority voting\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_1_* --aggregate\n\n# Force single run mode (if needed)\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_1_*/run_1 --single\n\n# Specify custom output file\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_1_* -o my_voted_predictions.jsonl\n</code></pre>"},{"location":"futurex/#voting-output-format","title":"Voting Output Format","text":"<p>The voting aggregation generates a submission file with the following format:</p> <pre><code>{\"id\": \"687104310a994c0060ef87a9\", \"prediction\": \"No\", \"vote_counts\": {\"No\": 2}}\n{\"id\": \"68a9b46e961bd3003c8f006b\", \"prediction\": \"Yes\", \"vote_counts\": {\"Yes\": 2}}\n</code></pre> <p>The output includes:</p> <ul> <li><code>id</code>: Task identifier</li> <li><code>prediction</code>: Final voted prediction (without <code>\\boxed{}</code> wrapper)</li> <li><code>vote_counts</code>: Dictionary showing how many runs predicted each option</li> </ul> <p>For example, <code>\"vote_counts\": {\"No\": 2}</code> means 2 out of 2 runs predicted \"No\", indicating high confidence.</p>"},{"location":"futurex/#evaluation-notes","title":"Evaluation Notes","text":"<p>No Ground Truth Available</p> <p>Since Futurex-Online is a prediction dataset, there are no ground truth answers available for evaluation. The focus is on:</p> <ul> <li>Response generation quality</li> <li>Reasoning process documentation</li> <li>Prediction confidence and methodology</li> </ul> <p>Output Analysis</p> <p>The evaluation generates detailed execution traces showing:</p> <ul> <li>Research process for each prediction</li> <li>Information gathering from web sources</li> <li>Reasoning chains leading to predictions</li> <li>Final boxed answers in required format</li> </ul>"},{"location":"futurex/#directory-structure","title":"Directory Structure","text":"<p>After running multiple evaluations, you'll find the following structure:</p> <pre><code>logs/futurex/agent_quickstart_1_YYYYMMDD_HHMM/\n\u251c\u2500\u2500 futurex_submission.jsonl          # Final voted predictions\n\u251c\u2500\u2500 run_1/                            # First run results\n\u2502   \u251c\u2500\u2500 benchmark_results.jsonl       # Individual task results\n\u2502   \u251c\u2500\u2500 benchmark_results_pass_at_1_accuracy.txt\n\u2502   \u2514\u2500\u2500 task_*_attempt_1.json        # Detailed execution traces\n\u251c\u2500\u2500 run_2/                            # Second run results\n\u2502   \u2514\u2500\u2500 ... (same structure as run_1)\n\u251c\u2500\u2500 run_1_output.log                  # Run 1 execution log\n\u2514\u2500\u2500 run_2_output.log                  # Run 2 execution log\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_test/","title":"GAIA Test","text":"<p>The GAIA (General AI Assistant) test set provides a comprehensive evaluation dataset for assessing AI agents' capabilities in complex, real-world reasoning tasks. This benchmark tests agents' ability to perform multi-step problem solving, information synthesis, and tool usage across diverse scenarios.</p> <p>More details: GAIA: a benchmark for General AI Assistants</p>"},{"location":"gaia_test/#setup-and-evaluation-guide","title":"Setup and Evaluation Guide","text":""},{"location":"gaia_test/#step-1-download-the-gaia-test-dataset","title":"Step 1: Download the GAIA Test Dataset","text":"<p>Direct Download (Recommended)</p> <pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-test.zip\nunzip gaia-test.zip\n# Unzip passcode: pf4*\n</code></pre>"},{"location":"gaia_test/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Required API Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and O3 hints\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"gaia_test/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the following command:</p> Run GAIA Test Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-test \\\n  output_dir=\"logs/gaia-test/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_test/#step-4-monitor-progress-and-resume","title":"Step 4: Monitor Progress and Resume","text":"<p>Progress Tracking</p> <p>You can monitor the evaluation progress in real-time:</p> Check Progress<pre><code>uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p> <p>Resume Capability</p> <p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-test \\\n  output_dir=\"logs/gaia-test/20250922_1430\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_validation/","title":"GAIA Validation","text":"<p>MiroFlow demonstrates state-of-the-art performance on the GAIA validation benchmark, showcasing exceptional capabilities in complex reasoning tasks that require multi-step problem solving, information synthesis, and tool usage.</p> <p>More details: GAIA: a benchmark for General AI Assistants</p>"},{"location":"gaia_validation/#about-the-gaia-dataset","title":"About the GAIA Dataset","text":"<p>What is GAIA?</p> <p>GAIA (General AI Assistant) is a comprehensive benchmark designed to evaluate AI agents' ability to perform complex reasoning tasks that require multiple skills including web browsing, file manipulation, data analysis, and multi-step problem solving.</p>"},{"location":"gaia_validation/#performance-comparison","title":"Performance Comparison","text":"<p>State-of-the-Art Performance</p> <p>MiroFlow achieves state-of-the-art (SOTA) performance among open-source agent frameworks on the GAIA validation set.</p> <p></p> <p>Key Performance Metrics</p> <ul> <li>Pass@3: 81.8%</li> <li>Majority Vote: 82.4%</li> <li>Pass@1 (best@3): 74.5%</li> <li>Pass@1 (avg@3): 72.2%</li> </ul> <p>Reproducibility Guarantee</p> <p>Unlike other frameworks with unclear evaluation methods, MiroFlow's results are fully reproducible. Note that Hugging Face access was disabled during inference to prevent direct answer retrieval.</p>"},{"location":"gaia_validation/#setup-and-evaluation-guide","title":"Setup and Evaluation Guide","text":"<p>Complete Reproduction Instructions</p> <p>This section provides comprehensive step-by-step instructions to reproduce our GAIA validation benchmark results. All results are fully reproducible using our open-source framework.</p>"},{"location":"gaia_validation/#step-1-prepare-the-gaia-validation-dataset","title":"Step 1: Prepare the GAIA Validation Dataset","text":"<p>Choose one of the following methods to obtain the GAIA validation dataset:</p> <p>Method 1: Direct Download (Recommended)</p> <p>No Authentication Required</p> <p>This method does not require HuggingFace tokens or access permissions.</p> Manual Dataset Download<pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-val.zip\nunzip gaia-val.zip\n# Unzip passcode: pf4*\n</code></pre> <p>Method 2: Using the prepare-benchmark command</p> <p>Prerequisites Required</p> <p>This method requires HuggingFace dataset access and token configuration.</p> <p>First, you need to request access and configure your environment:</p> <ol> <li>Request Dataset Access: Visit https://huggingface.co/datasets/gaia-benchmark/GAIA and request access</li> <li>Configure Environment:     <pre><code>cp .env.template .env\n</code></pre>    Edit the <code>.env</code> file:    <pre><code>HF_TOKEN=\"your-actual-huggingface-token-here\"\nDATA_DIR=\"data/\"\n</code></pre></li> </ol> <p>Getting Your Hugging Face Token</p> <ol> <li>Go to https://huggingface.co/settings/tokens</li> <li>Create a new token with at least \"Read\" permissions</li> <li>Add your token to the <code>.env</code> file</li> </ol> <p>Then download the dataset:</p> Download via Script<pre><code>uv run main.py prepare-benchmark get gaia-val\n</code></pre>"},{"location":"gaia_validation/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Required API Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and O3 hints\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre> <p>Why OpenRouter?</p> <p>We use Claude-3.7-Sonnet through the OpenRouter backend as the primary LLM provider because OpenRouter offers better response rates and improved reliability compared to direct API access.</p>"},{"location":"gaia_validation/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the following command:</p> Run GAIA Validation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation \\\n  output_dir=\"logs/gaia-validation/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_validation/#step-4-monitor-progress-and-resume","title":"Step 4: Monitor Progress and Resume","text":"<p>Progress Tracking</p> <p>You can monitor the evaluation progress in real-time:</p> Check Progress<pre><code>uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p> <p>Resume Capability</p> <p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation \\\n  output_dir=\"logs/gaia-validation/20250922_1430\"\n</code></pre>"},{"location":"gaia_validation/#execution-traces","title":"Execution Traces","text":"<p>Complete Execution Traces</p> <p>We have released our complete execution traces for the <code>gaia-validation</code> dataset on Hugging Face. This comprehensive collection includes a full run of 165 tasks with an overall accuracy of 73.94%.</p> <p>You can download them using the following command:</p> Download Execution Traces<pre><code>wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia_validation_miroflow_trace_public_20250825.zip\nunzip gaia_validation_miroflow_trace_public_20250825.zip\n# Unzip passcode: pf4*\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_validation_text_only/","title":"GAIA Validation Text-Only","text":"<p>The GAIA (General AI Assistant) benchmark is a comprehensive evaluation dataset designed to assess AI agents' capabilities in complex, real-world reasoning tasks. The text-only variant focuses specifically on tasks that can be completed using textual reasoning and web-based research, without requiring image or video processing capabilities.</p> <p>More Details: WebThinker: Empowering Large Reasoning Models with Deep Research Capability</p> <p>Evaluation Methodology</p> <p>The text-only subset uses an LLM-as-judge evaluation approach, which differs from the exact-match evaluation used in GAIA-Validation or GAIA-Text. This methodology was established in the original WebThinker paper, and subsequent work should align with this approach for fair comparison.</p>"},{"location":"gaia_validation_text_only/#setup-and-evaluation-guide","title":"Setup and Evaluation Guide","text":""},{"location":"gaia_validation_text_only/#step-1-download-the-dataset","title":"Step 1: Download the Dataset","text":"<p>Choose one of the following methods to obtain the GAIA Validation Text-Only dataset:</p> <p>Method 1: Automated Download (Recommended)</p> Download via MiroFlow Command<pre><code>uv run main.py prepare-benchmark get gaia-val-text-only\n</code></pre> <p>Method 2: Manual Download</p> Manual Dataset Download<pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-val-text-only.zip\nunzip gaia-val-text-only.zip\n# Unzip passcode: pf4*\n</code></pre>"},{"location":"gaia_validation_text_only/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Required API Configuration</p> <p>Before running the evaluation, you must configure the necessary API keys in your <code>.env</code> file. Each service serves a specific purpose in the evaluation pipeline.</p> .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and O3 hints\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre> <p>Why OpenRouter?</p> <p>We use Claude-3.7-Sonnet through the OpenRouter backend as the primary LLM provider because OpenRouter offers better response rates and improved reliability compared to direct API access.</p>"},{"location":"gaia_validation_text_only/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the following command structure:</p> Run GAIA Validation Text-Only Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation-text-only \\\n  output_dir=\"logs/gaia-validation-text-only/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_validation_text_only/#step-4-monitor-progress-and-resume","title":"Step 4: Monitor Progress and Resume","text":"<p>Progress Tracking</p> <p>You can monitor the evaluation progress in real-time using the progress checker:</p> Check Evaluation Progress<pre><code>uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p> <p>Resume Capability</p> <p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation-text-only \\\n  output_dir=\"logs/gaia-validation-text-only/20250922_1430\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#overview","title":"Overview","text":"<p>MiroFlow is released under the Apache License 2.0, which is a permissive open-source license that allows for both commercial and non-commercial use.</p> <p>License Summary</p> <ul> <li>\u2705 Commercial use - Use MiroFlow in commercial projects</li> <li>\u2705 Modification - Modify and adapt the code for your needs  </li> <li>\u2705 Distribution - Distribute original or modified versions</li> <li>\u2705 Private use - Use MiroFlow in private projects</li> <li>\u26a0\ufe0f Trademark - You cannot use MiroMind AI trademarks</li> <li>\u26a0\ufe0f Liability - No warranty or liability from the authors</li> </ul>"},{"location":"license/#apache-license-20","title":"Apache License 2.0","text":"<p>The full text of the Apache License 2.0 can be found at: https://www.apache.org/licenses/LICENSE-2.0</p>"},{"location":"license/#component-licenses","title":"Component Licenses","text":"<p>Some components within MiroFlow may have different licenses:</p> <p>Third-Party Components</p> <p>Individual components, dependencies, or integrated tools may have their own license terms. Please check the respective file headers, <code>LICENSE</code> files, or documentation for specific licensing information.</p>"},{"location":"license/#attribution","title":"Attribution","text":"<p>When using MiroFlow in your projects, attribution is appreciated but not required. You may include:</p> <pre><code>Powered by MiroFlow - https://github.com/MiroMindAI/MiroFlow\n</code></pre>"},{"location":"license/#questions","title":"Questions","text":"<p>For licensing questions or clarifications, please:</p> <ul> <li>Review the full Apache License 2.0 text</li> <li>Check individual component licenses</li> <li>Open an issue on our GitHub repository</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"llm_clients_overview/","title":"LLM Clients Overview","text":"<p>MiroFlow supports multiple LLM providers through a unified client interface. Each client handles provider-specific API communication while maintaining consistent functionality.</p>"},{"location":"llm_clients_overview/#available-clients","title":"Available Clients","text":"Client Provider Model Environment Variables <code>ClaudeAnthropicClient</code> Anthropic Direct claude-3-7-sonnet <code>ANTHROPIC_API_KEY</code>, <code>ANTHROPIC_BASE_URL</code> <code>ClaudeOpenRouterClient</code> OpenRouter anthropic/claude-3.7-sonnet, and other supported models <code>OPENROUTER_API_KEY</code>, <code>OPENROUTER_BASE_URL</code> <code>GPTOpenAIClient</code> OpenAI gpt-4, gpt-3.5 <code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code> <code>MiroThinkerSGLangClient</code> SGLang MiroThinker series <code>OAI_MIROTHINKER_API_KEY</code>, <code>OAI_MIROTHINKER_BASE_URL</code>"},{"location":"llm_clients_overview/#basic-configuration","title":"Basic Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"ClientName\"\n    model_name: \"model-name\"\n    api_key_param: \"${oc.env:API_KEY,???}\"\n    base_url_param: \"${oc.env:BASE_URL,default-url}\"\n</code></pre>"},{"location":"llm_clients_overview/#quick-setup","title":"Quick Setup","text":"<ol> <li>Set relevant environment variables for your chosen provider</li> <li>Update your YAML config file with the appropriate client</li> <li>Run: <code>uv run main.py trace --config_file_name=your_config_file --task=\"task\"</code></li> </ol> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"mirothinker/","title":"MiroThinker","text":"<p>MiroThinker (4B/7B/14B/32B) is our suite of open-source agentic models, designed to work seamlessly with the MiroFlow framework. Our models are specifically built to handle complex, multi-tool tasks, leveraging the reproducible and robust foundation that MiroFlow provides.</p> <p>By combining MiroFlow's reliable orchestration with MiroThinker's advanced reasoning capabilities, we offer a powerful, end-to-end solution for building high-performing, reproducible AI agents.</p> <p>These models are a direct result of our extensive data collection efforts, utilizing MiroFlow to generate high-quality, post-training agent trace data. This unique approach enables MiroThinker to excel in planning, executing, and reasoning through complex multi-step tasks.</p>"},{"location":"mirothinker/#deploying-mirothinker-32b-with-miroflow","title":"Deploying MiroThinker-32B with MiroFlow","text":"<p>This guide explains how to deploy the MiroThinker-32B-DPO-v0.2 model from Hugging Face and integrate it with MiroFlow.</p>"},{"location":"mirothinker/#prerequisites","title":"Prerequisites","text":"<ul> <li>SGLang installed</li> <li>Sufficient GPU memory for the model</li> <li>MiroFlow repository set up</li> </ul>"},{"location":"mirothinker/#step-1-deploy-model-with-sglang","title":"Step 1: Deploy Model with SGLang","text":"<p>Deploy the MiroThinker-32B model using SGLang:</p> SGLang Server Deployment<pre><code>python3 -m sglang.launch_server \\\n    --model-path miromind-ai/MiroThinker-32B-DPO-v0.2 \\\n    --tp 8 \\\n    --dp 1 \\\n    --host 0.0.0.0 \\\n    --port 61005 \\\n    --trust-remote-code \\\n    --chat-template qwen3_nonthinking.jinja\n</code></pre> <p>Important Notes</p> <ul> <li>Adjust the <code>--tp</code> (tensor parallelism) parameter to match your number of GPUs</li> <li>Download the chat template from: qwen3_nonthinking.jinja</li> <li>Ensure the port you used (in this case 61005) is available on your system</li> </ul>"},{"location":"mirothinker/#step-2-configure-miroflow","title":"Step 2: Configure MiroFlow","text":"<p>Once the SGLang server is running, configure MiroFlow by adding the following to your <code>.env</code> file:</p> Environment Configuration<pre><code>OAI_MIROTHINKER_API_KEY=\"dummy_key\"\nOAI_MIROTHINKER_BASE_URL=\"http://localhost:61005/v1\"\n</code></pre> <p>Configuration Notes</p> <ul> <li>If your model requires authentication, replace <code>dummy_key</code> with your actual API key</li> <li>Replace <code>localhost</code> with the appropriate hostname if deploying on a remote server</li> </ul>"},{"location":"mirothinker/#step-3-test-the-integration","title":"Step 3: Test the Integration","text":"<p>Test your setup with the following command:</p> Test Command<pre><code>uv run main.py trace --config_file_name=agent_mirothinker \\\n    --task=\"What is the first country listed in the XLSX file that have names starting with Co?\" \\\n    --task_file_name=\"data/FSI-2023-DOWNLOAD.xlsx\"\n</code></pre> <p>This command will: - Use the <code>agent_mirothinker</code> configuration with the dedicated MiroThinkerSGLangClient - Process the specified Excel file - Query the model to find countries starting with \"Co\"</p>"},{"location":"mirothinker/#configuration-details","title":"Configuration Details","text":"<p>The <code>./config/agent_mirothinker.yaml</code> configuration file uses:</p> <ul> <li><code>provider_class: \"MiroThinkerSGLangClient\"</code> - A dedicated client for MiroThinker models deployed with SGLang</li> <li>Model path and generation parameters (temperature, top_p, max_tokens, etc.)</li> <li>Environment variables for API endpoint configuration</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Xalp @ MiroMind AI</p>"},{"location":"openai-gpt/","title":"OpenAI GPT Models","text":"<p>OpenAI's latest models including GPT-4o and O3 reasoning models with strong coding, vision, and reasoning capabilities.</p>"},{"location":"openai-gpt/#client-used","title":"Client Used","text":"<p><code>GPTOpenAIClient</code></p>"},{"location":"openai-gpt/#environment-setup","title":"Environment Setup","text":"Environment Variables<pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # optional\n</code></pre>"},{"location":"openai-gpt/#configuration","title":"Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"GPTOpenAIClient\"\n    model_name: \"gpt-4o\"  # or o3, etc.\n    openai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n    openai_base_url: \"${oc.env:OPENAI_BASE_URL,https://api.openai.com/v1}\"\n</code></pre>"},{"location":"openai-gpt/#usage","title":"Usage","text":"Example Command<pre><code># Create custom OpenAI config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"openrouter-claude-3.7-sonnet/","title":"OpenRouter Claude 3.7 Sonnet (Recommended)","text":"<p>Access multiple models via OpenRouter using unified OpenAI chat format. Supports Claude, GPT, and other models with higher rate limits.</p>"},{"location":"openrouter-claude-3.7-sonnet/#client-used","title":"Client Used","text":"<p><code>ClaudeOpenRouterClient</code></p>"},{"location":"openrouter-claude-3.7-sonnet/#environment-setup","title":"Environment Setup","text":"Environment Variables<pre><code>export OPENROUTER_API_KEY=\"your-openrouter-key\"\nexport OPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"  # optional\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#configuration","title":"Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"  # or openai/gpt-4, etc.\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n    openrouter_base_url: \"${oc.env:OPENROUTER_BASE_URL,https://openrouter.ai/api/v1}\"\n    openrouter_provider: \"anthropic\"  # Force provider, or \"\" for auto\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#other-supported-models","title":"Other Supported Models","text":"<ul> <li><code>openai/gpt-4</code></li> <li><code>openai/gpt-3.5-turbo</code></li> <li><code>anthropic/claude-3-opus</code></li> <li><code>google/gemini-pro</code></li> <li>Many others via unified OpenAI format</li> </ul>"},{"location":"openrouter-claude-3.7-sonnet/#usage","title":"Usage","text":"Example Command<pre><code># Use existing OpenRouter config\nuv run main.py trace --config_file_name=your_config_file \\\n    --task=\"Your task\" --task_file_name=\"data/file.txt\"\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#benefits-vs-direct-api","title":"Benefits vs Direct API","text":"<ul> <li>Unified chat format</li> <li>Higher rate limits</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"quickstart/","title":"\ud83d\ude80 Get Started in Under 5 Minutes","text":"<p>Clone the repository, configure your API key, and run your first intelligent agent. You'll just need one <code>OPENROUTER_API_KEY</code>.</p>"},{"location":"quickstart/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>System Requirements</p> <ul> <li>Python: 3.12 or higher</li> <li>Package Manager: <code>uv</code>, https://docs.astral.sh/uv/</li> <li>Operating System: Linux, macOS</li> </ul>"},{"location":"quickstart/#quick-setup","title":"\u26a1 Quick Setup","text":""},{"location":"quickstart/#example-1-intelligent-document-analysis-with-file-processing-capabilities","title":"Example 1: Intelligent document analysis with file processing capabilities","text":"<p>File Processing Demo</p> <p>This example demonstrates MiroFlow's document analysis capabilities.</p> Setup Commands<pre><code># 1. Clone and setup\ngit clone https://github.com/MiroMindAI/MiroFlow &amp;&amp; cd MiroFlow\nuv sync\n\n# 2. Configure API key\ncp .env.template .env\n# Edit .env and add your OPENROUTER_API_KEY\n\n# 3. Run your first agent\nuv run main.py trace --config_file_name=agent_quickstart_1 --task=\"What is the first country listed in the XLSX file that have names starting with Co?\" --task_file_name=\"data/FSI-2023-DOWNLOAD.xlsx\"\n</code></pre> <p>Expected Output</p> <p>\ud83c\udf89 Expected Output: Your agent should return \\boxed{Congo Democratic Republic} \ud83d\ude0a</p> <p>Troubleshooting</p> <p>\ud83d\udca1 Tip: If you encounter issues, check that your API key is correctly set in the <code>.env</code> file and that all dependencies are installed.</p> <p>Coming Soon</p> <p>Coming Soon: We will add a video demo for this example</p>"},{"location":"quickstart/#example-2-web-research-and-multi-agent-orchestration","title":"Example 2: Web research and multi-agent orchestration","text":"<p>Work in Progress</p> <p>The example is not complete yet, to be completed</p> Web Research Command<pre><code>uv run main.py trace --config_file_name=agent_quickstart_2 --task=\"What is the Nasdaq Composite Index at today?\"\n</code></pre> <p>Coming Soon</p> <p>Coming Soon: Web research and multi-agent orchestration example</p> <p>Documentation Info</p> <p>Last Updated: Sep 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_overview/","title":"Tool Overview","text":"<p>MiroFlow provides a comprehensive set of tools that extend agent capabilities through the Model Context Protocol (MCP).</p>"},{"location":"tool_overview/#available-tools","title":"Available Tools","text":"<p>Core Tools</p> <ul> <li>Python Tools - Code execution in secure sandbox</li> <li>Searching Tools - Web search and content retrieval  </li> <li>Vision Tools - Image analysis and video processing</li> <li>Reasoning Tools - Advanced logical analysis</li> </ul> <p>Additional Tools Available</p> <p>MiroFlow includes additional tools for audio processing, document reading, web browsing, and markdown conversion. See the <code>config/tool/</code> directory for complete tool configurations.</p>"},{"location":"tool_overview/#quick-setup","title":"Quick Setup","text":"<p>Tools are configured in agent YAML files and require API keys in your <code>.env</code> file. See individual tool documentation for detailed setup instructions.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_python/","title":"Python Tools (<code>python_server.py</code>)","text":"<p>The Python Execution Server provides a secure sandboxed environment for running Python code and shell commands using E2B server. This tool enables agents to execute code safely, manipulate files, and perform computational tasks in an isolated environment.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Sandbox Management: Create and manage isolated execution environments</li> <li>Code Execution: Run Python code and shell commands safely</li> <li>File Operations: Upload, download, and transfer files between local and sandbox</li> <li>Internet Access: Download files directly from web sources to sandbox</li> </ul>"},{"location":"tool_python/#function-reference","title":"Function Reference","text":"<p>The following functions are provided by the <code>python_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_python/#create_sandbox","title":"<code>create_sandbox()</code>","text":"<p>Creates a Linux sandbox for safely executing commands and running Python code.</p> <p>Returns: - <code>str</code>: The <code>sandbox_id</code> of the newly created sandbox</p> <p>Important Usage Notes</p> <ul> <li>Required First Step: This tool must be called before using other tools within this MCP server</li> <li>Session Management: The sandbox may timeout and automatically shut down after inactivity</li> <li>Pre-installed Environment: The sandbox comes pre-installed with common packages for data science and document processing. For a detailed list and advanced usage information, see E2B Extension</li> </ul>"},{"location":"tool_python/#run_commandsandbox_id-str-command-str","title":"<code>run_command(sandbox_id: str, command: str)</code>","text":"<p>Execute shell commands in the Linux sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox (must be created first) - <code>command</code>: Shell command to execute</p> <p>Returns: - <code>str</code>: Command execution result (stderr, stdout, exit_code, error)</p> <p>Features: - Automatic retry mechanism - Permission hints for sudo commands</p>"},{"location":"tool_python/#run_python_codesandbox_id-str-code_block-str","title":"<code>run_python_code(sandbox_id: str, code_block: str)</code>","text":"<p>Run Python code in the sandbox and return execution results.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>code_block</code>: Python code to execute</p> <p>Returns: - <code>str</code>: Code execution result (stderr, stdout, exit_code, error)</p> <p>Features: - Automatic retry mechanism</p>"},{"location":"tool_python/#upload_file_from_local_to_sandboxsandbox_id-str-local_file_path-str-sandbox_file_path-str-homeuser","title":"<code>upload_file_from_local_to_sandbox(sandbox_id: str, local_file_path: str, sandbox_file_path: str = \"/home/user\")</code>","text":"<p>Upload local files to the sandbox environment.</p> <p>When to Use</p> <p>When a local file is provided to the agent, the agent needs to call this tool to copy the file from local storage to the sandbox for further file processing.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>local_file_path</code>: Local path of the file to upload - <code>sandbox_file_path</code>: Target directory in sandbox (default: <code>/home/user</code>)</p> <p>Returns: - <code>str</code>: Path of uploaded file in sandbox or error message</p>"},{"location":"tool_python/#download_file_from_internet_to_sandboxsandbox_id-str-url-str-sandbox_file_path-str-homeuser","title":"<code>download_file_from_internet_to_sandbox(sandbox_id: str, url: str, sandbox_file_path: str = \"/home/user\")</code>","text":"<p>Download files from the internet directly to the sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>url</code>: URL of the file to download - <code>sandbox_file_path</code>: Target directory in sandbox (default: <code>/home/user</code>)</p> <p>Returns: - <code>str</code>: Path of downloaded file in sandbox or error message</p> <p>Features: - Automatic retry mechanism</p>"},{"location":"tool_python/#download_file_from_sandbox_to_localsandbox_id-str-sandbox_file_path-str-local_filename-str-none","title":"<code>download_file_from_sandbox_to_local(sandbox_id: str, sandbox_file_path: str, local_filename: str = None)</code>","text":"<p>Download files from sandbox to local system for processing by other tools.</p> <p>Inter-tool Communication</p> <p>Other MCP tools (such as visual question answering) cannot access files in a sandbox. Therefore, this tool should be called when the agent wants other tools to analyze files in the sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the sandbox - <code>sandbox_file_path</code>: Path of file in sandbox - <code>local_filename</code>: Optional local filename (uses original if not provided)</p> <p>Returns: - <code>str</code>: Local path of downloaded file or error message</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_reasoning/","title":"Reasoning Tools (<code>reasoning_mcp_server.py</code>)","text":"<p>The Reasoning MCP Server provides a pure text-based reasoning engine. It supports logical analysis, problem solving, and planning, using LLM backends (OpenAI or Anthropic) with retry and exponential backoff for robustness.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Pure Text Reasoning: Logical analysis and problem solving using advanced LLM backends</li> <li>Step-by-Step Analysis: Structured reasoning with detailed explanations</li> <li>Multi-Backend Support: OpenAI or Anthropic models with automatic fallback</li> </ul>"},{"location":"tool_reasoning/#environment-variables","title":"Environment Variables","text":"<p>Configuration Location</p> <p>The <code>reasoning_mcp_server.py</code> reads environment variables that are passed through the <code>tool-reasoning.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <p>OpenAI Configuration:</p> <ul> <li><code>OPENAI_API_KEY</code>: Required API key for OpenAI services</li> <li><code>OPENAI_BASE_URL</code>: Default = <code>https://api.openai.com/v1</code></li> <li><code>OPENAI_MODEL_NAME</code>: Default = <code>o3</code></li> </ul> <p>Anthropic Configuration:</p> <ul> <li><code>ANTHROPIC_API_KEY</code>: Required API key for Anthropic services</li> <li><code>ANTHROPIC_BASE_URL</code>: Default = <code>https://api.anthropic.com</code></li> <li><code>ANTHROPIC_MODEL_NAME</code>: Default = <code>claude-3-7-sonnet-20250219</code></li> </ul>"},{"location":"tool_reasoning/#function-reference","title":"Function Reference","text":"<p>The following function is provided by the <code>reasoning_mcp_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_reasoning/#reasoningquestion-str","title":"<code>reasoning(question: str)</code>","text":"<p>Perform step-by-step reasoning, analysis, and planning over a text-only input. This tool is specialized for complex thinking tasks.</p> <p>Text-Only Processing</p> <p>This tool processes only the provided text input and will not fetch external data or context. Ensure all necessary information is included in the question.</p> <p>Parameters:</p> <ul> <li><code>question</code>: A detailed, complex question or problem statement that includes all necessary information</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: A structured, step-by-step reasoned answer</li> </ul> <p>Features:</p> <ul> <li>Runs on OpenAI or Anthropic models, depending on available API keys</li> <li>Exponential backoff retry logic (up to 5 attempts)</li> <li>For Anthropic, uses Thinking mode with token budget (21k max, 19k thinking)</li> <li>Ensures non-empty responses with fallback error reporting</li> <li>Automatic backend selection based on available API keys</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_searching/","title":"Searching Tools (<code>searching_mcp_server.py</code>)","text":"<p>The Searching MCP Server provides comprehensive search capabilities including Google search, Wikipedia content retrieval, archive searching, and web scraping functionality.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Google Search: Comprehensive web search with filtering and localization</li> <li>Wikipedia Access: Page content retrieval and revision history tracking</li> <li>Archive Search: Wayback Machine integration for historical web content</li> <li>Web Scraping: Content extraction from websites and YouTube videos</li> </ul>"},{"location":"tool_searching/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables configure the search tools:</p> <ul> <li><code>SERPER_API_KEY</code>: Required API key for Serper service, used by <code>google_search</code> and as a fallback for <code>scrape_website</code></li> <li><code>JINA_API_KEY</code>: Required API key for JINA service. Default choice for scraping websites in <code>scrape_website</code></li> <li><code>REMOVE_SNIPPETS</code>: Set to \"true\" to filter out snippets from results. Used in <code>google_search</code> to filter the search results returned by Serper</li> <li><code>REMOVE_KNOWLEDGE_GRAPH</code>: Set to \"true\" to remove knowledge graph data. Used in <code>google_search</code> to filter the search results returned by Serper</li> <li><code>REMOVE_ANSWER_BOX</code>: Set to \"true\" to remove answer box content. Used in <code>google_search</code> to filter the search results returned by Serper</li> </ul>"},{"location":"tool_searching/#function-reference","title":"Function Reference","text":"<p>The following functions are provided by the <code>searching_mcp_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_searching/#google_searchq-str-gl-str-us-hl-str-en-location-str-none-num-int-10-tbs-str-none-page-int-1","title":"<code>google_search(q: str, gl: str = \"us\", hl: str = \"en\", location: str = None, num: int = 10, tbs: str = None, page: int = 1)</code>","text":"<p>Perform Google searches via Serper API and retrieve rich search results including organic results, people also ask, related searches, and knowledge graph.</p> <p>Parameters:</p> <ul> <li><code>q</code>: Search query string</li> <li><code>gl</code>: Country context for search (e.g., 'us' for United States, 'cn' for China, 'uk' for United Kingdom). Default: 'us'</li> <li><code>hl</code>: Google interface language (e.g., 'en' for English, 'zh' for Chinese, 'es' for Spanish). Default: 'en'</li> <li><code>location</code>: City-level location for search results (e.g., 'SoHo, New York, United States', 'California, United States')</li> <li><code>num</code>: Number of results to return. Default: 10</li> <li><code>tbs</code>: Time-based search filter ('qdr:h' for past hour, 'qdr:d' for past day, 'qdr:w' for past week, 'qdr:m' for past month, 'qdr:y' for past year)</li> <li><code>page</code>: Page number of results to return. Default: 1</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: JSON formatted search results with organic results and related information</li> </ul> <p>Features:</p> <ul> <li>Automatic retry mechanism (up to 5 attempts)</li> <li>Configurable result filtering via environment variables</li> <li>Support for regional and language-specific searches</li> </ul>"},{"location":"tool_searching/#wiki_get_page_contententity-str-first_sentences-int-10","title":"<code>wiki_get_page_content(entity: str, first_sentences: int = 10)</code>","text":"<p>Get specific Wikipedia page content for entities (people, places, concepts, events) and return structured information.</p> <p>Parameters:</p> <ul> <li><code>entity</code>: The entity to search for in Wikipedia</li> <li><code>first_sentences</code>: Number of first sentences to return from the page. Set to 0 to return full content. Default: 10</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted content containing page title, introduction/full content, and URL</li> </ul> <p>Features:</p> <ul> <li>Handles disambiguation pages automatically</li> <li>Provides clean, structured output</li> <li>Fallback search suggestions when page not found</li> <li>Automatic content truncation for manageable output</li> </ul>"},{"location":"tool_searching/#search_wiki_revisionentity-str-year-int-month-int-max_revisions-int-50","title":"<code>search_wiki_revision(entity: str, year: int, month: int, max_revisions: int = 50)</code>","text":"<p>Search for an entity in Wikipedia and return the revision history for a specific month.</p> <p>Parameters:</p> <ul> <li><code>entity</code>: The entity to search for in Wikipedia</li> <li><code>year</code>: The year of the revision (e.g., 2024)</li> <li><code>month</code>: The month of the revision (1-12)</li> <li><code>max_revisions</code>: Maximum number of revisions to return. Default: 50</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted revision history with timestamps, revision IDs, and URLs</li> </ul> <p>Features:</p> <ul> <li>Automatic date validation and adjustment</li> <li>Support for date range from 2000 to current year</li> <li>Detailed revision metadata including timestamps and direct links</li> <li>Clear error handling for invalid dates or missing pages</li> </ul>"},{"location":"tool_searching/#search_archived_webpageurl-str-year-int-month-int-day-int","title":"<code>search_archived_webpage(url: str, year: int, month: int, day: int)</code>","text":"<p>Search the Wayback Machine (archive.org) for archived versions of a webpage for a specific date.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL to search for in the Wayback Machine</li> <li><code>year</code>: The target year (e.g., 2023)</li> <li><code>month</code>: The target month (1-12)</li> <li><code>day</code>: The target day (1-31)</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted archive information including archived URL, timestamp, and availability status</li> </ul> <p>Features:</p> <ul> <li>Automatic URL protocol detection and correction</li> <li>Date validation and adjustment (1995 to present)</li> <li>Fallback to most recent archive if specific date not found</li> <li>Special handling for Wikipedia URLs with tool suggestions</li> <li>Automatic retry mechanism for reliable results</li> </ul>"},{"location":"tool_searching/#scrape_websiteurl-str","title":"<code>scrape_website(url: str)</code>","text":"<p>Scrape website content including support for regular websites and YouTube video information.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL of the website to scrape</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Scraped website content including text, metadata, and structured information</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_vqa/","title":"Vision Tools (<code>vision_mcp_server.py</code>)","text":"<p>The Vision MCP Server enables OCR + Visual Question Answering (VQA) over images and multimodal understanding of YouTube videos, with pluggable backends (Anthropic, OpenAI, Google Gemini).</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Visual Question Answering: OCR and VQA analysis of images with dual-pass processing</li> <li>YouTube Video Analysis: Audio and visual analysis of public YouTube videos</li> <li>Multi-Backend Support: Configurable vision backends (Anthropic, OpenAI, Gemini)</li> </ul>"},{"location":"tool_vqa/#environment-variables","title":"Environment Variables","text":"<p>Configuration Location</p> <p>The <code>vision_mcp_server.py</code> reads environment variables that are passed through the <code>tool-image-video.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <p>Vision Backend Control:</p> <ul> <li><code>ENABLE_CLAUDE_VISION</code>: <code>\"true\"</code> to allow Anthropic Vision backend</li> <li><code>ENABLE_OPENAI_VISION</code>: <code>\"true\"</code> to allow OpenAI Vision backend</li> </ul> <p>Anthropic Configuration:</p> <ul> <li><code>ANTHROPIC_API_KEY</code>: Required API key for Anthropic services</li> <li><code>ANTHROPIC_BASE_URL</code>: Default = <code>https://api.anthropic.com</code></li> <li><code>ANTHROPIC_MODEL_NAME</code>: Default = <code>claude-3-7-sonnet-20250219</code></li> </ul> <p>OpenAI Configuration:</p> <ul> <li><code>OPENAI_API_KEY</code>: Required API key for OpenAI services</li> <li><code>OPENAI_BASE_URL</code>: Default = <code>https://api.openai.com/v1</code></li> <li><code>OPENAI_MODEL_NAME</code>: Default = <code>gpt-4o</code></li> </ul> <p>Gemini Configuration:</p> <ul> <li><code>GEMINI_API_KEY</code>: Required API key for Google Gemini services</li> <li><code>GEMINI_MODEL_NAME</code>: Default = <code>gemini-2.5-pro</code></li> </ul>"},{"location":"tool_vqa/#function-reference","title":"Function Reference","text":"<p>The following functions are provided by the <code>vision_mcp_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_vqa/#visual_question_answeringimage_path_or_url-str-question-str","title":"<code>visual_question_answering(image_path_or_url: str, question: str)</code>","text":"<p>Ask questions about an image using a dual-pass analysis approach for comprehensive understanding.</p> <p>Two-Pass Analysis</p> <p>This function runs two passes:</p> <ol> <li>OCR pass using the selected vision backend with a meticulous extraction prompt</li> <li>VQA pass that analyzes the image and cross-checks against OCR text</li> </ol> <p>Parameters:</p> <ul> <li><code>image_path_or_url</code>: Local path (accessible to server) or web URL. HTTP URLs are auto-upgraded/validated to HTTPS for some backends</li> <li><code>question</code>: The user's question about the image</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Concatenated text with:<ul> <li><code>OCR results: ...</code></li> <li><code>VQA result: ...</code></li> </ul> </li> </ul> <p>Features:</p> <ul> <li>Automatic MIME detection, reads magic bytes, falls back to extension, final default is <code>image/jpeg</code></li> <li>Multi-backend support for different vision models</li> <li>Cross-validation between OCR and VQA results</li> </ul>"},{"location":"tool_vqa/#visual_audio_youtube_analyzingurl-str-question-str-provide_transcribe-bool-false","title":"<code>visual_audio_youtube_analyzing(url: str, question: str = \"\", provide_transcribe: bool = False)</code>","text":"<p>Analyze public YouTube videos (audio + visual). Supports watch pages, Shorts, and Live VODs.</p> <p>Supported URL Patterns</p> <p>Accepted URL patterns: <code>youtube.com/watch</code>, <code>youtube.com/shorts</code>, <code>youtube.com/live</code></p> <p>Parameters:</p> <ul> <li><code>url</code>: YouTube video URL (publicly accessible)</li> <li><code>question</code> (optional): A specific question about the video. You can scope by time using <code>MM:SS</code> or <code>MM:SS-MM:SS</code> (e.g., <code>01:45</code>, <code>03:20-03:45</code>)</li> <li><code>provide_transcribe</code> (optional, default <code>False</code>): If <code>True</code>, returns a timestamped transcription including salient events and brief visual descriptions</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Transcription of the video (if requested) and answer to the question</li> </ul> <p>Features:</p> <ul> <li>Gemini-powered video analysis (requires <code>GEMINI_API_KEY</code>)</li> <li>Dual mode: full transcript, targeted Q&amp;A, or both</li> <li>Time-scoped question answering for specific video segments</li> <li>Support for multiple YouTube video formats</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"xbench_ds/","title":"xbench-DeepSearch","text":"<p>The xbench benchmark is an evaluation framework designed to measure both the intelligence frontier and real-world utility of AI agents. It consists of complementary tracks that test core model capabilities like reasoning, tool use, memory, and workflows grounded in business and professional settings. Its DeepSearch sub-track measures agents\u2019 ability to conduct open-domain information retrieval, combining fact finding, comparison, and synthesis through multi-step search and tool use.</p> <p>See more details at xbench official website and xbench-DeepSearch Eval Card.</p>"},{"location":"xbench_ds/#setup-and-evaluation-guide","title":"Setup and Evaluation Guide","text":""},{"location":"xbench_ds/#step-1-download-the-xbench-deepsearch-dataset","title":"Step 1: Download the xbench-DeepSearch Dataset","text":"<p>Direct Download (Recommended)</p> <p>Dataset Setup</p> <p>Use the integrated prepare-benchmark command to download and process the dataset:</p> <pre><code>uv run main.py prepare-benchmark get xbench-ds\n</code></pre> <p>By default, this will create the standardized dataset at data/xbench-ds/standardized_data.jsonl.</p>"},{"location":"xbench_ds/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Required API Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM as judge, reasoning, and O3 hints\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"xbench_ds/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_xbench-ds \\\n  output_dir=\"logs/xbench-ds/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"xbench_ds/#step-4-monitor-progress-and-resume","title":"Step 4: Monitor Progress and Resume","text":"<p>Progress Tracking</p> <p>You can monitor the evaluation progress in real-time:</p> Check Progress<pre><code>uv run utils/progress_check/check_xbench_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p> <p>Resume Capability</p> <p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_xbench-ds \\\n  output_dir=\"logs/xbench-ds/20250922_1430\"\n</code></pre>"},{"location":"xbench_ds/#post-processing-for-enhanced-performance","title":"Post-Processing for Enhanced Performance","text":"<p>Test-Time Scaling for Improved Reliability</p> <p>Test-time scaling can significantly improve the reliability of model responses. Instead of simple majority voting, we employ a comprehensive parallel thinking approach that:</p> <ul> <li>Aggregates final summary steps from each agent run before outputting results</li> <li>Uses another agent (o3 by default) to make final decisions based on equivalence and source reliability criteria</li> <li>Provides more robust and accurate final answers</li> </ul> <p>Execute the following command to run multiple xbench-DeepSearch evaluations and automatically enable parallel thinking for enhanced performance.</p> Multiple runs with parallel thinking post-processing<pre><code>bash scripts/run_evaluate_mulitple_runs_xbench-ds.sh\n</code></pre>"},{"location":"xbench_ds/#running-parallel-thinking-analysis-alone","title":"Running Parallel Thinking Analysis alone","text":"<p>After completing evaluations (single or multiple runs), you can apply parallel thinking post-processing to aggregate and generate the final result.</p> Parallel Thinking Post-Processing<pre><code>uv run utils/util_llm_parallel_thinking.py \\\n  --benchmark xbench-ds \\\n  --results_dir \"logs/xbench-ds/20250922_1430\"\n</code></pre> <p>The program automatically reads results from each run in the specified directory and performs aggregated analysis. The final output files are generated in the <code>results_dir</code>:</p> <ul> <li><code>llm_parallel_thinking_Nruns.json</code> - Detailed analysis results</li> <li><code>llm_parallel_thinking_accuracy_Nruns.txt</code> - Final accuracy</li> </ul> <p>Where <code>N</code> represents the total number of experimental runs (minimum of 1).</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"yaml_config/","title":"YAML Configuration Guide","text":"<p>MiroFlow uses a Hydra-based configuration system for customizing AI agents, tools, and benchmarks.</p>"},{"location":"yaml_config/#configuration-structure","title":"Configuration Structure","text":"Configuration Directory<pre><code>config/\n\u251c\u2500\u2500 agent_*.yaml                      # Agent configurations\n\u251c\u2500\u2500 agent_prompts/                    # Prompt classes\n\u251c\u2500\u2500 benchmark/                        # Benchmark settings\n\u2514\u2500\u2500 tool/                             # Tool configurations\n</code></pre>"},{"location":"yaml_config/#quick-start","title":"Quick Start","text":"<p>Run Benchmarks <pre><code># GAIA validation\nuv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation \\\n  output_dir=\"logs/gaia-validation/$(date +\"%Y%m%d_%H%M\")\"\n\n# GAIA text-only\nuv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation-text-only \\\n  output_dir=\"logs/gaia-validation-text-only/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre></p> <p>Single Task <pre><code>uv run main.py trace \\\n  --config_file_name=agent_quickstart_1 \\\n  --task=\"Your task here\" \\\n  --task_file_name=\"data/file.xlsx\"\n</code></pre></p>"},{"location":"yaml_config/#core-configuration","title":"Core Configuration","text":""},{"location":"yaml_config/#basic-agent-setup","title":"Basic Agent Setup","text":"Basic Agent Configuration<pre><code>defaults:\n  - benchmark: gaia-validation\n  - override hydra/job_logging: none\n  - _self_\n\nmain_agent:\n  prompt_class: MainAgentPromptBoxedAnswer\n  llm:\n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.3\n    max_tokens: 32000\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n\n  tool_config: []  # Tools for main agent\n  max_turns: -1    # -1 = unlimited\n\nsub_agents:\n  agent-worker:\n    prompt_class: SubAgentWorkerPrompt\n    llm:\n      provider_class: \"ClaudeOpenRouterClient\"\n      model_name: \"anthropic/claude-3.7-sonnet\"\n    tool_config:\n      - tool-reading\n      - tool-searching\n    max_turns: -1\n\noutput_dir: logs/\ndata_dir: \"${oc.env:DATA_DIR,data}\"\n</code></pre>"},{"location":"yaml_config/#llm-providers","title":"LLM Providers","text":"<p>Available Providers</p> <ul> <li>Claude: <code>ClaudeOpenRouterClient</code>, <code>ClaudeAnthropicClient</code></li> <li>OpenAI: <code>GPTOpenAIClient</code></li> <li>MiroThinker: <code>MiroThinkerSGLangClient</code></li> <li>Qwen: <code>QwenSGLangClient</code></li> <li>DeepSeek: <code>DeepSeekNewAPIClient</code> (limited support)</li> </ul> <p>See LLM Clients Overview for details.</p>"},{"location":"yaml_config/#available-tools","title":"Available Tools","text":"<p>Tool Options</p> <ul> <li><code>tool-reasoning</code>: Enhanced reasoning capabilities</li> <li><code>tool-searching</code>: Web search and retrieval</li> <li><code>tool-reading</code>: Document processing</li> <li><code>tool-code</code>: Python code execution</li> <li><code>tool-image-video</code>: Visual content analysis</li> <li><code>tool-audio</code>: Audio processing</li> <li><code>tool-browsing</code>: Web browsing</li> </ul> <p>See Tool Overview for configurations.</p>"},{"location":"yaml_config/#advanced-features","title":"Advanced Features","text":""},{"location":"yaml_config/#gaia-benchmark-configuration","title":"GAIA Benchmark Configuration","text":"GAIA-Optimized Setup<pre><code>main_agent:\n  prompt_class: MainAgentPrompt_GAIA\n  tool_config:\n    - tool-reasoning\n\n  input_process:\n    o3_hint: true              # Use O3 for task hints\n  output_process:\n    o3_final_answer: true      # Use O3 for answer extraction\n\nsub_agents:\n  agent-worker:\n    tool_config:\n      - tool-searching\n      - tool-reading\n      - tool-code\n      - tool-image-video\n      - tool-audio\n</code></pre>"},{"location":"yaml_config/#benchmark-settings","title":"Benchmark Settings","text":"Benchmark Configuration<pre><code>name: \"your-benchmark\"\ndata:\n  data_dir: \"${data_dir}/your-data\"\nexecution:\n  max_tasks: null      # null = no limit\n  max_concurrent: 3    # Parallel tasks\n  pass_at_k: 1         # Attempts per task\n</code></pre>"},{"location":"yaml_config/#environment-variables","title":"Environment Variables","text":"Required .env Configuration<pre><code># LLM Providers\nOPENROUTER_API_KEY=\"your_key\"\nANTHROPIC_API_KEY=\"your_key\"\nOPENAI_API_KEY=\"your_key\"\n\n# Tools\nSERPER_API_KEY=\"your_key\"\nJINA_API_KEY=\"your_key\"\nE2B_API_KEY=\"your_key\"\n\n# Optional\nDATA_DIR=\"data/\"\nCHINESE_CONTEXT=\"false\"\n</code></pre>"},{"location":"yaml_config/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>temperature</code> LLM creativity (0.0-1.0) 0.3 <code>max_tokens</code> Response length limit 32000 <code>max_turns</code> Conversation turns (-1 = unlimited) -1 <code>max_tool_calls_per_turn</code> Tool calls per turn 10 <code>max_concurrent</code> Parallel benchmark tasks 3"},{"location":"yaml_config/#best-practices","title":"Best Practices","text":"<p>Quick Tips</p> <ul> <li>Start simple: Use <code>agent_quickstart_1.yaml</code> as a base</li> <li>Tool selection: Choose tools based on your task requirements</li> <li>API keys: Always use environment variables, never hardcode</li> <li>Resource limits: Set <code>max_concurrent</code> and <code>max_tokens</code> appropriately</li> <li>Development: Use higher <code>temperature</code> and unlimited <code>max_turns</code> for exploration</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"}]}