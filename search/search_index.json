{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MiroFlow","text":""},{"location":"#what-is-miroflow","title":"\ud83d\ude80 What is MiroFlow?","text":"<p>MiroFlow is an agentic AI platform for building intelligent agents with flexible tool integration and comprehensive benchmark evaluations.</p>"},{"location":"#recent-updates","title":"\ud83d\udcdd Recent Updates","text":"<p>Latest Changes &amp; Improvements</p> <p>Oct 2025 - </p> <ul> <li>\ud83d\udcdd Added support for HLE-Text evaluation benchmark #81</li> <li>\ud83e\udde0 Added support for HLE (Humanity's Last Exam) benchmark #79</li> <li>\ud83c\udf10 Added support for BrowseComp-EN evaluation benchmark #78</li> <li> <p>\ud83d\udd0c Added support for MiroAPI integration #76</p> </li> <li> <p>\ud83d\udcca Added support for FinSearchComp evaluation benchmark #51</p> </li> <li>\ud83d\udd0d Added support for XBench-DS (Deep Search) evaluation #47</li> <li>\ud83e\udde0 Updated o3 hints and summary to more models #58</li> <li>\u2728 Added support for GPT-5 integration #52</li> <li>\ud83d\udd27 Improved tool logs and per-task log storage #69</li> <li>\ud83e\udd16 Added support for single agent mode #67</li> <li>\ud83d\udcda Added comprehensive collection of agentic AI research papers #65</li> </ul>"},{"location":"#quick-start","title":"\ud83c\udfaf Quick Start","text":"<p>Ready to get started? Choose your path:</p> <p>Get Started in Minutes</p> Quick SetupEvaluationDevelopment <p>Jump right in with our quickstart guide:</p> <p>Get Started </p> <p>Evaluate existing models on benchmarks:</p> <p>Run Evaluations </p> <p>Build your own AI agents:</p> <p>Core Concepts </p>"},{"location":"#ecosystem","title":"\ud83d\udd17 Ecosystem","text":"<p>Explore the complete MiroMind AI ecosystem:</p> <p>MiroMind AI Products</p> Name Description Link MiroFlow Core framework for AI agent platform Documentation MiroThinker State-of-the-art agent foundation models Hugging Face MiroVerse Curated datasets for model training Dataset MiroTrain Complete training recipes and tools GitHub <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI \u00b7 Version: v0.3</p>"},{"location":"all_about_agents/","title":"\ud83d\udcda All About Agents","text":"<p>Welcome to our comprehensive resource collection for AI agents. This page curates valuable tools, frameworks, research papers, and learning materials to help you understand and build sophisticated agent systems.</p>"},{"location":"all_about_agents/#table-of-contents","title":"Table of Contents","text":"<p>Resource Categories</p> <ol> <li>Agent Papers</li> <li>Agent Frameworks</li> <li>Evaluation</li> <li>Agent Memory</li> <li>Blogs</li> </ol>"},{"location":"all_about_agents/#agent-papers","title":"\ud83d\udd2c Agent Papers","text":"<p>Research Papers &amp; Publications</p> <p>Latest research in agent systems, methodologies, and theoretical foundations.</p> <p>P001 - WebThinker: Empowering Large Reasoning Models with Deep Research Capability     -  Paper \u00b7  GitHub</p> <p>P002 - Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld      -  Paper</p> <p>P003 - AFlow: Automating Agentic Workflow Generation      -  Paper</p> <p>P004 - AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs      -  Paper</p> <p>P005 - Throttling Web Agents Using Reasoning Gates     -  Paper</p> <p>P006 - The Landscape of Agentic Reinforcement Learning for LLMs: A Survey     -  Paper</p> <p>P007 - BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair     -  Paper \u00b7  GitHub</p> <p>P008 - Long Term Memory: The Foundation of AI Self-Evolution     -  Paper</p> <p>P009 - DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments     -  Paper \u00b7  GitHub</p> <p>P010 - Web-Shepherd: Advancing PRMs for Reinforcing Web Agents     -  Paper \u00b7  GitHub</p> <p>P011 - SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis     -  Paper \u00b7  GitHub</p> <p>P012 - Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution     -  Paper \u00b7  GitHub</p> <p>P013 - MCP-Zero: Active Tool Discovery for Autonomous LLM Agents     -  Paper \u00b7  GitHub</p> <p>P014 - AgentOrchestra: Orchestrating Hierarchical Multi-Agent Intelligence with the Tool-Environment-Agent(TEA) Protocol     -  Paper \u00b7  GitHub</p> <p>P015 - Deep Research Agents: A Systematic Examination And Roadmap     -  Paper \u00b7  GitHub</p> <p>P016 - SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?     -  Paper \u00b7  GitHub</p> <p>P017 - Deep Researcher with Test-Time Diffusion: Enhancing research capabilities through diffusion-based test-time adaptation     -  Paper</p> <p>P018 - Multi-Agent Tool-Integrated Policy Optimization: Enhancing multi-agent systems through integrated tool usage and policy optimization     -  Paper</p> <p>P019 - WALT: Web Agents that Learn Tools     -  Paper</p> <p>P020 - Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation     -  Paper</p> <p>P021 - SurveyBench: Can LLM(-Agents) Write Academic Surveys that Align with Reader Needs?     -  Paper</p> <p>P022 - FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents     -  Paper</p> <p>P023 - LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions     -  Paper</p> <p>P024 - Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models     -  Paper</p> <p>P025 - MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning     -  Paper</p> <p>P026 - QuantAgents: Towards Multi-agent Financial System via Simulated Trading     -  Paper \u00b7  Project</p> <p>P027 - Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade-offs     -  Paper</p> <p>P028 - Open Agent Specification (Agent Spec): Technical Report     -  Paper</p> <p>P029 - AudioToolAgent: An Agentic Framework for Audio-Language Models     -  Paper</p> <p>P030 - ThinkBrake: Mitigating Overthinking in Tool Reasoning     -  Paper</p> <p>P031 - TOUCAN: Synthesizing 1.5M Tool-Agentic Trajectories from Real Environments     -  Paper</p> <p>P032 - ToolTweak: An Attack on Tool Selection in LLM-Based Agents     -  Paper</p> <p>P033 - ToolBrain: A Flexible RL Framework for Agentic Tools     -  Paper</p> <p>P034 - TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture     -  Paper</p> <p>P035 - WebDancer: Towards Autonomous Information Seeking Agency     -  Paper</p> <p>P036 - WebSailor: Navigating Super-human Reasoning for Web Agent     -  Paper</p> <p>P037 - WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization     -  Paper</p> <p>P038 - WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent     -  Paper</p> <p>P039 - WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning     -  Paper</p> <p>P040 - WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents     -  Paper</p> <p>P041 - Scaling Agents via Continual Pre-training: Enhancing agent capabilities through continuous learning approaches     -  Paper</p> <p>P042 - Towards General Agentic Intelligence via Environment Scaling: Advancing general AI through scalable environment interactions     -  Paper</p> <p>P043 - WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research     -  Paper</p> <p>P044 - ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization     -  Paper</p> <p>P045 - Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents     -  Paper</p> <p>P046 - AgentFlow: In-the-Flow Agentic System Optimization: Effective Planning and Tool Use     -  Paper \u00b7  GitHub</p> <p>P047 - ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems     -  Paper</p> <p>P048 - Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping     -  Paper</p> <p>P049 - CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards     -  Paper</p> <p>P050 - Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window     -  Paper</p> <p>P051 - Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks     -  Paper</p> <p>P052 - MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning     -  Paper</p> <p>P053 - Agent Learning via Early Experience     -  Paper</p> <p>P054 - CaRT: Teaching LLM Agents to Know When They Know Enough     -  Paper</p> <p>P055 - AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents     -  Paper</p> <p>P056 - Opponent Shaping in LLM Agents     -  Paper</p> <p>P057 - NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions     -  Paper</p> <p>P058 - VoiceAgentBench: Are Voice Assistants ready for agentic tasks?     -  Paper</p> <p>P059 - Self-Improving LLM Agents at Test-Time     -  Paper</p> <p>P060 - AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework     -  Paper</p> <p>P061 - Adaptive Tool Generation with Models as Tools and Reinforcement Learning     -  Paper</p> <p>P062 - TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents     -  Paper</p> <p>P063 - A Survey on Agentic Security: Applications, Threats and Defenses     -  Paper</p> <p>P064 - A Multi-Agent Framework for Stateful Inference-Time Search     -  Paper</p> <p>P065 - AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning     -  Paper</p> <p>P066 - Democratizing AI Scientists using ToolUniverse     -  Paper \u00b7  GitHub</p> <p>P067 - Dyna-Mind: Learning to Simulate from Experience for Better AI Agents     -  Paper</p> <p>P068 - DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents     -  Paper</p> <p>P069 - DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning     -  Paper</p> <p>P070 - MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding     -  Paper</p> <p>P071 - MASA: LLM-Driven Multi-Agent Systems for Autoformalization     -  Paper</p> <p>P072 - Exploiting Web Search Tools of AI Agents for Data Exfiltration     -  Paper</p> <p>P073 - Auto-scaling Continuous Memory for GUI Agent     -  Paper</p> <p>P074 - StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models     -  Paper</p> <p>P075 - WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent     -  Paper</p> <p>P076 - LLM\u00d7MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System     -  Paper</p> <p>P077 - BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions     -  Paper</p> <p>P078 - AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation     -  Paper</p> <p>P079 - FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth     -  Paper</p> <p>P080 - MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems     -  Paper</p> <p>P081 - Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?     -  Paper</p> <p>P082 - A Survey on Agentic Multimodal Large Language Models     -  Paper</p>"},{"location":"all_about_agents/#agent-frameworks","title":"\ud83d\udee0\ufe0f Agent Frameworks","text":"<p>Popular Agent Development Frameworks</p> <p>Comprehensive frameworks for building and deploying AI agents across different domains.</p> <p>F001 - MiroFlow: Build, manage, and scale your AI agents with ease     -  GitHub</p> <p>F002 - Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models     -  GitHub</p> <p>F003 - OpenManus: No fortress, purely open ground. OpenManus is Coming     -  GitHub</p> <p>F004 - OpenBB Platform: Financial data platform for analysts, quants and AI agents      -  Project</p> <p>F005 - TradingAgents: Multi-Agents LLM Financial Trading Framework     -  Paper \u00b7  GitHub</p> <p>F006 - JoyAgent-JDGenie: Technical Report on the GAIA     -  Paper \u00b7  GitHub</p>"},{"location":"all_about_agents/#evaluation","title":"\ud83d\udcca Evaluation","text":"<p>Benchmarks &amp; Evaluation Frameworks</p> <p>Comprehensive evaluation tools and benchmarks for measuring agent performance across various tasks.</p> <p>E001 - LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries      -  Paper</p> <p>E002 - BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent      -  Paper</p> <p>E003 - HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering     -  Paper</p> <p>E004 - GAIA: a benchmark for General AI Assistants      -  Paper \u00b7  Leaderboard</p> <p>E005 - xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations      -  Paper</p> <p>E006 - MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers      -  Paper</p> <p>E007 - FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction      -  Paper</p> <p>E008 - Terminal-Bench: the benchmark for testing AI agents in real terminal environments      -  GitHub</p> <p>E009 - Gaia2 and ARE: Empowering the Community to Evaluate Agents     -  Blog Post</p> <p>E010 - GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark     -  Paper \u00b7  GitHub</p> <p>E011 - WebWalkerQA: WebWalker: Benchmarking LLMs in Web Traversal     -  Paper \u00b7  GitHub \u00b7  Leaderboard</p> <p>E012 - HLE: Humanity's Last Exam     -  Paper \u00b7  Website</p> <p>E013 - BFCL: Berkeley Function Calling Leaderboard     -  GitHub \u00b7  Leaderboard</p> <p>E014 - When2Call: When (not) to Call Tools     -  Paper \u00b7  GitHub</p> <p>E015 - ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities     -  Paper \u00b7  GitHub</p> <p>E016 - ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models     -  Paper \u00b7  GitHub</p> <p>E017 - SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines     -  Paper \u00b7  Website</p> <p>E018 - Terminal-Bench: A benchmark for testing AI agents in terminal environments     -  Leaderboard \u00b7  Website</p> <p>E019 - \u03c4-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains     -  Paper \u00b7  GitHub</p> <p>E020 - \u03c42-Bench: Evaluating Conversational Agents in a Dual-Control Environment     -  Paper \u00b7  GitHub</p> <p>E021 - Deep Research Bench: Evaluating AI Web Research Agents     -  Paper \u00b7  Website</p> <p>E022 - Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents     -  Paper</p> <p>E023 - TRAJECT-Bench: A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use     -  Paper</p> <p>E024 - ARC-AGI: The General Intelligence Benchmark     -  Website</p> <p>E025 - Demystifying Deep Search: A Holistic Evaluation with Hint-Free Multi-Hop Questions and Factorised Metrics     -  Paper</p> <p>E026 - BrowseComp-VL: A Comprehensive Benchmark for Vision-Language Web Browsing     -  Paper</p> <p>E027 - ACEBench: Who Wins the Match Point in Tool Usage?     -  Paper</p> <p>E028 - Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation     -  Paper \u00b7  GitHub</p> <p>E029 - DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation     -  Paper</p> <p>E030 - When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents     -  Paper</p> <p>E031 - A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System     -  Paper</p>"},{"location":"all_about_agents/#agent-memory","title":"\ud83e\udde0 Agent Memory","text":"<p>Memory Systems for Persistent Agent Intelligence</p> <p>Advanced memory solutions for building agents with long-term context and learning capabilities.</p> <p>M001 - Mem0: Building Production- Ready AI Agents with Scalable Long-Term Memory     -  GitHub</p> <p>M002 - memobase: Profile-Based Long-Term Memory for AI Applications     -  GitHub</p> <p>M003 - Memento: Fine-tuning LLM Agents without Fine-tuning LLMs     -  Paper \u00b7  GitHub</p> <p>M004 - MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments     -  Paper</p> <p>M005 - A-MEM: Agentic Memory for LLM Agents     -  Paper \u00b7  GitHub</p> <p>M006 - MemoryOS: Memory OS of AI Agent     -  Paper \u00b7  GitHub</p> <p>M007 - Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning     -  Paper</p> <p>M008 - HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models     -  Paper \u00b7  GitHub</p> <p>M009 - MaxKB: Open-source platform for building enterprise-grade agents     -  GitHub</p> <p>M010 - MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent     -  Paper \u00b7  Website</p> <p>M011 - LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation     -  Paper</p> <p>M012 - Memp: Exploring Agent Procedural Memory     -  Paper</p> <p>M013 - MIRIX: Multi-Agent Memory System for LLM-Based Agents     -  Paper \u00b7  Website</p> <p>M014 - A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory     -  Paper</p> <p>M015 - ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory     -  Paper</p> <p>M016 - CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension     -  Paper</p> <p>M017 - Mem-\u03b1: Learning Memory Construction via Reinforcement Learning     -  Paper</p> <p>M018 - Preference-Aware Memory Update for Long-Term LLM Agents     -  Paper</p>"},{"location":"all_about_agents/#blogs","title":"Blogs","text":"<p>Blog Posts &amp; Tutorials</p> <p>Curated collection of blog posts, tutorials, and articles about AI agents from various sources and languages.</p>"},{"location":"all_about_agents/#general-blogs","title":"General Blogs","text":"<ul> <li> <p>ChatGPT Agent: Introducing ChatGPT Agent</p> <ul> <li> Blog Post \u00b7 OpenAI's latest agent capabilities and features</li> </ul> </li> <li> <p>Tongyi DeepResearch: Deep Research Agent for Complex Tasks</p> <ul> <li> Blog Post \u00b7 Alibaba's advanced research agent system</li> </ul> </li> </ul>"},{"location":"all_about_agents/#chinese-blogs","title":"Chinese Blogs","text":"<p>\u4e2d\u6587\u535a\u5ba2\u4e0e\u8d44\u6e90</p> <p>\u7cbe\u9009\u7684\u4e2d\u6587AI\u667a\u80fd\u4f53\u76f8\u5173\u535a\u5ba2\u6587\u7ae0\u3001\u6559\u7a0b\u548c\u8d44\u6e90\uff0c\u5e2e\u52a9\u4e2d\u6587\u7528\u6237\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u7528\u667a\u80fd\u4f53\u6280\u672f\u3002</p> <ul> <li> <p>17\u4e2a\u4e3b\u6d41 Agent \u6846\u67b6\u5feb\u901f\u5bf9\u6bd4</p> <ul> <li> \u535a\u5ba2\u94fe\u63a5 \u00b7 \u77e5\u4e4e\u4e13\u680f\u6587\u7ae0\uff0c\u5bf9\u6bd4\u5206\u6790\u4e3b\u6d41\u667a\u80fd\u4f53\u6846\u67b6</li> </ul> </li> <li> <p>\u901a\u4e49 DeepResearch</p> <ul> <li> Blog Post \u00b7 \u963f\u91cc\u5df4\u5df4\u901a\u4e49\u667a\u80fd\u4f53\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u4ecb\u7ecd</li> </ul> </li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"applications/","title":"\ud83d\udcf1 Applications","text":"<p>MiroFlow enables building a wide variety of intelligent applications across different domains and use cases.</p>"},{"location":"applications/#available-applications","title":"\ud83c\udfaf Available Applications","text":"<p>Ready-to-Use Applications</p> <p>Experience MiroFlow's capabilities through these available interfaces and demos.</p>"},{"location":"applications/#gradio-demo","title":"Gradio Demo","text":"<p>Local Development Interface</p> <p>Interactive web interface for testing MiroFlow agents locally. Currently available at MiroThinker Gradio Demo.</p>"},{"location":"applications/#live-demo","title":"Live Demo","text":"<p>Online Experience</p> <p>Experience MiroFlow's capabilities through our online demo for deep research tasks.</p>"},{"location":"applications/#development-status","title":"\ud83d\udd04 Development Status","text":"<p>Integration Progress</p> <p>The MiroThinker model workflows are being integrated into the main MiroFlow framework. This will provide a unified experience for all applications and demos.</p> <p>Stay tuned for updates on application availability and new integrations!</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"browsecomp_en/","title":"BrowseComp-EN (English)","text":"<p>MiroFlow's evaluation on the BrowseComp-EN benchmark demonstrates advanced web browsing and information retrieval capabilities.</p> <p>More details: BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents</p>"},{"location":"browsecomp_en/#dataset-overview","title":"Dataset Overview","text":"<p>Key Dataset Characteristics</p> <ul> <li>Total Tasks: 1,266 tasks in the test split</li> <li>Language: English</li> <li>Task Types: Web browsing, search, and information retrieval</li> <li>Evaluation: Automated comparison with ground truth answers</li> </ul>"},{"location":"browsecomp_en/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"browsecomp_en/#step-1-prepare-the-browsecomp-en-dataset","title":"Step 1: Prepare the BrowseComp-EN Dataset","text":"Download BrowseComp-EN Dataset<pre><code>uv run main.py prepare-benchmark get browsecomp-test\n</code></pre> <p>This will create the standardized dataset at <code>data/browsecomp-test/standardized_data.jsonl</code>.</p> <p>Requires HuggingFace Token</p> <p>Add your HuggingFace token to <code>.env</code>: <code>HF_TOKEN=\"your_token_here\"</code></p>"},{"location":"browsecomp_en/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":".env Configuration<pre><code># Search and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# Code execution\nE2B_API_KEY=\"xxx\"\n\n# LLM (Claude 3.7 Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Evaluation and hint generation\nOPENAI_API_KEY=\"xxx\"\n\n# Vision capabilities\nANTHROPIC_API_KEY=\"xxx\"\nGEMINI_API_KEY=\"xxx\"\n</code></pre>"},{"location":"browsecomp_en/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"Run BrowseComp-EN Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_browsecomp-en_claude37sonnet benchmark=browsecomp-en output_dir=\"logs/browsecomp-en/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Results are automatically generated in the output directory: - <code>benchmark_results.jsonl</code> - Detailed results for each task - <code>benchmark_results_pass_at_1_accuracy.txt</code> - Summary accuracy statistics</p>"},{"location":"browsecomp_en/#usage-examples","title":"Usage Examples","text":"Limited Task Testing<pre><code># Test with 10 tasks only\nuv run main.py common-benchmark --config_file_name=agent_browsecomp-en_claude37sonnet benchmark=browsecomp-en benchmark.execution.max_tasks=10 output_dir=\"logs/browsecomp-en/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> Using MiroThinker Model<pre><code>uv run main.py common-benchmark --config_file_name=agent_browsecomp-en_mirothinker benchmark=browsecomp-en output_dir=\"logs/browsecomp-en/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"browsecomp_en/#available-agent-configurations","title":"Available Agent Configurations","text":"Agent Configuration Model Use Case <code>agent_browsecomp-en_claude37sonnet</code> Claude 3.7 Sonnet Recommended for better performance <code>agent_browsecomp-en_mirothinker</code> MiroThinker For local deployment <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"claude-3.7-sonnet/","title":"Claude 3.7 Sonnet","text":"<p>Anthropic's Claude 3.7 Sonnet model with 200K context, strong reasoning, and tool use capabilities.</p>"},{"location":"claude-3.7-sonnet/#available-clients","title":"Available Clients","text":""},{"location":"claude-3.7-sonnet/#claudeanthropicclient-direct-api","title":"ClaudeAnthropicClient (Direct API)","text":"<p>Environment Setup:</p> Environment Variables<pre><code>export ANTHROPIC_API_KEY=\"your-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.anthropic.com\"  # optional\n</code></pre> <p>Configuration:</p> Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeAnthropicClient\"\n    model_name: \"claude-3-7-sonnet-20250219\"  # Use actual model name from Anthropic API\n    async_client: true\n    temperature: 0.3\n    top_p: 0.95\n    min_p: 0.0\n    top_k: -1\n    max_tokens: 32000\n    anthropic_api_key: \"${oc.env:ANTHROPIC_API_KEY,???}\"\n    anthropic_base_url: \"${oc.env:ANTHROPIC_BASE_URL,https://api.anthropic.com}\"\n    disable_cache_control: false\n    keep_tool_result: -1\n    oai_tool_thinking: false\n</code></pre> <p>Sampling Parameters</p> <ul> <li><code>min_p</code> and <code>top_k</code> are required in the configuration</li> <li>Anthropic API natively supports <code>top_k</code>, but <code>min_p</code> is not used by the API</li> <li>Set <code>min_p: 0.0</code> (disabled) and <code>top_k: -1</code> (disabled) or a specific value like <code>top_k: 40</code></li> </ul>"},{"location":"claude-3.7-sonnet/#usage","title":"Usage","text":"Example Command<pre><code># Run with Claude 3.7 Sonnet (Anthropic SDK) on example dataset\nuv run main.py common-benchmark --config_file_name=agent_llm_claude37sonnet_anthropic output_dir=\"logs/test\"\n</code></pre> <p>The <code>agent_llm_claude37sonnet_anthropic.yaml</code> configuration file provides a ready-to-use setup with the example dataset benchmark.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_benchmarks/","title":"Contributing New Benchmarks to MiroFlow","text":"<p>This comprehensive guide walks you through adding new evaluation benchmarks to the MiroFlow framework. MiroFlow's modular architecture makes it easy to integrate diverse evaluation datasets while maintaining consistency and reproducibility.</p>"},{"location":"contribute_benchmarks/#overview","title":"Overview","text":"<p>Why Add New Benchmarks?</p> <p>Adding new benchmarks serves multiple purposes:</p> <ul> <li>Internal Testing: Validate your agent's performance on custom tasks and domains specific to your use case</li> <li>Development Iteration: Create targeted test sets to debug and improve specific agent capabilities</li> <li>Domain-Specific Evaluation: Test agents on proprietary or specialized datasets relevant to your application</li> <li>Research Contributions: Expand MiroFlow's benchmark coverage to advance the field with new evaluation paradigms</li> <li>Comparative Analysis: Benchmark your agent against custom baselines or competitors</li> </ul>"},{"location":"contribute_benchmarks/#step-by-step-implementation-guide","title":"Step-by-Step Implementation Guide","text":""},{"location":"contribute_benchmarks/#step-1-prepare-your-dataset","title":"Step 1: Prepare Your Dataset","text":"<p>Your benchmark dataset must follow MiroFlow's standardized structure for seamless integration.</p>"},{"location":"contribute_benchmarks/#required-directory-structure","title":"Required Directory Structure","text":"<pre><code>your-benchmark/\n\u251c\u2500\u2500 standardized_data.jsonl    # Metadata file (required)\n\u251c\u2500\u2500 file1.pdf                  # Optional: Binary files referenced by tasks\n\u251c\u2500\u2500 file2.png                  # Optional: Images, documents, etc.\n\u251c\u2500\u2500 data.csv                   # Optional: Additional data files\n\u2514\u2500\u2500 ...                        # Any other supporting files\n</code></pre>"},{"location":"contribute_benchmarks/#metadata-format-specification","title":"Metadata Format Specification","text":"<p>Each line in <code>standardized_data.jsonl</code> must be a valid JSON object with the following schema:</p> <p>Required Fields</p> <pre><code>{\n  \"task_id\": \"unique_task_identifier\",\n  \"task_question\": \"The question or instruction for the task\",\n  \"ground_truth\": \"The expected answer or solution\",\n  \"file_path\": \"path/to/file.pdf\",  // Optional, can be null\n  \"metadata\": {                     // Optional, can be empty object or other structure\n    \"difficulty\": \"hard\",\n    \"category\": \"reasoning\",\n    \"source\": \"original_dataset_name\"\n  }\n}\n</code></pre>"},{"location":"contribute_benchmarks/#example-tasks","title":"Example Tasks","text":"<p>Simple Text-Only Task: <pre><code>{\n  \"task_id\": \"math_001\",\n  \"task_question\": \"What is the integral of x^2 from 0 to 2?\",\n  \"ground_truth\": \"8/3\",\n  \"file_path\": null,\n  \"metadata\": {\n    \"difficulty\": \"medium\",\n    \"category\": \"calculus\",\n    \"source\": \"custom_math_problems\"\n  }\n}\n</code></pre></p> <p>File-Based Task: <pre><code>{\n  \"task_id\": \"doc_analysis_001\",\n  \"task_question\": \"Based on the provided financial report, what was the company's revenue growth rate?\",\n  \"ground_truth\": \"12.5%\",\n  \"file_path\": \"reports/financial_q3_2023.pdf\",\n  \"metadata\": {\n    \"difficulty\": \"hard\",\n    \"category\": \"document_analysis\",\n    \"file_type\": \"pdf\"\n  }\n}\n</code></pre></p>"},{"location":"contribute_benchmarks/#step-2-create-benchmark-configuration","title":"Step 2: Create Benchmark Configuration","text":"<p>Create a configuration file to define how MiroFlow should handle your benchmark.</p>"},{"location":"contribute_benchmarks/#configuration-file-location","title":"Configuration File Location","text":"<p>Create: <code>config/benchmark/your-benchmark.yaml</code></p>"},{"location":"contribute_benchmarks/#configuration-template","title":"Configuration Template","text":"config/benchmark/your-benchmark.yaml<pre><code># Benchmark configuration for your custom dataset\ndefaults:\n  - default          # Use default benchmark settings\n  - _self_           # Allow overrides in this file\n\nname: \"your-benchmark\"\n\ndata:\n  data_dir: \"${data_dir}/your-benchmark\"        # Dataset location\n  metadata_file: \"standardized_data.jsonl\"     # Metadata filename\n\nexecution:\n  max_tasks: null          # null = no limit, number = max tasks to run\n  max_concurrent: 5        # Number of parallel task executions\n  pass_at_k: 1             # Number of attempts per task for pass@k evaluation\n\n# LLM judge configuration for evaluation\nopenai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n</code></pre>"},{"location":"contribute_benchmarks/#configuration-options","title":"Configuration Options","text":"<p>Execution Parameters</p> <ul> <li>max_tasks: Control dataset size during development (use small numbers for testing)</li> <li>max_concurrent: Balance speed vs. resource usage</li> <li>pass_at_k: Enable multiple attempts for better success measurement</li> </ul>"},{"location":"contribute_benchmarks/#step-3-set-up-data-directory","title":"Step 3: Set Up Data Directory","text":"<p>Organize your dataset files in the MiroFlow data structure.</p> Data Directory Setup<pre><code># Create the benchmark data directory\nmkdir -p data/your-benchmark\n\n# Copy your dataset files\ncp your-dataset/* data/your-benchmark/\n\n# Verify the structure\nls -la data/your-benchmark/\n</code></pre> <p>File Path Consistency</p> <p>Ensure that all <code>file_path</code> entries in your JSONL metadata correctly reference files in your data directory.</p>"},{"location":"contribute_benchmarks/#step-4-test-your-benchmark","title":"Step 4: Test Your Benchmark","text":"<p>Validate your benchmark integration with comprehensive testing.</p>"},{"location":"contribute_benchmarks/#initial-testing","title":"Initial Testing","text":"<p>Start with a small subset to verify everything works correctly:</p> Test Benchmark Integration<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_quickstart_reading \\\n  benchmark=your-benchmark \\\n  benchmark.execution.max_tasks=3 \\\n  output_dir=\"logs/test-your-benchmark/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"contribute_benchmarks/#full-evaluation","title":"Full Evaluation","text":"<p>Once testing passes, run the complete benchmark:</p> Run Full Benchmark<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_quickstart_reading \\\n  benchmark=your-benchmark \\\n  output_dir=\"logs/your-benchmark/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"contribute_benchmarks/#step-5-validate-results","title":"Step 5: Validate Results","text":"<p>Review the evaluation outputs to ensure proper integration:</p>"},{"location":"contribute_benchmarks/#check-output-files","title":"Check Output Files","text":"Verify Results<pre><code># List generated files\nls -la logs/your-benchmark/\n\n# Review a sample task log\ncat logs/your-benchmark/task_*_attempt_1.json | head -50\n</code></pre>"},{"location":"contribute_benchmarks/#expected-output-structure","title":"Expected Output Structure","text":"<p>Your benchmark should generate:</p> <ul> <li>Individual task execution logs</li> <li>Aggregate benchmark results (<code>benchmark_results.jsonl</code>)</li> <li>Accuracy summary files</li> <li>Hydra configuration logs</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_llm_clients/","title":"Contributing New LLM Clients","text":"<p>Add support for new LLM providers to MiroFlow by creating a provider class that integrates with the existing client infrastructure.</p>"},{"location":"contribute_llm_clients/#client-structure","title":"Client Structure","text":"<p>Each LLM client inherits from <code>LLMProviderClientBase</code> and implements 4 required methods:</p> <ul> <li><code>_create_client()</code> - Initialize API client</li> <li><code>_create_message()</code> - Make API calls  </li> <li><code>process_llm_response()</code> - Handle responses</li> <li><code>extract_tool_calls_info()</code> - Parse tool calls</li> </ul>"},{"location":"contribute_llm_clients/#implementation-steps","title":"Implementation Steps","text":""},{"location":"contribute_llm_clients/#step-1-create-provider-file","title":"Step 1: Create Provider File","text":"<p>Create <code>src/llm/providers/your_provider_client.py</code>:</p> Provider Implementation<pre><code>import dataclasses\nfrom src.llm.provider_client_base import LLMProviderClientBase\n\n@dataclasses.dataclass\nclass YourProviderClient(LLMProviderClientBase):\n    def _create_client(self, config):\n        # Initialize your API client\n        pass\n\n    async def _create_message(self, system_prompt, messages, tools_definitions, keep_tool_result=-1):\n        # Make API call\n        pass\n\n    def process_llm_response(self, llm_response, message_history, agent_type=\"main\"):\n        # Extract response text, return (text, should_exit)\n        pass\n\n    def extract_tool_calls_info(self, llm_response, assistant_response_text):\n        # Parse tool calls, return (tool_calls, tool_names)\n        pass\n</code></pre>"},{"location":"contribute_llm_clients/#step-2-create-configuration","title":"Step 2: Create Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"YourProviderClient\"\n    model_name: \"your-model\"\n    your_api_key: \"${oc.env:YOUR_API_KEY,???}\"\n    your_base_url: \"${oc.env:YOUR_BASE_URL,https://api.yourprovider.com/v1}\"\n</code></pre>"},{"location":"contribute_llm_clients/#step-3-set-environment-variables","title":"Step 3: Set Environment Variables","text":"Environment Setup<pre><code>export YOUR_API_KEY=\"your-key\"\nexport YOUR_BASE_URL=\"https://api.yourprovider.com/v1\"  # optional if using default\n</code></pre>"},{"location":"contribute_llm_clients/#examples","title":"Examples","text":"<p>See existing providers in <code>src/llm/providers/</code>:</p> <ul> <li><code>ClaudeAnthropicClient</code> - Direct API</li> <li><code>ClaudeOpenRouterClient</code> - Proxy API  </li> <li><code>GPTOpenAIClient</code> - OpenAI API</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contribute_tools/","title":"Contributing New Tools","text":"<p>MiroFlow's extensible tool system allows you to add custom functionality by implementing new MCP (Model Context Protocol) servers. Each tool extends the agent's capabilities and can be easily integrated into the framework.</p>"},{"location":"contribute_tools/#overview","title":"Overview","text":"<p>What This Does</p> <p>Extend the agent's functionality by introducing a new tool. Each tool is implemented as an MCP server and registered via configuration, enabling agents to access new capabilities seamlessly.</p>"},{"location":"contribute_tools/#implementation-steps","title":"Implementation Steps","text":""},{"location":"contribute_tools/#step-1-create-mcp-server","title":"Step 1: Create MCP Server","text":"<p>Create a new file <code>src/tool/mcp_servers/new-mcp-server.py</code> that implements the tool's core logic.</p> src/tool/mcp_servers/new-mcp-server.py<pre><code>from fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"new-mcp-server\")\n\n@mcp.tool()\nasync def tool_name(param: str) -&gt; str:\n    \"\"\"\n    Explanation of the tool, its parameters, and return value.\n    \"\"\"\n    tool_result = ...  # Your logic here\n    return tool_result\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre> <p>Automatic Schema Generation</p> <p>Tool schemas are automatically generated from <code>docstrings</code> and <code>type hints</code> via the FastMCP protocol.</p>"},{"location":"contribute_tools/#step-2-create-tool-configuration","title":"Step 2: Create Tool Configuration","text":"<p>Add a new configuration file at <code>config/tool/new-tool-name.yaml</code>:</p> config/tool/new-tool-name.yaml<pre><code>name: \"new-tool-name\"\ntool_command: \"python\"\nargs:\n  - \"-m\"\n  - \"src.tool.mcp_servers.new-mcp-server\"  # Match the server file created above\n</code></pre>"},{"location":"contribute_tools/#step-3-register-tool-in-agent-configuration","title":"Step 3: Register Tool in Agent Configuration","text":"<p>Enable the new tool inside your agent configuration (e.g., <code>config/agent-with-new-tool.yaml</code>):</p> config/agent-with-new-tool.yaml<pre><code>main_agent:\n  # ... other configuration ...\n  tool_config:\n    - tool-reasoning\n    - new-tool-name   # \ud83d\udc48 Add your new tool here\n  # ... other configuration ...\n\nsub_agents:\n  agent-worker:\n    # ... other configuration ...\n    tool_config:\n      - tool-searching\n      - tool-image-video\n      - tool-reading\n      - tool-code\n      - tool-audio\n      - new-tool-name # \ud83d\udc48 Add your new tool here\n    # ... other configuration ...\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"contributors/","title":"\ud83d\udcdd Contributors","text":"<p>Thank you to all the amazing contributors who have helped make MiroFlow better! \ud83d\ude4f</p>"},{"location":"contributors/#core-team","title":"Core Team","text":"<p>Development Team</p> <p>The MiroFlow framework is developed and maintained by the MiroMind AI team.</p>"},{"location":"contributors/#community-contributors","title":"Community Contributors","text":"<p>Community Appreciation</p> <p>We welcome contributions from the community! Whether you're fixing bugs, adding features, improving documentation, or helping with benchmarks, your contributions are valued.</p> <p></p>"},{"location":"contributors/#how-to-contribute","title":"How to Contribute","text":"<p>Contribution Opportunities</p> <p>There are many ways to contribute to MiroFlow:</p>"},{"location":"contributors/#bug-reports-feature-requests","title":"\ud83d\udc1b Bug Reports &amp; Feature Requests","text":"<p>Issue Reporting</p> <ul> <li>Report bugs or request features via GitHub Issues</li> <li>Use clear, descriptive titles and provide detailed information</li> </ul>"},{"location":"contributors/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<p>Development Workflow</p> <ul> <li>Fork the repository and create a feature branch</li> <li>Follow our coding standards and include tests</li> <li>Submit a pull request with a clear description of your changes</li> </ul>"},{"location":"contributors/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Documentation Help</p> <ul> <li>Help improve our documentation</li> <li>Add examples and tutorials</li> <li>Fix typos and clarify explanations</li> </ul>"},{"location":"contributors/#testing-benchmarks","title":"\ud83e\uddea Testing &amp; Benchmarks","text":"<p>Quality Assurance</p> <ul> <li>Help us test MiroFlow on different platforms</li> <li>Contribute new benchmark datasets</li> <li>Improve existing evaluation scripts</li> </ul>"},{"location":"contributors/#community-support","title":"\ud83d\udcac Community Support","text":"<p>Community Engagement</p> <ul> <li>Answer questions in our Discord community</li> <li>Help other users in GitHub discussions</li> <li>Share your experiences and use cases</li> </ul>"},{"location":"contributors/#recognition","title":"Recognition","text":"<p>Contributor Acknowledgment</p> <p>All contributors are recognized in our:</p> <ul> <li>GitHub contributors graph</li> <li>Release notes for significant contributions</li> <li>Community acknowledgments</li> </ul>"},{"location":"contributors/#getting-started","title":"Getting Started","text":"<p>Quick Start Guide</p> <ol> <li>Check out our GitHub repository</li> <li>Read the contributing guidelines</li> <li>Join our Discord community to connect with other contributors</li> </ol> <p>Thank You</p> <p>Thank you for helping us build the future of AI agents! \ud83d\ude80</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"core_concepts/","title":"Core Concepts","text":"<p>MiroFlow is a flexible framework for building and deploying intelligent agents capable of complex reasoning and tool use.</p>"},{"location":"core_concepts/#architecture-overview","title":"Architecture Overview","text":"<p>Multi-Stage Agentic Process</p> <p>MiroFlow processes user queries through a structured workflow:</p> <ol> <li>Intent Recognition &amp; Query Augmentation - LLMs analyze and refine user input</li> <li>Planning &amp; Task Orchestration - Main agent creates execution plans and coordinates sub-agents</li> <li>Delegation to Sub-Agents - Specialized agents handle domain-specific tasks</li> <li>Tool Access via MCP Servers - Agents leverage external capabilities through MCP protocol</li> <li>Result Synthesis &amp; Output Alignment - Final results are synthesized and formatted</li> </ol>"},{"location":"core_concepts/#core-components","title":"Core Components","text":""},{"location":"core_concepts/#agent-system","title":"Agent System","text":"<p>Agent Architecture</p> <p>Main Agent: Primary coordinator that receives tasks, creates plans, and manages overall execution. Can use reasoning tools and delegate to sub-agents.</p> <p>Sub-Agents: Specialized agents for specific domains:</p> <ul> <li><code>agent-worker</code>: General-purpose agent with comprehensive tool access (search, files, code, media)</li> <li>Each sub-agent has dedicated configurations and can operate independently</li> </ul>"},{"location":"core_concepts/#tool-integration","title":"Tool Integration","text":"<p>Tool System</p> <p>Tool Manager: Connects to MCP servers and manages tool availability</p> <p>Available Tools:</p> <ul> <li>Code Execution: Python sandbox via E2B integration</li> <li>Web Search: Google search with content retrieval</li> <li>Document Processing: Multi-format file reading and analysis</li> <li>Visual Processing: Image and video analysis</li> <li>Audio Processing: Transcription and audio analysis</li> <li>Enhanced Reasoning: Advanced reasoning via high-quality LLMs</li> </ul> <p>See Tool Overview for detailed tool configurations and capabilities.</p>"},{"location":"core_concepts/#llm-support","title":"LLM Support","text":"<p>Multi-Provider Support</p> <p>Unified interface supporting:</p> <ul> <li>Anthropic Claude (via Anthropic API, OpenRouter)</li> <li>OpenAI GPT (via OpenAI API)</li> <li>Qwen (via SGLang)</li> <li>MiroThinker (via SGLang)</li> <li>see LLM Clients Overview for details*</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"data/","title":"\ud83d\udcca Data","text":"<p>The MiroVerse dataset collection provides comprehensive training data for building advanced AI agents with full trajectory coverage.</p>"},{"location":"data/#news-updates","title":"\ud83d\udd25 News &amp; Updates","text":"<p>Latest Releases</p> <ul> <li>The data is released over Huggingface.</li> <li>MiroVerse v0.1 has been released. This dataset can be used with our training framework, MiroTrain. In MiroVerse v0.1, we provide both SFT and DPO data, making it easy to reproduce MiroThinker-v0.1's benchmark performance on Qwen3. Give it a try!</li> </ul>"},{"location":"data/#first-batch-of-miroverse","title":"\ud83d\udd25 First Batch of MiroVerse","text":"<p>What makes this release special</p> <p>\u2728 Special Features:</p> <ul> <li>\ud83d\udcda Diverse Verified Open Source Data \u2014 Carefully curated and validated community datasets</li> <li>\ud83e\udde0 Fresh Large-Scale Deep Research Data \u2014 Generated by our proprietary data engine</li> <li>\ud83d\udd04 Complete Trajectory Coverage \u2014 Every single sample includes full rollout trajectories</li> <li>\u2705 Quality Assurance: \u2014 Each trajectory has been verified, ensuring high-quality training data for your models.</li> <li>\ud83c\udf31 Always Growing, Always Open \u2014 Regular updates, powered by collaboration with the community</li> </ul>"},{"location":"data/#dataset-overview","title":"\ud83d\udce6 Dataset Overview","text":"<p>MiroVerse-v0.1 Statistics</p> <p>MiroVerse-v0.1 is a large-scale agent dataset with 147K+ samples featuring full rollout trajectories across diverse AI agent tasks including multi-hop QA, web navigation, and scientific reasoning. Every single sample includes complete execution traces with 1.9B+ tokens and 602K+ tool interactions, providing comprehensive training data for tool-using and web-browsing AI agents.</p> <p></p> Split #Sample #Main Trace #Browse Trace #Token #Turns #Tools License MiroVerse-Voyager1.0 59097 19115 39982 1129113893 444723 325537 CC-BY-NC-4.0 MiroVerse-MuSiQue 29572 10422 19150 294351053 143080 90486 CC-BY-4.0 MiroVerse-HotpotQA 12942 6553 6389 67352039 46320 20524 CC-BY-SA-4.0 MiroVerse-WebWalkerQA-Silver 10817 4961 5856 107650324 67846 46215 Apache 2.0 MiroVerse-MegaScience 10615 8270 2345 111120264 63594 42443 CC-BY-NC-SA-4.0 MiroVerse-TaskCraft 8890 4277 4613 95518109 35013 17236 MIT MiroVerse-QA-Expert-Multi-Hop-V1.0 6187 2091 4096 63983151 31957 19585 Apache 2.0 MiroVerse-OneGen-TrainDataset-MultiHopQA 3289 1347 1942 33214386 17187 11449 MIT MiroVerse-2WikiMultihopQA 3001 1410 1591 28977451 13982 7981 Apache 2.0 MiroVerse-WikiTables 1606 1288 318 16461870 12089 8877 MIT MiroVerse-WebShaper 1514 486 1028 31240265 12126 9578 MIT MiroVerse-WebDancer 455 192 263 7817689 3170 2268 MIT MiroVerse-v0.1 147985 60412 87573 1993099086 891087 602179 / <p>Dataset Details</p> <p>Every sample includes successful MiroFlow rollout trajactories that reached the verified answer\u2014one JSON line, zero secrets.</p> <p>Licensing Information</p> <p>MiroVerse-v0.1 dataset follows a hybrid licensing model: query and answer data retain their original source licenses, while all trace data is licensed under CC-BY-NC-4.0; for commercial use, please contact us to request a commercial license.</p>"},{"location":"data/#why-were-different","title":"\ud83c\udd9a Why We're Different","text":"<p>Our Philosophy</p> <p>While high-quality data is essential for training advanced models and often kept private, we believe that the path to truly general-purpose agents is still long. That's why we're committed to open-sourcing as much of our data as possible\u2014including raw samples and exploration traces\u2014to support and accelerate progress across the community.</p> Org Work Samples Trace Data Reproducible? OpenAI Deep Research \u2014 \u274c \u274c Gemini Gemini Deep Research \u2014 \u274c \u274c Tencent Cognitive Kernel-Pro 7 k \u274c \u274c Tongyi WebShaper 500 \u274c \u274c MiroMind (ours) this repo 147 k+ \u2705 \u2705"},{"location":"data/#benchmark-performance","title":"\ud83d\udcc8 Benchmark Performance","text":"<p>Training Results</p> <p>MiroVerse-v0.1 is used in the training of our MiroThinker-v0.1 models.</p> <p>By using this dataset, we achieved the following benchmark performance.</p>"},{"location":"data/#gaia-benchmark","title":"GAIA Benchmark","text":"Method Text-103Best Pass@1 Text-103Pass@1 (Avg@8) Val-165Best Pass@1 Val-165Pass@1 (Avg@8) Search-o1-7B 17.5 - - - R1-Searcher-7B 20.4 - - - WebDancer-7B 31.0 - - - WebSailor-7B 37.9 - - - CK-Pro-8B 43.7 - 35.2 - MiroThinker-8B-SFT-v0.1 44.7 40.1 34.6 31.8 + Commercial Tools 46.6 42.1 37.6 33.9 MiroThinker-8B-DPO-v0.1 46.6 44.8 37.0 35.4 + Commercial Tools 50.5 46.7 38.2 35.9 MiroThinker-14B-SFT-v0.1 47.6 44.4 37.0 34.4 + Commercial Tools 49.5 47.5 41.8 39.8 MiroThinker-14B-DPO-v0.1 48.5 46.6 42.4 39.2 + Commercial Tools 52.4 48.5 45.5 42.0 Qwen3-32B 31.1 26.7 29.7 26.4 Search-o1-32B 28.2 - - - WebThinker-32B-RL 48.5 - - - WebDancer-QwQ-32B 51.5 - - - WebSailor-32B 53.2 - - - WebShaper-QwQ-32B 53.3 - - - MiroThinker-32B-SFT-v0.1 55.3 51.3 44.9 42.7 + Commercial Tools 58.3 54.2 48.5 45.8 MiroThinker-32B-DPO-v0.1 57.3 54.1 48.5 45.9 + Commercial Tools 60.2 57.9 50.9 48.9 <ol> <li> <p>Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.</p> </li> <li> <p>For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.</p> </li> <li> <p>By default, we use open-source tools wherever possible, except for the code tool E2B and the Google search tool Serper. We use Whisper, Qwen2.5-VL-72B-Instruct, and Qwen3-235B-A22B-Thinking-2507 in our implementation. The framework can be easily extended to other open-source tools of your choice.</p> </li> <li> <p>Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our models.</p> </li> </ol>"},{"location":"data/#more-benchmarks","title":"More Benchmarks","text":"Method HLEPass@1 FramesPass@1 BrowseCompPass@1 BrowseComp-ZHPass@1 WebWalkerQAPass@1 OpenAI Deep Research 26.6 - 51.5 42.9 - Gemini Deep Research 26.9 - - - - Kimi-Researcher 26.9 78.8 - - - WebDancer-7B - - - - 36.0 WebSailor-7B - - 6.7 14.2 - MiroThinker-8B-SFT-v0.1 - 58.0 5.5 9.3 41.3 MiroThinker-8B-DPO-v0.1 - 64.4 8.7 13.5 45.7 WebThinker-32B-RL - - - - 46.5 WebDancer-QwQ-32B - - 3.8 18.0 47.9 WebSailor-32B - - 10.5 25.5 - WebShaper-32B - - - - 51.4 MiroThinker-32B-SFT-v0.1 10.2 70.4 10.6 13.8 45.7 MiroThinker-32B-DPO-v0.1 11.8 71.7 13.0 17.0 49.3 <ol> <li> <p>MiroThinker\u2019s performance was tested with this repository and open-source tools; other models\u2019 results are from their papers and official sites.</p> </li> <li> <p>As MiroVerse-v0.1 mainly contains English data, the model's Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.</p> </li> </ol>"},{"location":"data/#examples","title":"\ud83e\udde9 Examples","text":"<p>Sample QA Examples</p> <p>Below are two QA examples synthesized by our data engine (MiroVerse-Voyager1.0).</p> <p>Case 1</p> <p>Q: A female lead actress received her first major annual Hindi film performance award for best actress for her role in a late-2000s comedy-drama, directed by the filmmaker who later created a sports-themed drama released in 2023 starring an actress known for completing an athletic triathlon event in Berlin. What is the title of the film for which this actress first won that award?</p> <p>A: Paa</p> <p>Case 2</p> <p>Q: Identify the agricultural practice, unique to a mountain range that forms a border including an independent principality and known for spectacular geologic landforms, that was one of the key reasons for part of the range's inscription as a UNESCO World Heritage Site in the decade before the 21st century. This region's history features a brief early-1800s reorganization of provincial boundaries after a liberal revolution in the southern country, and the northern country is globally recognized as the leading tourist destination with the fourth-largest number of heritage sites. What is this traditional agricultural system called?</p> <p>A: transhumance</p>"},{"location":"data/#free-trace-rollout-let-us-help-you-train","title":"\ud83d\udee0\ufe0f Free Trace Rollout: Let Us Help You Train","text":"<p>Community Support</p> <p>Generating high-quality training trajectories is expensive \u2014 on average, $1.50 per sample using top-tier commercial models.</p> <p>To empower the community, we're offering free rollout services for qualifying seed data:</p>"},{"location":"data/#how-it-works","title":"How It Works","text":"<p>Process Steps</p> <p>1. Submit a Request</p> <p>Open a ticket via this template and provide the basic info, rollout requirements, and up to 100 sample rows in one go.</p> <p>2. Review &amp; Rollout</p> <p>We'll review your submission within 48 hours. Once approved, we'll reach out to you for the full dataset and then launch the complete trace rollout using top-tier commercial models.</p> <p>3. Delivery &amp; Recognition</p> <p>Upon completion, we'll send the augmented dataset to you via email.</p> <p>With your explicit consent, we'll also publish it publicly and credit you as a Community Contributor \u2014 with a permanent badge in this README.</p>"},{"location":"data/#license","title":"\ud83e\udd1d License","text":"<p>License Terms</p> <p>This project is released under the CC BY-NC 4.0. Parts of this project contain code and models from other sources, which are subject to their respective licenses. For commercial use cases, please contact us at: service@miromind.ai.</p>"},{"location":"data/#citation","title":"\ud83d\udcdc Citation","text":"<p>Academic Citation</p> <p>If you find this project useful in your research, please consider cite:</p> BibTeX Citation<pre><code>@misc{miromind2024opendata,\n  title={MiroVerse V0.1: A Reproducible, Full-Trajectory, Ever-Growing Deep Research Dataset},\n  author={MiroMind Data Team},\n  year={2025},\n  url={https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1}\n}\n</code></pre>"},{"location":"data/#contact-us","title":"Contact Us","text":"<p>Get in Touch</p> <p>MiroVerse is developed by the MiroMind Data Team. If you would like to leave us a message, feel free to get in touch.  In addition to GitHub,  Discord,  WeChat,  and RedNote,  you can also reach us via email at service@miromind.ai.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"download_datasets/","title":"Dataset Download Instructions","text":"<p>This guide walks you through downloading and preparing benchmark datasets for MiroFlow evaluation.</p>"},{"location":"download_datasets/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>Before downloading datasets, ensure you have completed both access requests and environment setup below.</p>"},{"location":"download_datasets/#1-request-dataset-access","title":"1. Request Dataset Access","text":"<p>You must request access to the following Hugging Face datasets:</p> <p>Required Datasets</p> <ul> <li>GAIA Dataset: https://huggingface.co/datasets/gaia-benchmark/GAIA</li> <li>HLE Dataset: https://huggingface.co/datasets/cais/hle</li> </ul> <p>Visit the links above and request access to both datasets.</p>"},{"location":"download_datasets/#2-configure-environment-variables","title":"2. Configure Environment Variables","text":"<p>Copy the template file and create your environment configuration:</p> <pre><code>cp .env.template .env\n</code></pre> <p>Edit the <code>.env</code> file and configure these essential variables:</p> .env<pre><code># Required: Your Hugging Face token for dataset access\nHF_TOKEN=\"your-actual-huggingface-token-here\"\n\n# Data directory path \nDATA_DIR=\"data/\"\n</code></pre> <p>Getting Your Hugging Face Token</p> <ol> <li>Go to https://huggingface.co/settings/tokens</li> <li>Create a new token with at least \"Read\" permissions</li> <li>Replace <code>your-actual-huggingface-token-here</code> in the <code>.env</code> file with your actual token</li> </ol>"},{"location":"download_datasets/#download-and-prepare-datasets","title":"Download and Prepare Datasets","text":"<p>Once you have been granted access to the required datasets, run the preparation script to download all benchmark datasets.</p>"},{"location":"download_datasets/#running-the-download-script","title":"Running the Download Script","text":"<p>Execute the following command to start the download process for all datasets, if a single dataset is needed, you could run the specific command:</p> <pre><code>bash scripts/run_prepare_benchmark.sh\n</code></pre> <p>Script Contents</p> <p>The script contains the following logic and dataset downloads. You can comment out any unwanted datasets by adding <code>#</code> at the start of the line.</p> scripts/run_prepare_benchmark.sh<pre><code>#!/bin/bash\necho \"Please grant access to these datasets:\"\necho \"- https://huggingface.co/datasets/gaia-benchmark/GAIA\"\necho \"- https://huggingface.co/datasets/cais/hle\"\necho\n\nread -p \"Have you granted access? [Y/n]: \" answer\nanswer=${answer:-Y}\nif [[ ! $answer =~ ^[Yy] ]]; then\n    echo \"Please grant access to the datasets first\"\n    exit 1\nfi\necho \"Access confirmed\"\n\n# Comment out any unwanted datasets by adding # at the start of the line\nuv run main.py prepare-benchmark get gaia-val\nuv run main.py prepare-benchmark get gaia-val-text-only\nuv run main.py prepare-benchmark get frames-test\nuv run main.py prepare-benchmark get webwalkerqa\nuv run main.py prepare-benchmark get browsecomp-test\nuv run main.py prepare-benchmark get browsecomp-zh-test\nuv run main.py prepare-benchmark get hle\nuv run main.py prepare-benchmark get xbench-ds\nuv run main.py prepare-benchmark get futurex\n</code></pre>"},{"location":"download_datasets/#what-this-script-does","title":"What This Script Does","text":"<p>Script Actions</p> <ol> <li>Confirms dataset access - Verifies you have requested access to required datasets</li> <li>Downloads benchmark datasets - Retrieves the following datasets:<ul> <li><code>gaia-val</code> - GAIA validation set</li> <li><code>gaia-val-text-only</code> - Text-only GAIA validation</li> <li><code>frames-test</code> - Frames test dataset</li> <li><code>webwalkerqa</code> - Web Walker QA dataset</li> <li><code>browsecomp-test</code> - English BrowseComp test set</li> <li><code>browsecomp-zh-test</code> - Chinese BrowseComp test set</li> <li><code>hle</code> - HLE dataset</li> <li><code>xbench-ds</code> - xbench-DeepSearch dataset</li> <li><code>futurex</code> - Futurex-Online dataset</li> </ul> </li> </ol>"},{"location":"download_datasets/#customizing-dataset-selection","title":"Customizing Dataset Selection","text":"<p>To download only specific datasets, edit the script and comment out unwanted lines:</p> <pre><code># Comment out unwanted datasets like this:\n# uv run main.py prepare-benchmark get gaia-val\nuv run main.py prepare-benchmark get gaia-val-text-only\n# uv run main.py prepare-benchmark get frames-test\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"e2b_advanced_features/","title":"E2B Advanced Features","text":"<p>Preview Documentation</p> <p>This section is in preview and not fully ready. The features and instructions may change in future releases.</p> <p>MiroFlow provides advanced E2B (Execute to Build) sandbox capabilities for enhanced code execution environments with pre-installed packages and custom configurations.</p>"},{"location":"e2b_advanced_features/#local-e2b-sandbox-deployment","title":"Local E2B Sandbox Deployment","text":"<p>Recommended Setup</p> <p>To achieve our best benchmark results, we recommend using a pre-defined sandbox template that includes the most commonly used Python and apt packages.</p> <p>If you prefer not to use a sandbox template, you can disable it by commenting out the line <code>template=DEFAULT_TEMPLATE_ID,</code> in <code>libs/miroflow-tool/src/miroflow/tool/mcp_servers/python_server.py</code> (line 145).</p>"},{"location":"e2b_advanced_features/#sandbox-setup-guide","title":"Sandbox Setup Guide","text":"<p>Prerequisites</p> <ul> <li>npm installed locally</li> <li>Docker running locally</li> <li>E2B API key configured</li> </ul>"},{"location":"e2b_advanced_features/#step-1-install-e2b-cli","title":"Step 1: Install E2B CLI","text":"Install E2B Command Line<pre><code># Install e2b\nnpm install -g @e2b/cli\n\n# Verify installation\nwhich e2b \n</code></pre>"},{"location":"e2b_advanced_features/#step-2-download-pre-configured-dockerfile","title":"Step 2: Download Pre-configured Dockerfile","text":"<p>Download our pre-configured Dockerfile from the repository:</p> Download Dockerfile<pre><code>wget https://github.com/MiroMindAI/MiroFlow/blob/main/docs/e2b.Dockerfile\n</code></pre>"},{"location":"e2b_advanced_features/#step-3-build-template","title":"Step 3: Build Template","text":"<p>Run the <code>e2b template build</code> command to create your custom template:</p> Build E2B Template<pre><code># Set your E2B access token\nE2B_ACCESS_TOKEN=${your-token}\n\n# Build the template with docker build locally\ne2b template build -c \"/root/.jupyter/start-up.sh\" -n \"all_pip_apt_pkg\" -d ./e2b.Dockerfile\n\n# Verify template was built successfully\nE2B_ACCESS_TOKEN=${your-token} e2b template list\n</code></pre> <p>Custom Templates</p> <p>You can create your own custom sandbox template for specific use cases by following similar steps. For more information, refer to the E2B Docker documentation.</p>"},{"location":"e2b_advanced_features/#e2b-docker-configuration","title":"E2B Docker Configuration","text":"<p>This custom E2B Docker environment provides a sandboxed environment with pre-installed scientific computing libraries, data analysis tools, and dependencies commonly needed for AI agent tasks.</p> e2b.Dockerfile<pre><code># You can use most Debian-based base images\nFROM e2bdev/code-interpreter\n\n# Update package list and install Python 3.10 and pip\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    portaudio19-dev \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel\n\n# Install dependencies and customize sandbox\nRUN python3 -m pip install --no-cache-dir \\\n    Flask \\\n    IPython \\\n    Pillow \\\n    PyGithub \\\n    PyMuPDF \\\n    PyPDF2 \\\n    arch \\\n    arm-pyart \\\n    arxiv \\\n    ase \\\n    astropy \\\n    astroquery \\\n    awscli \\\n    beautifulsoup4 \\\n    biopython \\\n    boto3 \\\n    brian2 \\\n    cairosvg \\\n    cgt \\\n    chardet \\\n    chess \\\n    cinemagoer \\\n    clifford \\\n    contextily \\\n    control \\\n    cryptography \\\n    cvxpy \\\n    datasets \\\n    descarteslabs \\\n    duckduckgo-search \\\n    edalize \\\n    english_words \\\n    ephem \\\n    esp-docs \\\n    flask \\\n    folium \\\n    geopandas \\\n    geopy \\\n    google-search-results \\\n    googlesearch-python \\\n    googletrans \\\n    habanero \\\n    helics \\\n    hijri_converter \\\n    imbalanced-learn \\\n    inflect \\\n    isbnlib \\\n    kaggle \\\n    lifelines \\\n    lxml \\\n    lxml_html_clean \\\n    mapclassify \\\n    markdown \\\n    'matplotlib&gt;=3.8' \\\n    mendeleev \\\n    metpy \\\n    music21 \\\n    networkx \\\n    nipype \\\n    numba \\\n    'numpy&gt;=2' \\\n    opencv-python \\\n    openpyxl \\\n    'pandas&gt;=2' \\\n    pandas_datareader \\\n    parsl \\\n    pdf2image \\\n    pdfminer \\\n    pdfplumber \\\n    periodictable \\\n    plotly \\\n    polars \\\n    psycopg2-binary \\\n    pulp \\\n    pyXSteam \\\n    pybel \\\n    pycryptodome \\\n    pydot \\\n    pygplates \\\n    pymatgen \\\n    pymupdf \\\n    pypdf2 \\\n    pypinyin \\\n    pyscf \\\n    pytesseract \\\n    python-docx \\\n    pytube \\\n    pywavelets \\\n    rdflib \\\n    reportlab \\\n    requests \\\n    requests-html \\\n    scanpy \\\n    scikit-image \\\n    scikit-learn \\\n    scipy \\\n    scvelo \\\n    seaborn \\\n    selenium \\\n    semanticscholar \\\n    shap \\\n    shapely \\\n    siphon \\\n    skyfield \\\n    smbus2 \\\n    snappy \\\n    spglib \\\n    sphinx \\\n    splink \\\n    statsmodels \\\n    stockfish \\\n    sympy \\\n    tabulate \\\n    torch \\\n    torchvision \\\n    transformers \\\n    uncertainpy \\\n    us \\\n    virtualenv \\\n    wbdata \\\n    webdriver-manager \\\n    wikipedia-api \\\n    wolframalpha \\\n    wordfreq \\\n    yfinance \\\n    yt-dlp \\\n    docx2txt \\\n    rdkit \\\n    stockfish \\\n    yfinance \\\n    seaborn \\\n    python-pptx \\\n    pyaudio \\\n    pyshp \\\n    SpeechRecognition \\\n    waybackpy\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n    # \u2500\u2500 Basic build &amp; Python \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    build-essential gfortran cmake pkg-config git curl wget ca-certificates \\\n    # \u2500\u2500 scientific computing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libopenblas-dev liblapack-dev libatlas-base-dev \\\n    libssl-dev libffi-dev zlib1g-dev \\\n    # \u2500\u2500 image / OpenCV / Pillow \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    libgl1 libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender1 \\\n    libjpeg-dev libpng-dev libwebp-dev libfreetype6-dev libopenjp2-7 liblcms2-dev \\\n    # \u2500\u2500 video / audio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ffmpeg libsndfile1 sox portaudio19-dev \\\n    # \u2500\u2500 PDF / doc / OCR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    poppler-utils pdfgrep ghostscript \\\n    tesseract-ocr tesseract-ocr-deu \\\n    libxml2-dev libxslt1-dev \\\n    # \u2500\u2500 other tools \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    imagemagick unlambda stockfish \\\n    unzip zip tar nano &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"evaluation_overview/","title":"\ud83d\udcca Performance Benchmarks","text":"<p>MiroFlow achieves state-of-the-art performance across multiple agentic benchmarks, demonstrating its effectiveness in complex reasoning and tool-use tasks.</p>"},{"location":"evaluation_overview/#performance-on-future-prediction","title":"Performance on Future Prediction","text":"<p>Future X Benchmark Results</p> <p>MiroFlow demonstrates exceptional performance in future prediction tasks.</p> <p></p>"},{"location":"evaluation_overview/#performance-on-benchmarks","title":"\u2728 Performance on Benchmarks","text":"<p>Comprehensive Benchmark Analysis</p> <p>We benchmark MiroFlow on a series of benchmarks including GAIA, HLE, BrowseComp and xBench-DeepSearch.</p> <p></p>"},{"location":"evaluation_overview/#other-benchmark-results","title":"Other Benchmark Results","text":"<p>Detailed Performance Comparison</p> <p>Comprehensive comparison across multiple benchmark categories and competing frameworks.</p>"},{"location":"evaluation_overview/#reasoning-language-understanding","title":"Reasoning &amp; Language Understanding","text":"Model/Framework GAIA Val HLE HLE-Text MiroFlow 82.4% 27.2% 29.5% OpenAI Deep Research 67.4% 26.6% - Gemini Deep Research - 26.9% - Kimi Researcher - - 26.9% WebSailor-72B 55.4% - - Manus 73.3% - - DeepSeek v3.1 - - 29.8%"},{"location":"evaluation_overview/#web-browsing-search-tasks","title":"Web Browsing &amp; Search Tasks","text":"Model/Framework BrowserComp-EN BrowserComp-ZH xBench-DeepSearch MiroFlow 33.2% 47.1% 72.0% OpenAI Deep Research 51.5% 42.9% - Gemini Deep Research - - 50+% Kimi Researcher - - 69.0% WebSailor-72B - 30.1% 55.0% DeepSeek v3.1 - - 71.2% <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"faq_and_known_issues/","title":"Faq and known issues","text":""},{"location":"faq_and_known_issues/#faq","title":"FAQ","text":"<p>Q: What is the estimated cost of running the GAIA validation set for a single run? A: The cost is approximately $250 USD for a run with cache.</p> <p>Q: How long does it take to run the GAIA validation set for a single run? A: With the <code>max_concurrent</code> parameter set to 20, a full run takes about 2 hours to complete.</p> <p>Q: Are all the specified APIs required? A: Yes. To fully reproduce our published results, access to all the listed APIs in corresponding benchmark is necessary.</p> <p>Q: What is the difference between MiroFlow and MiroThinker? A:  MiroFlow is primarily focused on interacting with proprietary models; MiroThinker is designed for our own open-source models.</p> <p>We plan to merge these two projects in the future to create a single, unified platform.</p>"},{"location":"faq_and_known_issues/#known-issues-roadmap","title":"Known Issues &amp; Roadmap","text":""},{"location":"faq_and_known_issues/#currently-in-development","title":"\ud83d\udd04 Currently in Development","text":"<ul> <li>FutureX Benchmark: Adding support for FutureX benchmark evaluation</li> <li>Token Usage &amp; Cost Tracking: Implementing detailed usage analytics and cost calculation features</li> </ul> <p>Last Updated: Sep 2025 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"faqs/","title":"\ud83d\udc1b FAQ and Known Issues","text":"<p>Common questions and development roadmap for MiroFlow framework.</p>"},{"location":"faqs/#faq","title":"FAQ","text":"<p>Frequently Asked Questions</p> <p>Common questions about MiroFlow usage, costs, and platform differences.</p> <p>Q: What is the estimated cost of running the GAIA validation set for a single run?</p> <p>A: The cost is approximately $250 USD for a run with cache.</p> <p>Q: How long does it take to run the GAIA validation set for a single run?</p> <p>A: With the <code>max_concurrent</code> parameter set to 20, a full run takes about 2 hours to complete.</p> <p>Q: Are all the specified APIs required?</p> <p>A: Yes. To fully reproduce our published results, access to all the listed APIs in corresponding benchmark is necessary.</p> <p>Q: What is the difference between MiroFlow and MiroThinker?</p> <p>A: MiroFlow is primarily focused on interacting with proprietary models; MiroThinker is designed for our own open-source models.</p> <p>We plan to merge these two projects in the future to create a single, unified platform.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"finsearchcomp/","title":"FinSearchComp","text":"<p>MiroFlow's evaluation on the FinSearchComp benchmark demonstrates capabilities in financial information search and analysis tasks, showcasing advanced reasoning abilities in complex financial research scenarios.</p> <p>More details: FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning</p>"},{"location":"finsearchcomp/#dataset-overview","title":"Dataset Overview","text":"<p>FinSearchComp Dataset</p> <p>The FinSearchComp dataset consists of financial search and analysis tasks that require comprehensive research capabilities including:</p> <ul> <li>Financial data retrieval and analysis</li> <li>Market research and company analysis</li> <li>Investment decision support</li> <li>Financial news and report interpretation</li> <li>Time-sensitive financial information gathering</li> </ul> <p>Key Dataset Characteristics</p> <ul> <li>Total Tasks: 635 (across T1, T2, T3 categories)</li> <li>Task Types: <ul> <li>T1: Time-Sensitive Data Fetching</li> <li>T2: Financial Analysis and Research</li> <li>T3: Complex Historical Investigation</li> </ul> </li> <li>Answer Format: Detailed financial analysis and research reports</li> <li>Ground Truth: Available for T2 and T3 tasks, changes dynamically for T1 tasks</li> <li>Evaluation: Judge-based evaluation with correctness assessment</li> </ul>"},{"location":"finsearchcomp/#quick-start-guide","title":"Quick Start Guide","text":"<p>Quick Start Instructions</p> <p>This section provides step-by-step instructions to run the FinSearchComp benchmark and prepare submission results. Note: This is a quick start guide for running the benchmark, not for reproducing exact submitted results.</p>"},{"location":"finsearchcomp/#step-1-prepare-the-finsearchcomp-dataset","title":"Step 1: Prepare the FinSearchComp Dataset","text":"<p>Dataset Setup</p> <p>Use the integrated prepare-benchmark command to download and process the dataset:</p> Download FinSearchComp Dataset<pre><code>uv run main.py prepare-benchmark get finsearchcomp\n</code></pre> <p>This will create the standardized dataset at <code>data/finsearchcomp/standardized_data.jsonl</code>.</p>"},{"location":"finsearchcomp/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>API Key Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># For searching and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use Claude 3.7 Sonnet for financial analysis via OpenRouter\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Used for hint generation and final answer extraction\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n</code></pre>"},{"location":"finsearchcomp/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Evaluation Execution</p> <p>Execute the following command to run evaluation on the FinSearchComp dataset:</p> Run FinSearchComp Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp_claude37sonnet benchmark=finsearchcomp output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Progress Monitoring and Resume</p> <p>To check the progress while running:</p> Check Progress<pre><code>uv run utils/progress_check/check_finsearchcomp_progress.py $PATH_TO_LOG\n</code></pre> <p>If you need to resume an interrupted evaluation, specify the same output directory to continue from where you left off.</p> Resume Evaluation, e.g.<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp_claude37sonnet benchmark=finsearchcomp output_dir=${PATH_TO_LOG}\n</code></pre>"},{"location":"finsearchcomp/#step-4-extract-results","title":"Step 4: Extract Results","text":"<p>Result Extraction</p> <p>After evaluation completion, the results are automatically generated in the output directory:</p> <ul> <li><code>benchmark_results.jsonl</code>: Detailed results for each task</li> <li><code>benchmark_results_pass_at_1_accuracy.txt</code>: Summary accuracy statistics</li> <li><code>task_*_attempt_1.json</code>: Individual task execution traces</li> </ul>"},{"location":"finsearchcomp/#evaluation-notes","title":"Evaluation Notes","text":"<p>Task Type Considerations</p> <p>The FinSearchComp dataset includes different task types with varying evaluation criteria:</p> <ul> <li>T1 Tasks: Time-Sensitive Data Fetching tasks are excluded from correctness evaluation due to outdated ground truth, but completion is still tracked</li> <li>T2 Tasks: Financial Analysis tasks are evaluated for correctness and quality</li> <li>T3 Tasks: Complex Historical Investigation tasks require comprehensive research and analysis</li> </ul> <p>Output Analysis</p> <p>The evaluation generates detailed execution traces showing:</p> <ul> <li>Research process for each financial task</li> <li>Information gathering from multiple sources</li> <li>Financial calculations and analysis</li> <li>Comprehensive reports with insights and recommendations</li> </ul>"},{"location":"finsearchcomp/#directory-structure","title":"Directory Structure","text":"<p>After running evaluations, you'll find the following structure:</p> <pre><code>logs/finsearchcomp/agent_finsearchcomp_claude37sonnet_YYYYMMDD_HHMM/\n\u251c\u2500\u2500 benchmark_results.jsonl              # Task results summary\n\u251c\u2500\u2500 benchmark_results_pass_at_1_accuracy.txt  # Accuracy statistics\n\u251c\u2500\u2500 task_(T1)Time_Sensitive_Data_Fetching_*.json  # T1 task traces\n\u251c\u2500\u2500 task_(T2)Financial_Analysis_*.json   # T2 task traces\n\u251c\u2500\u2500 task_(T3)Complex_Historical_Investigation_*.json  # T3 task traces\n\u2514\u2500\u2500 output.log                           # Execution log\n</code></pre>"},{"location":"finsearchcomp/#task-categories-breakdown","title":"Task Categories Breakdown","text":"<p>The progress checker provides detailed statistics:</p> <ul> <li>Total Tasks: Complete count across all categories</li> <li>Completed Tasks: Successfully finished tasks</li> <li>Correct Tasks: Tasks with judge_result \"CORRECT\" (T2 and T3 only)</li> <li>Category Breakdown: Separate counts for T1, T2, and T3 tasks</li> <li>Accuracy Metrics: Pass@1 accuracy for evaluable tasks</li> </ul>"},{"location":"finsearchcomp/#usage-examples","title":"Usage Examples","text":""},{"location":"finsearchcomp/#single-run-evaluation","title":"Single Run Evaluation","text":"Basic Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp_claude37sonnet benchmark=finsearchcomp output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"finsearchcomp/#limited-task-testing","title":"Limited Task Testing","text":"Test with Limited Tasks<pre><code>uv run main.py common-benchmark --config_file_name=agent_finsearchcomp_claude37sonnet benchmark=finsearchcomp benchmark.execution.max_tasks=5 output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"finsearchcomp/#custom-agent-configuration","title":"Custom Agent Configuration","text":"Different Agent Setup<pre><code>uv run main.py common-benchmark --config_file_name=agent_gaia-validation benchmark=finsearchcomp output_dir=\"logs/finsearchcomp/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"finsearchcomp/#multiple-runs-for-reliability","title":"Multiple Runs for Reliability","text":"Multiple Runs<pre><code>NUM_RUNS=5 ./scripts/run_evaluate_multiple_runs_finsearchcomp.sh\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"futurex/","title":"Futurex-Online","text":"<p>MiroFlow's evaluation on the Futurex-Online benchmark demonstrates capabilities in future event prediction tasks.</p> <p>More details: FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction</p>"},{"location":"futurex/#dataset-overview","title":"Dataset Overview","text":"<p>Futurex-Online Dataset</p> <p>The Futurex-Online dataset consists of 61 prediction tasks covering various future events including:</p> <ul> <li>Political events (referendums, elections)</li> <li>Sports outcomes (football matches)</li> <li>Legal proceedings</li> <li>Economic indicators</li> </ul> <p>Key Dataset Characteristics</p> <ul> <li>Total Tasks: 61</li> <li>Task Type: Future event prediction</li> <li>Answer Format: Boxed answers (\\boxed{Yes/No} or \\boxed{A/B/C})</li> <li>Ground Truth: Not available (prediction tasks)</li> <li>Resolution Date: Around 2025-09-21 (GMT+8)</li> </ul>"},{"location":"futurex/#quick-start-guide","title":"Quick Start Guide","text":"<p>Quick Start Instructions</p> <p>This section provides step-by-step instructions to run the Futurex-Online benchmark and prepare submission results. Since this is a prediction dataset without ground truth, we focus on execution traces and response generation. Note: This is a quick start guide for running the benchmark, not for reproducing exact submitted results.</p>"},{"location":"futurex/#step-1-prepare-the-futurex-online-dataset","title":"Step 1: Prepare the Futurex-Online Dataset","text":"<p>Dataset Setup</p> <p>Use the integrated prepare-benchmark command to download and process the dataset:</p> Download Futurex-Online Dataset<pre><code>uv run main.py prepare-benchmark get futurex\n</code></pre> <p>This will create the standardized dataset at <code>data/futurex/standardized_data.jsonl</code>.</p>"},{"location":"futurex/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>API Key Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># For searching and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# We use Claude-3.7-Sonnet with OpenRouter backend to initialize the LLM\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Used for Claude vision understanding\nANTHROPIC_API_KEY=\"xxx\"\n\n# Used for Gemini vision\nGEMINI_API_KEY=\"xxx\"\n\n# Use for llm judge, reasoning, hint generation, etc.\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"futurex/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Evaluation Execution</p> <p>Execute the following command to run evaluation on the Futurex-Online dataset. This uses the basic <code>agent_quickstart_reading</code> configuration for quick start purposes.</p> Run Futurex-Online Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_quickstart_reading benchmark=futurex output_dir=\"logs/futurex/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Progress Monitoring and Resume</p> <p>To check the progress while running:</p> Check Progress<pre><code>uv run utils/progress_check/check_futurex_progress.py $PATH_TO_LOG\n</code></pre> <p>If you need to resume an interrupted evaluation, specify the same output directory to continue from where you left off.</p> Resume Evaluation, e.g.<pre><code>uv run main.py common-benchmark --config_file_name=agent_quickstart_reading benchmark=futurex output_dir=\"logs/futurex/20250918_1010\"\n</code></pre>"},{"location":"futurex/#step-4-extract-results","title":"Step 4: Extract Results","text":"<p>Result Extraction</p> <p>After evaluation completion, extract the results using the provided utility:</p> Extract Results<pre><code>uv run utils/extract_futurex_results.py logs/futurex/$(date +\"%Y%m%d_%H%M\")\n</code></pre> <p>This will generate:</p> <ul> <li><code>futurex_results.json</code>: Detailed results for each task</li> <li><code>futurex_summary.json</code>: Summary statistics</li> <li><code>futurex_predictions.csv</code>: Predictions in CSV format</li> </ul>"},{"location":"futurex/#sample-task-examples","title":"Sample Task Examples","text":""},{"location":"futurex/#political-prediction","title":"Political Prediction","text":"<pre><code>Task: \"Will the 2025 Guinea referendum pass? (resolved around 2025-09-21 (GMT+8))\"\nExpected Format: \\boxed{Yes} or \\boxed{No}\n</code></pre>"},{"location":"futurex/#sports-prediction","title":"Sports Prediction","text":"<pre><code>Task: \"Brighton vs. Tottenham (resolved around 2025-09-21 (GMT+8))\nA. Brighton win on 2025-09-20\nB. Brighton vs. Tottenham end in a draw  \nC. Tottenham win on 2025-09-20\"\nExpected Format: \\boxed{A}, \\boxed{B}, or \\boxed{C}\n</code></pre>"},{"location":"futurex/#multiple-runs-and-voting","title":"Multiple Runs and Voting","text":"<p>Improving Prediction Accuracy</p> <p>For better prediction accuracy, you can run multiple evaluations and use voting mechanisms to aggregate results. This approach helps reduce randomness and improve the reliability of predictions. Note: This is a quick start approach; production submissions may use more sophisticated configurations.</p>"},{"location":"futurex/#step-1-run-multiple-evaluations","title":"Step 1: Run Multiple Evaluations","text":"<p>Use the multiple runs script to execute several independent evaluations:</p> Run Multiple Evaluations<pre><code>./scripts/run_evaluate_multiple_runs_futurex.sh\n</code></pre> <p>This script will:</p> <ul> <li>Run 3 independent evaluations by default (configurable with <code>NUM_RUNS</code>)</li> <li>Execute all tasks in parallel for efficiency</li> <li>Generate separate result files for each run in <code>run_1/</code>, <code>run_2/</code>, etc.</li> <li>Create a consolidated <code>futurex_submission.jsonl</code> file with voting results</li> </ul>"},{"location":"futurex/#step-2-customize-multiple-runs","title":"Step 2: Customize Multiple Runs","text":"<p>You can customize the evaluation parameters:</p> Custom Multiple Runs<pre><code># Run 5 evaluations with limited tasks for testing\nNUM_RUNS=5 MAX_TASKS=10 ./scripts/run_evaluate_multiple_runs_futurex.sh\n\n# Use different agent configuration\nAGENT_SET=agent_gaia-validation ./scripts/run_evaluate_multiple_runs_futurex.sh\n\n# Adjust concurrency for resource management\nMAX_CONCURRENT=3 ./scripts/run_evaluate_multiple_runs_futurex.sh\n</code></pre>"},{"location":"futurex/#step-3-voting-and-aggregation","title":"Step 3: Voting and Aggregation","text":"<p>After multiple runs, the system automatically:</p> <ol> <li>Extracts predictions from all runs using <code>utils/extract_futurex_results.py</code></li> <li>Applies majority voting to aggregate predictions across runs</li> <li>Generates submission file in the format required by FutureX platform</li> <li>Provides voting statistics showing prediction distribution across runs</li> </ol> <p>The voting process works as follows:</p> <ul> <li>Majority Vote: Most common prediction across all runs wins</li> <li>Tie-breaking: If tied, chooses the prediction that appeared earliest across all runs</li> <li>Vote Counts: Tracks how many runs predicted each option</li> <li>Confidence Indicators: High agreement indicates more reliable predictions</li> </ul>"},{"location":"futurex/#step-4-analyze-voting-results","title":"Step 4: Analyze Voting Results","text":"<p>Check the generated files for voting analysis:</p> Check Voting Results<pre><code># View submission file with voting results\ncat logs/futurex/agent_quickstart_reading_*/futurex_submission.jsonl\n\n# Check individual run results\nls logs/futurex/agent_quickstart_reading_*/run_*/\n\n# Check progress and voting statistics\nuv run python utils/progress_check/check_futurex_progress.py logs/futurex/agent_quickstart_reading_*\n</code></pre>"},{"location":"futurex/#manual-voting-aggregation","title":"Manual Voting Aggregation","text":"<p>You can also manually run the voting aggregation:</p> Manual Voting Aggregation<pre><code># Aggregate multiple runs with majority voting\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_reading_* --aggregate\n\n# Force single run mode (if needed)\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_reading_*/run_1 --single\n\n# Specify custom output file\nuv run python utils/extract_futurex_results.py logs/futurex/agent_quickstart_reading_* -o my_voted_predictions.jsonl\n</code></pre>"},{"location":"futurex/#voting-output-format","title":"Voting Output Format","text":"<p>The voting aggregation generates a submission file with the following format:</p> <pre><code>{\"id\": \"687104310a994c0060ef87a9\", \"prediction\": \"No\", \"vote_counts\": {\"No\": 2}}\n{\"id\": \"68a9b46e961bd3003c8f006b\", \"prediction\": \"Yes\", \"vote_counts\": {\"Yes\": 2}}\n</code></pre> <p>The output includes:</p> <ul> <li><code>id</code>: Task identifier</li> <li><code>prediction</code>: Final voted prediction (without <code>\\boxed{}</code> wrapper)</li> <li><code>vote_counts</code>: Dictionary showing how many runs predicted each option</li> </ul> <p>For example, <code>\"vote_counts\": {\"No\": 2}</code> means 2 out of 2 runs predicted \"No\", indicating high confidence.</p>"},{"location":"futurex/#evaluation-notes","title":"Evaluation Notes","text":"<p>No Ground Truth Available</p> <p>Since Futurex-Online is a prediction dataset, there are no ground truth answers available for evaluation. The focus is on:</p> <ul> <li>Response generation quality</li> <li>Reasoning process documentation</li> <li>Prediction confidence and methodology</li> </ul> <p>Output Analysis</p> <p>The evaluation generates detailed execution traces showing:</p> <ul> <li>Research process for each prediction</li> <li>Information gathering from web sources</li> <li>Reasoning chains leading to predictions</li> <li>Final boxed answers in required format</li> </ul>"},{"location":"futurex/#directory-structure","title":"Directory Structure","text":"<p>After running multiple evaluations, you'll find the following structure:</p> <pre><code>logs/futurex/agent_quickstart_reading_YYYYMMDD_HHMM/\n\u251c\u2500\u2500 futurex_submission.jsonl          # Final voted predictions\n\u251c\u2500\u2500 run_1/                            # First run results\n\u2502   \u251c\u2500\u2500 benchmark_results.jsonl       # Individual task results\n\u2502   \u251c\u2500\u2500 benchmark_results_pass_at_1_accuracy.txt\n\u2502   \u2514\u2500\u2500 task_*_attempt_1.json        # Detailed execution traces\n\u251c\u2500\u2500 run_2/                            # Second run results\n\u2502   \u2514\u2500\u2500 ... (same structure as run_1)\n\u251c\u2500\u2500 run_1_output.log                  # Run 1 execution log\n\u2514\u2500\u2500 run_2_output.log                  # Run 2 execution log\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_test/","title":"GAIA Test","text":"<p>The GAIA (General AI Assistant) test set provides a comprehensive evaluation dataset for assessing AI agents' capabilities in complex, real-world reasoning tasks. This benchmark tests agents' ability to perform multi-step problem solving, information synthesis, and tool usage across diverse scenarios.</p> <p>More details: GAIA: a benchmark for General AI Assistants</p>"},{"location":"gaia_test/#setup-and-evaluation-guide","title":"Setup and Evaluation Guide","text":""},{"location":"gaia_test/#step-1-download-the-gaia-test-dataset","title":"Step 1: Download the GAIA Test Dataset","text":"<p>Direct Download (Recommended)</p> <pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-test.zip\nunzip gaia-test.zip\n# Unzip passcode: pf4*\n</code></pre>"},{"location":"gaia_test/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Required API Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and hint generation\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"gaia_test/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the following command:</p> Run GAIA Test Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-test \\\n  output_dir=\"logs/gaia-test/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_test/#step-4-monitor-progress-and-resume","title":"Step 4: Monitor Progress and Resume","text":"<p>Progress Tracking</p> <p>You can monitor the evaluation progress in real-time:</p> Check Progress<pre><code>uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p> <p>Resume Capability</p> <p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-test \\\n  output_dir=\"logs/gaia-test/20250922_1430\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_validation_claude37sonnet/","title":"GAIA Validation - Claude 3.7 Sonnet","text":"<p>MiroFlow demonstrates state-of-the-art performance on the GAIA validation benchmark using Claude 3.7 Sonnet models, showcasing exceptional capabilities in complex reasoning tasks that require multi-step problem solving, information synthesis, and tool usage.</p> <p>Prerequisites</p> <p>Before proceeding, please review the GAIA Validation Prerequisites document, which covers common setup requirements, dataset preparation, and API key configuration.</p>"},{"location":"gaia_validation_claude37sonnet/#performance-comparison","title":"Performance Comparison","text":"<p>State-of-the-Art Performance with Claude 3.7 Sonnet</p> <p>MiroFlow achieves state-of-the-art (SOTA) performance among open-source agent frameworks on the GAIA validation set using Claude 3.7 Sonnet.</p> <p></p> <p>Key Performance Metrics</p> <ul> <li>Pass@3: 81.8%</li> <li>Majority Vote: 82.4%</li> <li>Pass@1 (best@3): 74.5%</li> <li>Pass@1 (avg@3): 72.2%</li> </ul> <p>Reproducibility Guarantee</p> <p>Unlike other frameworks with unclear evaluation methods, MiroFlow's results are fully reproducible. Note that Hugging Face access was disabled during inference to prevent direct answer retrieval.</p>"},{"location":"gaia_validation_claude37sonnet/#running-the-evaluation","title":"Running the Evaluation","text":""},{"location":"gaia_validation_claude37sonnet/#step-1-dataset-preparation","title":"Step 1: Dataset Preparation","text":"<p>Follow the dataset preparation instructions in the prerequisites document.</p>"},{"location":"gaia_validation_claude37sonnet/#step-2-api-keys-configuration","title":"Step 2: API Keys Configuration","text":"<p>Configure the following API keys in your <code>.env</code> file:</p> Claude 3.7 Sonnet .env Configuration<pre><code># Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and hint generation\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"gaia_validation_claude37sonnet/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the Claude 3.7 Sonnet configuration:</p> Run GAIA Validation with Claude 3.7 Sonnet<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation_claude37sonnet \\\n  output_dir=\"logs/gaia-validation-claude37sonnet/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_validation_claude37sonnet/#step-4-monitor-progress","title":"Step 4: Monitor Progress","text":"<p>Follow the progress monitoring instructions in the prerequisites document.</p>"},{"location":"gaia_validation_claude37sonnet/#execution-traces","title":"Execution Traces","text":"<p>Complete Execution Traces</p> <p>We have released our complete execution traces for the <code>gaia-validation</code> dataset using Claude 3.7 Sonnet on Hugging Face. This comprehensive collection includes a full run of 165 tasks with an overall accuracy of 73.94% and detailed reasoning traces.</p> <p>You can download them using the following command:</p> Download Execution Traces<pre><code>wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia_validation_miroflow_trace_public_20250825.zip\nunzip gaia_validation_miroflow_trace_public_20250825.zip\n# Unzip passcode: pf4*\n</code></pre> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_validation_gpt5/","title":"GAIA Validation - GPT5","text":"<p>MiroFlow now supports GPT-5 with MCP tool invocation, providing a unified workflow for multi-step reasoning, information integration, and scalable tool coordination.</p> <p>Prerequisites</p> <p>Before proceeding, please review the GAIA Validation Prerequisites document, which covers common setup requirements, dataset preparation, and API key configuration.</p>"},{"location":"gaia_validation_gpt5/#running-the-evaluation","title":"Running the Evaluation","text":""},{"location":"gaia_validation_gpt5/#step-1-dataset-preparation","title":"Step 1: Dataset Preparation","text":"<p>Follow the dataset preparation instructions in the prerequisites document.</p>"},{"location":"gaia_validation_gpt5/#step-2-api-keys-configuration","title":"Step 2: API Keys Configuration","text":"<p>Configure the following API keys in your <code>.env</code> file:</p> GPT-5 .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# Primary LLM provider, LLM judge, reasoning, and hint generation\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"gaia_validation_gpt5/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the GPT-5 configuration:</p> Run GAIA Validation with GPT-5<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation-gpt5 \\\n  output_dir=\"logs/gaia-validation-gpt5/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_validation_gpt5/#step-4-monitor-progress","title":"Step 4: Monitor Progress","text":"<p>Follow the progress monitoring instructions in the prerequisites document.</p> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_validation_mirothinker/","title":"GAIA Validation - MiroThinker","text":"<p>MiroFlow demonstrates state-of-the-art performance on the GAIA validation benchmark using MiroThinker models, showcasing exceptional capabilities in complex reasoning tasks that require multi-step problem solving, information synthesis, and tool usage.</p> <p>Prerequisites</p> <p>Before proceeding, please review the GAIA Validation Prerequisites document, which covers common setup requirements, dataset preparation, and API key configuration.</p>"},{"location":"gaia_validation_mirothinker/#running-the-evaluation","title":"Running the Evaluation","text":""},{"location":"gaia_validation_mirothinker/#step-1-dataset-preparation","title":"Step 1: Dataset Preparation","text":"<p>Follow the dataset preparation instructions in the prerequisites document.</p>"},{"location":"gaia_validation_mirothinker/#step-2-api-keys-configuration","title":"Step 2: API Keys Configuration","text":"<p>Configure the following API keys in your <code>.env</code> file:</p> MiroThinker .env Configuration<pre><code># MiroThinker model access\nOAI_MIROTHINKER_API_KEY=\"your-mirothinker-api-key\"\nOAI_MIROTHINKER_BASE_URL=\"http://localhost:61005/v1\"\n\n# Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and hint generation\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n\n# Hint Generation and final answer with MiroThinker model\nHINT_LLM_BASE_URL=\"http://localhost:61005/v1\"\nFINAL_ANSWER_LLM_BASE_URL=\"http://localhost:61005/v1\"\n</code></pre>"},{"location":"gaia_validation_mirothinker/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the MiroThinker configuration:</p> Run GAIA Validation with MiroThinker<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation_mirothinker \\\n  output_dir=\"logs/gaia-validation-mirothinker/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_validation_mirothinker/#step-4-monitor-progress","title":"Step 4: Monitor Progress","text":"<p>Follow the progress monitoring instructions in the prerequisites document.</p>"},{"location":"gaia_validation_mirothinker/#multiple-runs","title":"Multiple Runs","text":"<p>Due to performance variance in MiroThinker models, it's recommended to run multiple evaluations for more reliable results.</p> Run Multiple MiroThinker Evaluations<pre><code>bash ./scripts/run_evaluate_multiple_runs_mirothinker_gaia-validation.sh\n</code></pre> <p>This script runs 3 evaluations in parallel and calculates average scores. You can modify <code>NUM_RUNS</code> in the script to change the number of runs.</p> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_validation_prerequisites/","title":"GAIA Validation Prerequisites","text":"<p>This document covers the common setup requirements and prerequisites for running GAIA validation benchmarks with MiroFlow, regardless of the specific model configuration used.</p>"},{"location":"gaia_validation_prerequisites/#about-the-gaia-dataset","title":"About the GAIA Dataset","text":"<p>What is GAIA?</p> <p>GAIA (General AI Assistant) is a comprehensive benchmark designed to evaluate AI agents' ability to perform complex reasoning tasks that require multiple skills including web browsing, file manipulation, data analysis, and multi-step problem solving.</p> <p>More details: GAIA: a benchmark for General AI Assistants</p>"},{"location":"gaia_validation_prerequisites/#dataset-preparation","title":"Dataset Preparation","text":""},{"location":"gaia_validation_prerequisites/#step-1-prepare-the-gaia-validation-dataset","title":"Step 1: Prepare the GAIA Validation Dataset","text":"<p>Choose one of the following methods to obtain the GAIA validation dataset:</p> <p>Method 1: Direct Download (Recommended)</p> <p>No Authentication Required</p> <p>This method does not require HuggingFace tokens or access permissions.</p> Manual Dataset Download<pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-val.zip\nunzip gaia-val.zip\n# Unzip passcode: pf4*\n</code></pre> <p>Method 2: Using the prepare-benchmark command</p> <p>Prerequisites Required</p> <p>This method requires HuggingFace dataset access and token configuration.</p> <p>First, you need to request access and configure your environment:</p> <ol> <li>Request Dataset Access: Visit https://huggingface.co/datasets/gaia-benchmark/GAIA and request access</li> <li>Configure Environment:     <pre><code>cp .env.template .env\n</code></pre>    Edit the <code>.env</code> file:    <pre><code>HF_TOKEN=\"your-actual-huggingface-token-here\"\nDATA_DIR=\"data/\"\n</code></pre></li> </ol> <p>Getting Your Hugging Face Token</p> <ol> <li>Go to https://huggingface.co/settings/tokens</li> <li>Create a new token with at least \"Read\" permissions</li> <li>Add your token to the <code>.env</code> file</li> </ol> <p>Then download the dataset:</p> Download via Script<pre><code>uv run main.py prepare-benchmark get gaia-val\n</code></pre>"},{"location":"gaia_validation_prerequisites/#progress-monitoring-and-resume","title":"Progress Monitoring and Resume","text":""},{"location":"gaia_validation_prerequisites/#progress-tracking","title":"Progress Tracking","text":"<p>You can monitor the evaluation progress in real-time:</p> Check Progress<pre><code>uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p>"},{"location":"gaia_validation_prerequisites/#resume-capability","title":"Resume Capability","text":"<p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=YOUR_CONFIG_FILE \\\n  output_dir=\"logs/gaia-validation/20250922_1430\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"gaia_validation_text_only/","title":"GAIA Validation Text-Only","text":"<p>The GAIA (General AI Assistant) benchmark is a comprehensive evaluation dataset designed to assess AI agents' capabilities in complex, real-world reasoning tasks. The text-only variant focuses specifically on tasks that can be completed using textual reasoning and web-based research, without requiring image or video processing capabilities.</p> <p>More Details: WebThinker: Empowering Large Reasoning Models with Deep Research Capability</p> <p>Evaluation Methodology</p> <p>The text-only subset uses an LLM-as-judge evaluation approach, which differs from the exact-match evaluation used in GAIA-Validation or GAIA-Text. This methodology was established in the original WebThinker paper, and subsequent work should align with this approach for fair comparison.</p>"},{"location":"gaia_validation_text_only/#setup-and-evaluation-guide","title":"Setup and Evaluation Guide","text":""},{"location":"gaia_validation_text_only/#step-1-download-the-dataset","title":"Step 1: Download the Dataset","text":"<p>Choose one of the following methods to obtain the GAIA Validation Text-Only dataset:</p> <p>Method 1: Automated Download (Recommended)</p> Download via MiroFlow Command<pre><code>uv run main.py prepare-benchmark get gaia-val-text-only\n</code></pre> <p>Method 2: Manual Download</p> Manual Dataset Download<pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-val-text-only.zip\nunzip gaia-val-text-only.zip\n# Unzip passcode: pf4*\n</code></pre>"},{"location":"gaia_validation_text_only/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Required API Configuration</p> <p>Before running the evaluation, you must configure the necessary API keys in your <code>.env</code> file. Each service serves a specific purpose in the evaluation pipeline.</p> .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and hint generation\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre> <p>Why OpenRouter?</p> <p>We use Claude-3.7-Sonnet through the OpenRouter backend as the primary LLM provider because OpenRouter offers better response rates and improved reliability compared to direct API access.</p>"},{"location":"gaia_validation_text_only/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Execute the evaluation using the following command structure:</p> Run GAIA Validation Text-Only Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation-text-only \\\n  output_dir=\"logs/gaia-validation-text-only/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"gaia_validation_text_only/#step-4-monitor-progress-and-resume","title":"Step 4: Monitor Progress and Resume","text":"<p>Progress Tracking</p> <p>You can monitor the evaluation progress in real-time using the progress checker:</p> Check Evaluation Progress<pre><code>uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p> <p>Resume Capability</p> <p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation-text-only \\\n  output_dir=\"logs/gaia-validation-text-only/20250922_1430\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"hle/","title":"HLE","text":"<p>MiroFlow's evaluation on the HLE benchmark demonstrates capabilities in multimodal reasoning and question answering tasks that require human-level understanding across vision and language.</p> <p>More details: Humanity's Last Exam</p>"},{"location":"hle/#dataset-overview","title":"Dataset Overview","text":"<p>HLE Dataset</p> <p>The HLE dataset consists of challenging multimodal tasks that test AI systems' ability to perform human-level reasoning with both visual and textual information.</p> <p>Key Dataset Characteristics</p> <ul> <li>Total Tasks: Test split from HuggingFace <code>cais/hle</code> dataset</li> <li>Task Type: Multimodal question answering and reasoning</li> <li>Modalities: Text + Images</li> <li>Ground Truth: Available for evaluation</li> </ul>"},{"location":"hle/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"hle/#step-1-prepare-the-hle-dataset","title":"Step 1: Prepare the HLE Dataset","text":"Download HLE Dataset<pre><code>uv run main.py prepare-benchmark get hle\n</code></pre> <p>This will download the dataset and save images to <code>data/hle/images/</code>.</p>"},{"location":"hle/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":".env Configuration<pre><code># For searching and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# Claude-3.7-Sonnet via OpenRouter\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding\nANTHROPIC_API_KEY=\"xxx\"\nGEMINI_API_KEY=\"xxx\"\n\n# Hint generation and final answer extraction\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"hle/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"Run HLE Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_hle_claude37sonnet output_dir=\"logs/hle/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Resume Interrupted Evaluation</p> <p>Specify the same output directory to continue from where you left off:</p> <pre><code>uv run main.py common-benchmark --config_file_name=agent_hle_claude37sonnet output_dir=\"logs/hle/20251014_1504\"\n</code></pre>"},{"location":"hle/#step-4-review-results","title":"Step 4: Review Results","text":"Check Results<pre><code># View accuracy summary\ncat logs/hle/*/benchmark_results_pass_at_1_accuracy.txt\n\n# View detailed results\ncat logs/hle/*/benchmark_results.jsonl\n</code></pre>"},{"location":"hle/#usage-examples","title":"Usage Examples","text":""},{"location":"hle/#test-with-limited-tasks","title":"Test with Limited Tasks","text":"<pre><code>uv run main.py common-benchmark --config_file_name=agent_hle_claude37sonnet benchmark.execution.max_tasks=10 output_dir=\"logs/hle/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"hle/#adjust-concurrency","title":"Adjust Concurrency","text":"<pre><code>uv run main.py common-benchmark --config_file_name=agent_hle_claude37sonnet benchmark.execution.max_concurrent=5 output_dir=\"logs/hle/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"hle_text_only/","title":"HLE","text":"<p>MiroFlow's evaluation on the HLE-text-only benchmark demonstrates capabilities in multimodal reasoning and question answering tasks that require human-level understanding across vision and language.</p> <p>More details: HLE text only Dataset on HuggingFace</p>"},{"location":"hle_text_only/#dataset-overview","title":"Dataset Overview","text":"<p>HLE Dataset (text only)</p> <p>The dataset is a text-only subset of HLE. </p>"},{"location":"hle_text_only/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"hle_text_only/#step-1-prepare-the-hletext-only-dataset","title":"Step 1: Prepare the HLE(text only) Dataset","text":"Download HLE(text only) Dataset<pre><code>uv run main.py prepare-benchmark get hle-text-only\n</code></pre> <p>This will download the dataset to <code>data/hle-text-only/</code>.</p>"},{"location":"hle_text_only/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":".env Configuration<pre><code># For searching and web scraping\nSERPER_API_KEY=\"xxx\"\nJINA_API_KEY=\"xxx\"\n\n# For Linux sandbox (code execution environment)\nE2B_API_KEY=\"xxx\"\n\n# Claude-3.7-Sonnet via OpenRouter\nOPENROUTER_API_KEY=\"xxx\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding\nANTHROPIC_API_KEY=\"xxx\"\nGEMINI_API_KEY=\"xxx\"\n\n# Hint generation and final answer extraction\nOPENAI_API_KEY=\"xxx\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"hle_text_only/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"Run HLE Evaluation<pre><code>uv run main.py common-benchmark --config_file_name=agent_hle-text-only_claude37sonnet output_dir=\"logs/hle-text-only/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Resume Interrupted Evaluation</p> <p>Specify the same output directory to continue from where you left off:</p> <pre><code>uv run main.py common-benchmark --config_file_name=agent_hle-text-only_claude37sonnet output_dir=\"logs/hle-text-only/20251014_1504\"\n</code></pre>"},{"location":"hle_text_only/#step-4-review-results","title":"Step 4: Review Results","text":"Check Results<pre><code># View accuracy summary\ncat logs/hle-text-only/*/benchmark_results_pass_at_1_accuracy.txt\n\n# View detailed results\ncat logs/hle-text-only/*/benchmark_results.jsonl\n</code></pre>"},{"location":"hle_text_only/#usage-examples","title":"Usage Examples","text":""},{"location":"hle_text_only/#test-with-limited-tasks","title":"Test with Limited Tasks","text":"<pre><code>uv run main.py common-benchmark --config_file_name=agent_hle-text-only_claude37sonnet benchmark.execution.max_tasks=10 output_dir=\"logs/hle-text-only/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"hle_text_only/#adjust-concurrency","title":"Adjust Concurrency","text":"<pre><code>uv run main.py common-benchmark --config_file_name=agent_hle-text-only_claude37sonnet benchmark.execution.max_concurrent=5 output_dir=\"logs/hle-text-only/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#overview","title":"Overview","text":"<p>MiroFlow is released under the Apache License 2.0, which is a permissive open-source license that allows for both commercial and non-commercial use.</p> <p>License Summary</p> <ul> <li>\u2705 Commercial use - Use MiroFlow in commercial projects</li> <li>\u2705 Modification - Modify and adapt the code for your needs  </li> <li>\u2705 Distribution - Distribute original or modified versions</li> <li>\u2705 Private use - Use MiroFlow in private projects</li> <li>\u26a0\ufe0f Trademark - You cannot use MiroMind AI trademarks</li> <li>\u26a0\ufe0f Liability - No warranty or liability from the authors</li> </ul>"},{"location":"license/#apache-license-20","title":"Apache License 2.0","text":"<p>The full text of the Apache License 2.0 can be found at: https://www.apache.org/licenses/LICENSE-2.0</p>"},{"location":"license/#component-licenses","title":"Component Licenses","text":"<p>Some components within MiroFlow may have different licenses:</p> <p>Third-Party Components</p> <p>Individual components, dependencies, or integrated tools may have their own license terms. Please check the respective file headers, <code>LICENSE</code> files, or documentation for specific licensing information.</p>"},{"location":"license/#attribution","title":"Attribution","text":"<p>When using MiroFlow in your projects, attribution is appreciated but not required. You may include:</p> <pre><code>Powered by MiroFlow - https://github.com/MiroMindAI/MiroFlow\n</code></pre>"},{"location":"license/#questions","title":"Questions","text":"<p>For licensing questions or clarifications, please:</p> <ul> <li>Review the full Apache License 2.0 text</li> <li>Check individual component licenses</li> <li>Open an issue on our GitHub repository</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"llm_clients_overview/","title":"LLM Clients Overview","text":"<p>MiroFlow supports multiple LLM providers through a unified client interface. Each client handles provider-specific API communication while maintaining consistent functionality.</p>"},{"location":"llm_clients_overview/#available-clients","title":"Available Clients","text":"Client Provider Model Environment Variables <code>ClaudeAnthropicClient</code> Anthropic Direct claude-3-7-sonnet <code>ANTHROPIC_API_KEY</code>, <code>ANTHROPIC_BASE_URL</code> <code>ClaudeOpenRouterClient</code> OpenRouter anthropic/claude-3.7-sonnet, and other supported models <code>OPENROUTER_API_KEY</code>, <code>OPENROUTER_BASE_URL</code> <code>GPTOpenAIClient</code> OpenAI gpt-4, gpt-3.5 <code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code> <code>GPT5OpenAIClient</code> OpenAI gpt-5 <code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code> <code>MiroThinkerSGLangClient</code> SGLang MiroThinker series <code>OAI_MIROTHINKER_API_KEY</code>, <code>OAI_MIROTHINKER_BASE_URL</code>"},{"location":"llm_clients_overview/#basic-configuration","title":"Basic Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"ClientName\"\n    model_name: \"model-name\"\n    api_key_param: \"${oc.env:API_KEY,???}\"\n    base_url_param: \"${oc.env:BASE_URL,default-url}\"\n</code></pre>"},{"location":"llm_clients_overview/#quick-setup","title":"Quick Setup","text":"<ol> <li>Set relevant environment variables for your chosen provider</li> <li>Update your YAML config file with the appropriate client</li> <li>Run: <code>uv run main.py trace --config_file_name=your_config_file --task=\"task\"</code></li> </ol> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"miro_api/","title":"MiroAPI","text":"<p>Preview Documentation</p> <p>This service is currently in preview and limited to internal access. Public release will follow once it is production-ready.</p>"},{"location":"miro_api/#overview","title":"Overview","text":"<p>MiroAPI provides an internal caching layer for Serper Search and Jina Scrape to reduce costs, speed up development, and enable reproducible \"go-back-in-time\" sandbox runs by serving recorded results when available.</p>"},{"location":"miro_api/#step-1-apply-for-a-miroapi-key","title":"Step 1: Apply for a MiroAPI key","text":"<pre><code>Request a MiroAPI key through the internal portal.\n</code></pre>"},{"location":"miro_api/#step-2-configure-env","title":"Step 2: Configure .env","text":"<pre><code># API for Google Search (recommended)\nSERPER_API_KEY=\"svc-miro-api01-replace-with-your-key\"\nSERPER_BASE_URL=\"https://miro-api.miromind.site/serper\"\n\n# API for Web Scraping (recommended)\nJINA_API_KEY=\"svc-miro-api01-replace-with-your-key\"\nJINA_BASE_URL=\"https://miro-api.miromind.site/jina\"\n</code></pre>"},{"location":"mirothinker/","title":"MiroThinker","text":"<p>MiroThinker (4B/7B/14B/32B) is our suite of open-source agentic models, designed to work seamlessly with the MiroFlow framework. Our models are specifically built to handle complex, multi-tool tasks, leveraging the reproducible and robust foundation that MiroFlow provides.</p> <p>By combining MiroFlow's reliable orchestration with MiroThinker's advanced reasoning capabilities, we offer a powerful, end-to-end solution for building high-performing, reproducible AI agents.</p> <p>These models are a direct result of our extensive data collection efforts, utilizing MiroFlow to generate high-quality, post-training agent trace data. This unique approach enables MiroThinker to excel in planning, executing, and reasoning through complex multi-step tasks.</p>"},{"location":"mirothinker/#deploying-mirothinker-32b-with-miroflow","title":"Deploying MiroThinker-32B with MiroFlow","text":"<p>This guide explains how to deploy the MiroThinker-32B-DPO-v0.2 model from Hugging Face and integrate it with MiroFlow.</p>"},{"location":"mirothinker/#prerequisites","title":"Prerequisites","text":"<ul> <li>SGLang installed</li> <li>Sufficient GPU memory for the model</li> <li>MiroFlow repository set up</li> </ul>"},{"location":"mirothinker/#step-1-deploy-model-with-sglang","title":"Step 1: Deploy Model with SGLang","text":"<p>Deploy the MiroThinker-32B model using SGLang:</p> SGLang Server Deployment<pre><code>python3 -m sglang.launch_server \\\n    --model-path miromind-ai/MiroThinker-32B-DPO-v0.2 \\\n    --tp 8 \\\n    --dp 1 \\\n    --host 0.0.0.0 \\\n    --port 61005 \\\n    --trust-remote-code \\\n    --chat-template qwen3_nonthinking.jinja\n</code></pre> <p>Important Notes</p> <ul> <li>Adjust the <code>--tp</code> (tensor parallelism) parameter to match your number of GPUs</li> <li>Download the chat template from: qwen3_nonthinking.jinja</li> <li>Ensure the port you used (in this case 61005) is available on your system</li> </ul>"},{"location":"mirothinker/#step-2-configure-miroflow","title":"Step 2: Configure MiroFlow","text":"<p>Once the SGLang server is running, configure MiroFlow by adding the following to your <code>.env</code> file:</p> Environment Configuration<pre><code>OAI_MIROTHINKER_API_KEY=\"dummy_key\"\nOAI_MIROTHINKER_BASE_URL=\"http://localhost:61005/v1\"\n</code></pre> <p>Configuration Notes</p> <ul> <li>If your model requires authentication, replace <code>dummy_key</code> with your actual API key</li> <li>Replace <code>localhost</code> with the appropriate hostname if deploying on a remote server</li> </ul>"},{"location":"mirothinker/#step-3-test-the-integration","title":"Step 3: Test the Integration","text":"<p>Test your setup with the following command:</p> Test Command<pre><code>uv run main.py common-benchmark --config_file_name=agent_llm_mirothinker output_dir=\"logs/test\"\n</code></pre> <p>This command will: - Use the <code>agent_llm_mirothinker</code> configuration with the dedicated MiroThinkerSGLangClient - Run the example dataset benchmark (configured in the YAML file) - Test the model's question-answering capabilities</p>"},{"location":"mirothinker/#configuration-details","title":"Configuration Details","text":"<p>The <code>./config/agent_llm_mirothinker.yaml</code> configuration file uses:</p> <ul> <li><code>provider_class: \"MiroThinkerSGLangClient\"</code> - A dedicated client for MiroThinker models deployed with SGLang</li> <li>Model path and generation parameters (temperature, top_p, max_tokens, etc.)</li> <li>Environment variables for API endpoint configuration</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Xalp @ MiroMind AI</p>"},{"location":"openai-gpt4o/","title":"OpenAI GPT-4o","text":"<p>OpenAI's GPT-4o model with multimodal capabilities, strong reasoning, and efficient performance.</p>"},{"location":"openai-gpt4o/#client-configuration","title":"Client Configuration","text":"<p>Client Class: <code>GPTOpenAIClient</code></p>"},{"location":"openai-gpt4o/#environment-setup","title":"Environment Setup","text":"Environment Variables<pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # optional\n</code></pre>"},{"location":"openai-gpt4o/#agent-configuration","title":"Agent Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"GPTOpenAIClient\"\n    model_name: \"gpt-4o\"  # or gpt-4o-mini\n    async_client: true\n    temperature: 0.7\n    top_p: 1.0\n    min_p: 0.0\n    top_k: -1\n    max_tokens: 16000\n    openai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n    openai_base_url: \"${oc.env:OPENAI_BASE_URL,https://api.openai.com/v1}\"\n</code></pre>"},{"location":"openai-gpt4o/#usage","title":"Usage","text":"Example Command<pre><code># Run with GPT-4o on example dataset\nuv run main.py common-benchmark --config_file_name=agent_llm_gpt4o output_dir=\"logs/test\"\n</code></pre> <p>The <code>agent_llm_gpt4o.yaml</code> configuration file provides a ready-to-use setup with the example dataset benchmark.</p> <p>Available Models</p> <p>The <code>GPTOpenAIClient</code> supports multiple GPT-4o variants: - <code>gpt-4o</code> - Full GPT-4o model - <code>gpt-4o-mini</code> - Smaller, faster variant</p> <p>GPT-5 Support</p> <p><code>GPTOpenAIClient</code> also supports GPT-5, but it has not been fully validated on MiroFlow yet. We recommend using <code>GPT5OpenAIClient</code> for GPT-5.</p> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"openai-gpt5/","title":"OpenAI GPT-5","text":"<p>OpenAI's GPT-5 model with advanced reasoning capabilities and strong coding, vision, and problem-solving abilities.</p>"},{"location":"openai-gpt5/#client-configuration","title":"Client Configuration","text":"<p>Client Class: <code>GPT5OpenAIClient</code></p>"},{"location":"openai-gpt5/#environment-setup","title":"Environment Setup","text":"Environment Variables<pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # optional\n</code></pre>"},{"location":"openai-gpt5/#agent-configuration","title":"Agent Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"GPT5OpenAIClient\"\n    model_name: \"gpt-5\"\n    async_client: true\n    temperature: 1.0\n    top_p: 1.0\n    min_p: 0.0\n    top_k: -1\n    max_tokens: 16000\n    reasoning_effort: \"high\" # Use high in the main agent, and use the default medium in the sub-agent.\n    openai_api_key: \"${oc.env:OPENAI_API_KEY,???}\"\n    openai_base_url: \"${oc.env:OPENAI_BASE_URL,https://api.openai.com/v1}\"\n</code></pre>"},{"location":"openai-gpt5/#usage","title":"Usage","text":"Example Command<pre><code># Run with GPT-5 on example dataset\nuv run main.py common-benchmark --config_file_name=agent_llm_gpt5 output_dir=\"logs/test\"\n</code></pre> <p>The <code>agent_llm_gpt5.yaml</code> configuration file provides a ready-to-use setup with the example dataset benchmark.</p> <p>Reasoning Effort</p> <p>GPT-5 supports the <code>reasoning_effort</code> parameter. The configuration uses <code>\"high\"</code> for better reasoning performance.</p> <p>Sampling Parameters</p> <p>While <code>min_p</code> and <code>top_k</code> are required in the configuration, OpenAI's API does not use them. Set them to <code>min_p: 0.0</code> and <code>top_k: -1</code> (disabled).</p> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"openrouter-claude-3.7-sonnet/","title":"OpenRouter Claude 3.7 Sonnet (Recommended)","text":"<p>Access multiple models via OpenRouter using unified OpenAI chat format. Supports Claude, GPT, and other models with higher rate limits.</p>"},{"location":"openrouter-claude-3.7-sonnet/#client-used","title":"Client Used","text":"<p><code>ClaudeOpenRouterClient</code></p>"},{"location":"openrouter-claude-3.7-sonnet/#environment-setup","title":"Environment Setup","text":"Environment Variables<pre><code>export OPENROUTER_API_KEY=\"your-openrouter-key\"\nexport OPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"  # optional\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#configuration","title":"Configuration","text":"Agent Configuration<pre><code>main_agent:\n  llm: \n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"  # or openai/gpt-4, etc.\n    async_client: true\n    temperature: 0.3\n    top_p: 0.95\n    min_p: 0.0\n    top_k: -1\n    max_tokens: 32000\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n    openrouter_base_url: \"${oc.env:OPENROUTER_BASE_URL,https://openrouter.ai/api/v1}\"\n    openrouter_provider: \"anthropic\"  # Force provider, or \"\" for auto\n    disable_cache_control: false\n    keep_tool_result: -1\n    oai_tool_thinking: false\n</code></pre>"},{"location":"openrouter-claude-3.7-sonnet/#usage","title":"Usage","text":"Example Command<pre><code># Run with Claude 3.7 Sonnet on example dataset\nuv run main.py common-benchmark --config_file_name=agent_llm_claude37sonnet output_dir=\"logs/test\"\n</code></pre> <p>The <code>agent_llm_claude37sonnet.yaml</code> configuration file provides a ready-to-use setup with the example dataset benchmark.</p>"},{"location":"openrouter-claude-3.7-sonnet/#benefits-vs-direct-api","title":"Benefits vs Direct API","text":"<ul> <li>Unified chat format</li> <li>Higher rate limits</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"prerequisite/","title":"GAIA Validation Prerequisites","text":"<p>This document covers the common setup requirements and prerequisites for running GAIA validation benchmarks with MiroFlow, regardless of the specific model configuration used.</p>"},{"location":"prerequisite/#about-the-gaia-dataset","title":"About the GAIA Dataset","text":"<p>What is GAIA?</p> <p>GAIA (General AI Assistant) is a comprehensive benchmark designed to evaluate AI agents' ability to perform complex reasoning tasks that require multiple skills including web browsing, file manipulation, data analysis, and multi-step problem solving.</p> <p>More details: GAIA: a benchmark for General AI Assistants</p>"},{"location":"prerequisite/#performance-comparison","title":"Performance Comparison","text":"<p>State-of-the-Art Performance</p> <p>MiroFlow achieves state-of-the-art (SOTA) performance among open-source agent frameworks on the GAIA validation set.</p> <p></p> <p>Key Performance Metrics</p> <ul> <li>Pass@3: 81.8%</li> <li>Majority Vote: 82.4%</li> <li>Pass@1 (best@3): 74.5%</li> <li>Pass@1 (avg@3): 72.2%</li> </ul> <p>Reproducibility Guarantee</p> <p>Unlike other frameworks with unclear evaluation methods, MiroFlow's results are fully reproducible. Note that Hugging Face access was disabled during inference to prevent direct answer retrieval.</p>"},{"location":"prerequisite/#dataset-preparation","title":"Dataset Preparation","text":""},{"location":"prerequisite/#step-1-prepare-the-gaia-validation-dataset","title":"Step 1: Prepare the GAIA Validation Dataset","text":"<p>Choose one of the following methods to obtain the GAIA validation dataset:</p> <p>Method 1: Direct Download (Recommended)</p> <p>No Authentication Required</p> <p>This method does not require HuggingFace tokens or access permissions.</p> Manual Dataset Download<pre><code>cd data\nwget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia-val.zip\nunzip gaia-val.zip\n# Unzip passcode: pf4*\n</code></pre> <p>Method 2: Using the prepare-benchmark command</p> <p>Prerequisites Required</p> <p>This method requires HuggingFace dataset access and token configuration.</p> <p>First, you need to request access and configure your environment:</p> <ol> <li>Request Dataset Access: Visit https://huggingface.co/datasets/gaia-benchmark/GAIA and request access</li> <li>Configure Environment:     <pre><code>cp .env.template .env\n</code></pre>    Edit the <code>.env</code> file:    <pre><code>HF_TOKEN=\"your-actual-huggingface-token-here\"\nDATA_DIR=\"data/\"\n</code></pre></li> </ol> <p>Getting Your Hugging Face Token</p> <ol> <li>Go to https://huggingface.co/settings/tokens</li> <li>Create a new token with at least \"Read\" permissions</li> <li>Add your token to the <code>.env</code> file</li> </ol> <p>Then download the dataset:</p> Download via Script<pre><code>uv run main.py prepare-benchmark get gaia-val\n</code></pre>"},{"location":"prerequisite/#common-api-keys-configuration","title":"Common API Keys Configuration","text":""},{"location":"prerequisite/#required-api-keys","title":"Required API Keys","text":"<p>The following API keys are required for all GAIA validation runs, regardless of the model configuration:</p> Common .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM judge, reasoning, and hint generation\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"prerequisite/#api-key-descriptions","title":"API Key Descriptions","text":"<ul> <li>SERPER_API_KEY: Required for web search functionality</li> <li>JINA_API_KEY: Required for web scraping and content extraction</li> <li>E2B_API_KEY: Required for secure code execution environment</li> <li>ANTHROPIC_API_KEY: Required for vision understanding capabilities</li> <li>GEMINI_API_KEY: Required for additional vision processing</li> <li>OPENAI_API_KEY: Required for hint generation and final answer extraction</li> </ul>"},{"location":"prerequisite/#progress-monitoring-and-resume","title":"Progress Monitoring and Resume","text":""},{"location":"prerequisite/#progress-tracking","title":"Progress Tracking","text":"<p>You can monitor the evaluation progress in real-time:</p> Check Progress<pre><code>uv run utils/progress_check/check_gaia_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p>"},{"location":"prerequisite/#resume-capability","title":"Resume Capability","text":"<p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=YOUR_CONFIG_FILE \\\n  output_dir=\"logs/gaia-validation/20250922_1430\"\n</code></pre>"},{"location":"prerequisite/#execution-traces","title":"Execution Traces","text":"<p>Complete Execution Traces</p> <p>We have released our complete execution traces for the <code>gaia-validation</code> dataset on Hugging Face. This comprehensive collection includes a full run of 165 tasks with detailed reasoning traces.</p> <p>You can download them using the following command:</p> Download Execution Traces<pre><code>wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/gaia_validation_miroflow_trace_public_20250825.zip\nunzip gaia_validation_miroflow_trace_public_20250825.zip\n# Unzip passcode: pf4*\n</code></pre> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"quickstart/","title":"\ud83d\ude80 Get Started in Under 5 Minutes","text":"<p>Clone the repository, configure your API keys, and run your first intelligent agent. MiroFlow provides multiple pre-configured agents for different use cases.</p>"},{"location":"quickstart/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>System Requirements</p> <ul> <li>Python: 3.12 or higher</li> <li>Package Manager: <code>uv</code>, https://docs.astral.sh/uv/</li> <li>Operating System: Linux, macOS</li> </ul>"},{"location":"quickstart/#example-1-document-analysis","title":"\ud83c\udfaf Example 1: Document Analysis","text":"<p>File Processing Demo</p> <p>Analyze structured data files (Excel, CSV, PDF, etc.) with intelligent document processing.</p> <p>Required: OPENROUTER_API_KEY: to access Claude 3.7 Sonnet</p> Setup and Run Document Analysis<pre><code># 1. Clone and setup\ngit clone https://github.com/MiroMindAI/MiroFlow &amp;&amp; cd MiroFlow\nuv sync\n\n# 2. Configure API key (REQUIRED for LLM access)\ncp .env.template .env\n# Edit .env and add your OPENROUTER_API_KEY\n# This key is necessary to access Claude 3.7 Sonnet for document analysis\n\n# 3. Run document analysis\nuv run main.py trace --config_file_name=agent_quickstart_reading --task=\"What is the first country listed in the XLSX file that have names starting with Co?\" --task_file_name=\"data/FSI-2023-DOWNLOAD.xlsx\"\n</code></pre> <p>What this does:</p> <ul> <li>Uses the <code>tool-reading</code> capability to process Excel files</li> <li>Leverages Claude 3.7 Sonnet (via OpenRouter API) for intelligent analysis</li> <li>Finds countries starting with \"Co\" and returns the first one</li> </ul> <p>Expected Output</p> <p>\ud83c\udf89 Expected Output: Your agent should return \\boxed{Congo Democratic Republic}</p>"},{"location":"quickstart/#example-2-web-search-analysis","title":"\ud83c\udfaf Example 2: Web Search Analysis","text":"<p>Real-time Web Research</p> <p>Search the web for current information and get intelligent analysis of the results.</p> <p>Required: OPENROUTER_API_KEY and SERPER_API_KEY</p> Setup and Run Web Search<pre><code># 1. Clone and setup (if not done already)\ngit clone https://github.com/MiroMindAI/MiroFlow &amp;&amp; cd MiroFlow\nuv sync\n\n# 2. Configure API keys (if not done already)\ncp .env.template .env\n# Edit .env and add your OPENROUTER_API_KEY and SERPER_API_KEY\n# These keys are necessary to access Claude 3.7 Sonnet and web search capabilities\n\n# 3. Run web search analysis\nuv run main.py trace --config_file_name=agent_quickstart_search --task=\"What is the current NASDAQ index price and what are the main factors affecting it today?\"\n</code></pre> <p>What this does:</p> <ul> <li>Uses the <code>tool-searching-serper</code> capability to search the web</li> <li>Leverages Claude 3.7 Sonnet (via OpenRouter API) for intelligent analysis</li> <li>Searches for current NASDAQ index information and market factors</li> <li>Provides real-time financial data analysis</li> </ul> <p>Expected Output</p> <p>\ud83c\udf89 Expected Output: Current NASDAQ index price with analysis of key market factors affecting it</p>"},{"location":"quickstart/#configuration-options","title":"\ud83d\udd27 Configuration Options","text":""},{"location":"quickstart/#available-agent-configurations","title":"Available Agent Configurations","text":"Agent Tools Use Case <code>agent_quickstart_reading</code> Document reading File analysis, data extraction, document summarization <code>agent_quickstart_search</code> Web search Real-time information, market data, current events"},{"location":"quickstart/#customizing-tasks","title":"Customizing Tasks","text":"<p>You can customize any task by modifying the <code>--task</code> parameter:</p> <pre><code># Analyze different files\nuv run main.py trace --config_file_name=agent_quickstart_reading \\\n  --task=\"Summarize the main findings in this document\" \\\n  --task_file_name=\"path/to/your/document.pdf\"\n\n# Search for different information\nuv run main.py trace --config_file_name=agent_quickstart_search \\\n  --task=\"What are the latest developments in AI technology?\"\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"quickstart/#common-issues","title":"Common Issues","text":"<p>API Key Issues</p> <p>Problem: Agent fails to start or returns errors Solution: Ensure your API keys are correctly set in the <code>.env</code> file: <pre><code>OPENROUTER_API_KEY=your_key_here\nSERPER_API_KEY=your_key_here  # For web search examples\n</code></pre></p> <p>Tool Execution Errors</p> <p>Problem: Tools fail to execute Solution: Check that all dependencies are installed: <pre><code>uv sync  # Reinstall dependencies\n</code></pre></p>"},{"location":"quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>Check the FAQ section for common questions</li> <li>Review the YAML Configuration Guide for advanced setup</li> <li>Explore Tool Documentation for available capabilities</li> </ul>"},{"location":"quickstart/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Once you've tried the examples above, explore more advanced features:</p> <ol> <li> <p>Custom Agent Configuration: Create your own agent setups    <pre><code># Copy and modify existing configs\ncp config/agent_quickstart_reading.yaml config/my_custom_agent.yaml\n</code></pre></p> </li> <li> <p>Tool Development: Add custom tools for your specific needs</p> </li> <li>See Contributing Tools guide</li> </ol> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_audio/","title":"Audio Tools (<code>tool-audio</code>)","text":"<p>Audio processing capabilities including transcription and audio-based question answering.</p>"},{"location":"tool_audio/#configuration","title":"Configuration","text":"Agent Configuration<pre><code>main_agent:\n  tool_config: \n    - tool-audio\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>OPENAI_API_KEY</code>: Required. OpenAI API key</li> <li><code>OPENAI_BASE_URL</code>: API base URL. Default: <code>https://api.openai.com/v1</code></li> <li><code>OPENAI_TRANSCRIPTION_MODEL_NAME</code>: Default: <code>gpt-4o-transcribe</code></li> <li><code>OPENAI_AUDIO_MODEL_NAME</code>: Default: <code>gpt-4o-audio-preview</code></li> </ul>"},{"location":"tool_audio/#function-reference","title":"Function Reference","text":""},{"location":"tool_audio/#audio_transcriptionaudio_path_or_url-str","title":"<code>audio_transcription(audio_path_or_url: str)</code>","text":"<p>Transcribe audio file to text using OpenAI's Whisper models.</p> <p>Parameters:</p> <ul> <li><code>audio_path_or_url</code>: Local file path or URL</li> <li>Supported formats: MP3, WAV, M4A, FLAC, OGG, WebM</li> <li>Not supported: E2B sandbox paths, YouTube URLs</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Full transcription of the audio file</li> </ul> <p>Example:</p> <pre><code># Transcribe local audio\ntranscription = await audio_transcription(\"/data/meeting.mp3\")\n\n# Transcribe from URL\ntranscription = await audio_transcription(\"https://example.com/podcast.wav\")\n</code></pre>"},{"location":"tool_audio/#audio_question_answeringaudio_path_or_url-str-question-str","title":"<code>audio_question_answering(audio_path_or_url: str, question: str)</code>","text":"<p>Answer questions based on audio content using GPT-4o Audio.</p> <p>Parameters:</p> <ul> <li><code>audio_path_or_url</code>: Local file path or URL (same formats as transcription)</li> <li><code>question</code>: Question to answer about the audio content</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Answer with audio duration information</li> </ul> <p>Example:</p> <pre><code># Ask about content\nanswer = await audio_question_answering(\n    \"/data/lecture.mp3\", \n    \"What are the main topics discussed?\"\n)\n\n# Get summary\nanswer = await audio_question_answering(\n    \"https://example.com/interview.wav\",\n    \"Summarize the key points.\"\n)\n</code></pre> <p>Important Notes:</p> <ul> <li>Cannot access E2B sandbox files (<code>/home/user/</code>)</li> <li>YouTube URLs not supported (use VQA tools instead)</li> <li>Includes audio duration in response</li> </ul> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_audio_os/","title":"Audio Tools - Open Source (<code>audio_mcp_server_os.py</code>)","text":"<p>The Audio MCP Server (Open Source) enables audio transcription using open-source Whisper models. It provides comprehensive audio-to-text conversion with support for multiple audio formats, local files, and URLs.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Audio Transcription: High-quality speech-to-text conversion</li> <li>Multi-Format Support: MP3, WAV, M4A, AAC, OGG, FLAC, WMA formats</li> <li>Flexible Input: Local file paths and web URLs</li> <li>Open-Source Model Support: Whisper-Large-v3-Turbo with automatic processing</li> </ul>"},{"location":"tool_audio_os/#environment-variables","title":"Environment Variables","text":"<p>Configuration Location</p> <p>The <code>audio_mcp_server_os.py</code> reads environment variables that are passed through the <code>tool-audio-os.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <p>Open-Source Model Configuration:</p> <ul> <li><code>WHISPER_API_KEY</code>: Required API key for the open-source Whisper service</li> <li><code>WHISPER_BASE_URL</code>: Base URL for the Whisper service API endpoint</li> <li><code>WHISPER_MODEL_NAME</code>: Model name (default: <code>openai/whisper-large-v3-turbo</code>)</li> </ul> <p>Example Configuration: <pre><code># API for Open-Source Audio Transcription Tool (for benchmark testing)\nWHISPER_MODEL_NAME=\"openai/whisper-large-v3-turbo\"\nWHISPER_API_KEY=your_whisper_key\nWHISPER_BASE_URL=\"https://your_whisper_base_url/v1\"\n</code></pre></p>"},{"location":"tool_audio_os/#local-deployment","title":"Local Deployment","text":""},{"location":"tool_audio_os/#using-vllm-server","title":"Using vLLM Server","text":"<p>For optimal performance with the Whisper-Large-v3-Turbo model, deploy using vLLM:</p> <pre><code>pip install vllm==0.10.0\npip install vllm[audio]\nvllm serve /path/to/whisper \\\n  --served-model-name whisper-large-v3-turbo \\\n  --task transcription\n</code></pre>"},{"location":"tool_audio_os/#configuration-for-local-deployment","title":"Configuration for Local Deployment","text":"<p>When using local deployment, configure your environment variables:</p> <pre><code>WHISPER_MODEL_NAME=\"openai/whisper-large-v3-turbo\"\nWHISPER_API_KEY=\"dummy_key\"  # Not required for local deployment\nWHISPER_BASE_URL=\"http://localhost:8000/v1\"\n</code></pre>"},{"location":"tool_audio_os/#function-reference","title":"Function Reference","text":"<p>The following function is provided by the <code>audio_mcp_server_os.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_audio_os/#audio_transcriptionaudio_path_or_url-str","title":"<code>audio_transcription(audio_path_or_url: str)</code>","text":"<p>Transcribe audio files to text using open-source Whisper models. Supports both local files and web URLs with automatic format detection and processing.</p> <p>Parameters:</p> <ul> <li><code>audio_path_or_url</code>: Local file path (accessible to server) or web URL</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: The transcription of the audio file</li> </ul> <p>Supported Audio Formats: - MP3 (.mp3) - WAV (.wav) - M4A (.m4a) - AAC (.aac) - OGG (.ogg) - FLAC (.flac) - WMA (.wma)</p>"},{"location":"tool_audio_os/#usage-examples","title":"Usage Examples","text":""},{"location":"tool_audio_os/#local-file-transcription","title":"Local File Transcription","text":"<pre><code># Local file transcription\nresult = audio_transcription(\n    audio_path_or_url=\"/path/to/audio.mp3\"\n)\n</code></pre>"},{"location":"tool_audio_os/#url-based-transcription","title":"URL-based Transcription","text":"<pre><code># URL transcription\nresult = audio_transcription(\n    audio_path_or_url=\"https://example.com/audio.wav\"\n)\n</code></pre>"},{"location":"tool_audio_os/#meeting-recording-transcription","title":"Meeting Recording Transcription","text":"<pre><code>result = audio_transcription(\n    audio_path_or_url=\"meeting_recording.m4a\"\n)\n</code></pre>"},{"location":"tool_audio_os/#podcast-transcription","title":"Podcast Transcription","text":"<pre><code>result = audio_transcription(\n    audio_path_or_url=\"podcast_episode.mp3\"\n)\n</code></pre>"},{"location":"tool_audio_os/#technical-implementation","title":"Technical Implementation","text":""},{"location":"tool_audio_os/#audio-processing-pipeline","title":"Audio Processing Pipeline","text":"<ol> <li>Input Validation: Checks if input is local file or URL</li> <li>Format Detection: Determines audio format from extension or content type</li> <li>File Handling: Downloads URL files to temporary storage with proper extensions</li> <li>API Request: Sends audio file to Whisper model for transcription</li> <li>Cleanup: Removes temporary files after processing</li> <li>Response Processing: Returns transcription text</li> </ol>"},{"location":"tool_audio_os/#error-handling","title":"Error Handling","text":"<ul> <li>File Access Errors: Graceful handling of inaccessible local files</li> <li>Network Errors: Robust URL fetching with retry logic (up to 3 attempts)</li> <li>Format Errors: Automatic format detection and validation</li> <li>API Errors: Clear error reporting for service issues</li> <li>Sandbox Restrictions: Prevents access to sandbox files with clear error messages</li> </ul>"},{"location":"tool_audio_os/#retry-logic","title":"Retry Logic","text":"<ul> <li>Maximum Retries: 3 attempts for failed requests</li> <li>Exponential Backoff: 5, 10, 20 second delays between retries</li> <li>Network Resilience: Handles temporary network issues and service unavailability</li> </ul> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_overview/","title":"Tool Overview","text":"<p>MiroFlow provides a comprehensive set of tools that extend agent capabilities through the Model Context Protocol (MCP).</p>"},{"location":"tool_overview/#available-tools","title":"Available Tools","text":"<p>Core Tools</p> <ul> <li>Python Tools - Code execution in secure sandbox</li> <li>Searching Tools - Web search and content retrieval  </li> <li>Vision Tools - Image analysis and video processing</li> <li>Reasoning Tools - Advanced logical analysis</li> </ul> <p>Additional Tools Available</p> <p>MiroFlow includes additional tools for audio processing, document reading, web browsing, and markdown conversion. See the <code>config/tool/</code> directory for complete tool configurations.</p>"},{"location":"tool_overview/#quick-setup","title":"Quick Setup","text":"<p>Tools are configured in agent YAML files and require API keys in your <code>.env</code> file. See individual tool documentation for detailed setup instructions.</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_python/","title":"Python Tools (<code>python_server.py</code>)","text":"<p>The Python Execution Server provides a secure sandboxed environment for running Python code and shell commands using E2B server. This tool enables agents to execute code safely, manipulate files, and perform computational tasks in an isolated environment.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Sandbox Management: Create and manage isolated execution environments</li> <li>Code Execution: Run Python code and shell commands safely</li> <li>File Operations: Upload, download, and transfer files between local and sandbox</li> <li>Internet Access: Download files directly from web sources to sandbox</li> </ul>"},{"location":"tool_python/#function-reference","title":"Function Reference","text":"<p>The following functions are provided by the <code>python_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_python/#create_sandbox","title":"<code>create_sandbox()</code>","text":"<p>Creates a Linux sandbox for safely executing commands and running Python code.</p> <p>Returns: - <code>str</code>: The <code>sandbox_id</code> of the newly created sandbox</p> <p>Important Usage Notes</p> <ul> <li>Required First Step: This tool must be called before using other tools within this MCP server</li> <li>Session Management: The sandbox may timeout and automatically shut down after inactivity</li> <li>Pre-installed Environment: The sandbox comes pre-installed with common packages for data science and document processing. For a detailed list and advanced usage information, see E2B Extension</li> </ul>"},{"location":"tool_python/#run_commandsandbox_id-str-command-str","title":"<code>run_command(sandbox_id: str, command: str)</code>","text":"<p>Execute shell commands in the Linux sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox (must be created first) - <code>command</code>: Shell command to execute</p> <p>Returns: - <code>str</code>: Command execution result (stderr, stdout, exit_code, error)</p> <p>Features: - Automatic retry mechanism - Permission hints for sudo commands</p>"},{"location":"tool_python/#run_python_codesandbox_id-str-code_block-str","title":"<code>run_python_code(sandbox_id: str, code_block: str)</code>","text":"<p>Run Python code in the sandbox and return execution results.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>code_block</code>: Python code to execute</p> <p>Returns: - <code>str</code>: Code execution result (stderr, stdout, exit_code, error)</p> <p>Features: - Automatic retry mechanism</p>"},{"location":"tool_python/#upload_file_from_local_to_sandboxsandbox_id-str-local_file_path-str-sandbox_file_path-str-homeuser","title":"<code>upload_file_from_local_to_sandbox(sandbox_id: str, local_file_path: str, sandbox_file_path: str = \"/home/user\")</code>","text":"<p>Upload local files to the sandbox environment.</p> <p>When to Use</p> <p>When a local file is provided to the agent, the agent needs to call this tool to copy the file from local storage to the sandbox for further file processing.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>local_file_path</code>: Local path of the file to upload - <code>sandbox_file_path</code>: Target directory in sandbox (default: <code>/home/user</code>)</p> <p>Returns: - <code>str</code>: Path of uploaded file in sandbox or error message</p>"},{"location":"tool_python/#download_file_from_internet_to_sandboxsandbox_id-str-url-str-sandbox_file_path-str-homeuser","title":"<code>download_file_from_internet_to_sandbox(sandbox_id: str, url: str, sandbox_file_path: str = \"/home/user\")</code>","text":"<p>Download files from the internet directly to the sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the existing sandbox - <code>url</code>: URL of the file to download - <code>sandbox_file_path</code>: Target directory in sandbox (default: <code>/home/user</code>)</p> <p>Returns: - <code>str</code>: Path of downloaded file in sandbox or error message</p> <p>Features: - Automatic retry mechanism</p>"},{"location":"tool_python/#download_file_from_sandbox_to_localsandbox_id-str-sandbox_file_path-str-local_filename-str-none","title":"<code>download_file_from_sandbox_to_local(sandbox_id: str, sandbox_file_path: str, local_filename: str = None)</code>","text":"<p>Download files from sandbox to local system for processing by other tools.</p> <p>Inter-tool Communication</p> <p>Other MCP tools (such as visual question answering) cannot access files in a sandbox. Therefore, this tool should be called when the agent wants other tools to analyze files in the sandbox.</p> <p>Parameters: - <code>sandbox_id</code>: ID of the sandbox - <code>sandbox_file_path</code>: Path of file in sandbox - <code>local_filename</code>: Optional local filename (uses original if not provided)</p> <p>Returns: - <code>str</code>: Local path of downloaded file or error message</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_reading/","title":"Reading Tools (<code>tool-reading</code>)","text":"<p>Read and convert various document formats (DOC, PDF, Excel, etc.) to markdown for easy processing.</p>"},{"location":"tool_reading/#configuration","title":"Configuration","text":"Agent Configuration<pre><code>main_agent:\n  tool_config: \n    - tool-reading\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>SERPER_API_KEY</code>: Required for certain operations</li> <li><code>JINA_API_KEY</code>: Required for document processing</li> </ul>"},{"location":"tool_reading/#function-reference","title":"Function Reference","text":""},{"location":"tool_reading/#read_fileuri-str","title":"<code>read_file(uri: str)</code>","text":"<p>Read various types of resources and convert them to markdown format.</p> <p>Parameters:</p> <ul> <li><code>uri</code>: The URI or path of the resource to read. Supported:</li> <li>Local file paths (e.g., <code>/path/to/document.pdf</code>)</li> <li><code>file:</code> URIs (e.g., <code>file:/path/to/document.pdf</code>)</li> <li><code>http:</code> / <code>https:</code> URLs (will be downloaded automatically)</li> <li><code>data:</code> URIs (base64-encoded)</li> </ul> <p>Supported Formats:</p> <ul> <li>Documents: DOC, DOCX, RTF, ODT</li> <li>Presentations: PPT, PPTX, ODP</li> <li>Spreadsheets: XLS, XLSX, CSV, ODS</li> <li>PDFs: PDF documents</li> <li>Archives: ZIP files</li> <li>Images and text files</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Content in markdown format, or error message if reading fails</li> </ul> <p>Example:</p> <pre><code># Read a local PDF\nresult = await read_file(\"file:/path/to/document.pdf\")\n\n# Read from URL\nresult = await read_file(\"https://example.com/report.pdf\")\n\n# Read local file (auto-converted to file: URI)\nresult = await read_file(\"/data/spreadsheet.xlsx\")\n</code></pre> <p>Important Notes:</p> <ul> <li>Cannot access E2B sandbox files (<code>/home/user/</code>)</li> <li>Use local file paths provided in the original instruction</li> <li>Downloaded files are automatically cleaned up</li> </ul> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_reasoning/","title":"Reasoning Tools (<code>reasoning_mcp_server.py</code>)","text":"<p>The Reasoning MCP Server provides a pure text-based reasoning engine. It supports logical analysis, problem solving, and planning, using LLM backends (OpenAI or Anthropic) with retry and exponential backoff for robustness.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Pure Text Reasoning: Logical analysis and problem solving using advanced LLM backends</li> <li>Step-by-Step Analysis: Structured reasoning with detailed explanations</li> <li>Multi-Backend Support: OpenAI or Anthropic models with automatic fallback</li> </ul>"},{"location":"tool_reasoning/#environment-variables","title":"Environment Variables","text":"<p>Configuration Location</p> <p>The <code>reasoning_mcp_server.py</code> reads environment variables that are passed through the <code>tool-reasoning.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <p>OpenAI Configuration:</p> <ul> <li><code>OPENAI_API_KEY</code>: Required API key for OpenAI services</li> <li><code>OPENAI_BASE_URL</code>: Default = <code>https://api.openai.com/v1</code></li> <li><code>OPENAI_MODEL_NAME</code>: Default = <code>o3</code></li> </ul> <p>Anthropic Configuration:</p> <ul> <li><code>ANTHROPIC_API_KEY</code>: Required API key for Anthropic services</li> <li><code>ANTHROPIC_BASE_URL</code>: Default = <code>https://api.anthropic.com</code></li> <li><code>ANTHROPIC_MODEL_NAME</code>: Default = <code>claude-3-7-sonnet-20250219</code></li> </ul>"},{"location":"tool_reasoning/#function-reference","title":"Function Reference","text":"<p>The following function is provided by the <code>reasoning_mcp_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_reasoning/#reasoningquestion-str","title":"<code>reasoning(question: str)</code>","text":"<p>Perform step-by-step reasoning, analysis, and planning over a text-only input. This tool is specialized for complex thinking tasks.</p> <p>Text-Only Processing</p> <p>This tool processes only the provided text input and will not fetch external data or context. Ensure all necessary information is included in the question.</p> <p>Parameters:</p> <ul> <li><code>question</code>: A detailed, complex question or problem statement that includes all necessary information</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: A structured, step-by-step reasoned answer</li> </ul> <p>Features:</p> <ul> <li>Runs on OpenAI or Anthropic models, depending on available API keys</li> <li>Exponential backoff retry logic (up to 5 attempts)</li> <li>For Anthropic, uses Thinking mode with token budget (21k max, 19k thinking)</li> <li>Ensures non-empty responses with fallback error reporting</li> <li>Automatic backend selection based on available API keys</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_reasoning_os/","title":"Reasoning Tools - Open Source (<code>reasoning_mcp_server_os.py</code>)","text":"<p>The Reasoning MCP Server (Open Source) provides a pure text-based reasoning engine using open-source models. It supports logical analysis, problem solving, and planning, with robust retry mechanisms and exponential backoff for reliability.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Pure Text Reasoning: Logical analysis and problem solving using open-source LLM backends</li> <li>Step-by-Step Analysis: Structured reasoning with detailed explanations</li> <li>Open-Source Model Support: Qwen3-235B-A22B-Thinking-2507 with automatic fallback</li> <li>Robust Error Handling: Exponential backoff retry logic (up to 10 attempts)</li> </ul>"},{"location":"tool_reasoning_os/#environment-variables","title":"Environment Variables","text":"<p>Configuration Location</p> <p>The <code>reasoning_mcp_server_os.py</code> reads environment variables that are passed through the <code>tool-reasoning-os.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <p>Open-Source Model Configuration:</p> <ul> <li><code>REASONING_API_KEY</code>: Required API key for the open-source reasoning service</li> <li><code>REASONING_BASE_URL</code>: Base URL for the reasoning service API endpoint</li> <li><code>REASONING_MODEL_NAME</code>: Model name (default: <code>Qwen/Qwen3-235B-A22B-Thinking-2507</code>)</li> </ul> <p>Example Configuration: <pre><code># API for Open-Source Reasoning Tool (for benchmark testing)\nREASONING_MODEL_NAME=\"Qwen/Qwen3-235B-A22B-Thinking-2507\"\nREASONING_API_KEY=your_reasoning_key\nREASONING_BASE_URL=\"https://your_reasoning_base_url/v1/chat/completions\"\n</code></pre></p>"},{"location":"tool_reasoning_os/#local-deployment","title":"Local Deployment","text":""},{"location":"tool_reasoning_os/#using-sglang-server","title":"Using SGLang Server","text":"<p>For optimal performance with the Qwen3-235B-A22B-Thinking model, deploy using SGLang:</p> <pre><code>python3 -m sglang.launch_server \\\n  --model-path /path/to/Qwen3-235B-A22B-Thinking-2507 \\\n  --tp 8 --host 0.0.0.0 --port 1234 \\\n  --trust-remote-code --enable-metrics \\\n  --log-level debug --log-level-http debug \\\n  --log-requests --log-requests-level 2 \\\n  --show-time-cost --context-length 131072\n</code></pre>"},{"location":"tool_reasoning_os/#configuration-for-local-deployment","title":"Configuration for Local Deployment","text":"<p>When using local deployment, configure your environment variables:</p> <pre><code>REASONING_MODEL_NAME=\"Qwen/Qwen3-235B-A22B-Thinking-2507\"\nREASONING_API_KEY=\"dummy_key\"  # Not required for local deployment\nREASONING_BASE_URL=\"http://localhost:1234/v1/chat/completions\"\n</code></pre>"},{"location":"tool_reasoning_os/#function-reference","title":"Function Reference","text":"<p>The following function is provided by the <code>reasoning_mcp_server_os.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_reasoning_os/#reasoningquestion-str","title":"<code>reasoning(question: str)</code>","text":"<p>Perform step-by-step reasoning, analysis, and planning over a text-only input. This tool is specialized for complex thinking tasks that require deep analytical reasoning.</p> <p>Text-Only Processing</p> <p>This tool processes only the provided text input and will not fetch external data or context. Ensure all necessary information is included in the question.</p> <p>Parameters:</p> <ul> <li><code>question</code>: A detailed, complex question or problem statement that includes all necessary information</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: A structured, step-by-step reasoned answer</li> </ul> <p>Features:</p> <ul> <li>Open-Source Model: Uses Qwen3-235B-A22B-Thinking-2507 for advanced reasoning</li> <li>Robust Retry Logic: Exponential backoff retry mechanism (up to 10 attempts)</li> <li>Thinking Mode Support: Automatically extracts reasoning content from thinking blocks</li> <li>Error Handling: Graceful fallback with informative error messages</li> <li>Timeout Protection: 600-second timeout for long-running reasoning tasks</li> <li>Jittered Backoff: Prevents thundering herd problems with randomized retry delays</li> </ul> <p>Retry Configuration: - Maximum retries: 10 attempts - Initial backoff: 1.0 seconds - Maximum backoff: 30.0 seconds - Exponential backoff with jitter (0.8-1.2x multiplier)</p>"},{"location":"tool_reasoning_os/#usage-examples","title":"Usage Examples","text":""},{"location":"tool_reasoning_os/#complex-mathematical-problems","title":"Complex Mathematical Problems","text":"<pre><code>question = \"\"\"\nSolve this complex optimization problem:\nA company wants to minimize costs while maximizing production. \nGiven constraints: 2x + 3y \u2264 100, x + y \u2264 50, x \u2265 0, y \u2265 0\nCost function: C = 5x + 8y\nProduction function: P = 3x + 4y\nFind the optimal values of x and y.\n\"\"\"\n</code></pre>"},{"location":"tool_reasoning_os/#logical-puzzles","title":"Logical Puzzles","text":"<pre><code>question = \"\"\"\nThree people are in a room: Alice, Bob, and Charlie. \n- Alice says: \"Bob is lying\"\n- Bob says: \"Charlie is lying\" \n- Charlie says: \"Alice is lying\"\nIf exactly one person is telling the truth, who is it?\n\"\"\"\n</code></pre>"},{"location":"tool_reasoning_os/#strategic-planning","title":"Strategic Planning","text":"<pre><code>question = \"\"\"\nDesign a strategy for a startup to enter a competitive market \nwith limited resources. Consider market analysis, competitive \npositioning, resource allocation, and risk mitigation.\n\"\"\"\n</code></pre> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_searching/","title":"Searching Tools (<code>searching_mcp_server.py</code>)","text":"<p>The Searching MCP Server provides comprehensive search capabilities including Google search, Wikipedia content retrieval, archive searching, and web scraping functionality.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Google Search: Comprehensive web search with filtering and localization</li> <li>Wikipedia Access: Page content retrieval and revision history tracking</li> <li>Archive Search: Wayback Machine integration for historical web content</li> <li>Web Scraping: Content extraction from websites and YouTube videos</li> </ul>"},{"location":"tool_searching/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables configure the search tools:</p> <ul> <li><code>SERPER_API_KEY</code>: Required API key for Serper service, used by <code>google_search</code> and as a fallback for <code>scrape_website</code></li> <li><code>JINA_API_KEY</code>: Required API key for JINA service. Default choice for scraping websites in <code>scrape_website</code></li> <li><code>REMOVE_SNIPPETS</code>: Set to \"true\" to filter out snippets from results. Used in <code>google_search</code> to filter the search results returned by Serper</li> <li><code>REMOVE_KNOWLEDGE_GRAPH</code>: Set to \"true\" to remove knowledge graph data. Used in <code>google_search</code> to filter the search results returned by Serper</li> <li><code>REMOVE_ANSWER_BOX</code>: Set to \"true\" to remove answer box content. Used in <code>google_search</code> to filter the search results returned by Serper</li> </ul>"},{"location":"tool_searching/#function-reference","title":"Function Reference","text":"<p>The following functions are provided by the <code>searching_mcp_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_searching/#google_searchq-str-gl-str-us-hl-str-en-location-str-none-num-int-10-tbs-str-none-page-int-1","title":"<code>google_search(q: str, gl: str = \"us\", hl: str = \"en\", location: str = None, num: int = 10, tbs: str = None, page: int = 1)</code>","text":"<p>Perform Google searches via Serper API and retrieve rich search results including organic results, people also ask, related searches, and knowledge graph.</p> <p>Parameters:</p> <ul> <li><code>q</code>: Search query string</li> <li><code>gl</code>: Country context for search (e.g., 'us' for United States, 'cn' for China, 'uk' for United Kingdom). Default: 'us'</li> <li><code>hl</code>: Google interface language (e.g., 'en' for English, 'zh' for Chinese, 'es' for Spanish). Default: 'en'</li> <li><code>location</code>: City-level location for search results (e.g., 'SoHo, New York, United States', 'California, United States')</li> <li><code>num</code>: Number of results to return. Default: 10</li> <li><code>tbs</code>: Time-based search filter ('qdr:h' for past hour, 'qdr:d' for past day, 'qdr:w' for past week, 'qdr:m' for past month, 'qdr:y' for past year)</li> <li><code>page</code>: Page number of results to return. Default: 1</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: JSON formatted search results with organic results and related information</li> </ul> <p>Features:</p> <ul> <li>Automatic retry mechanism (up to 5 attempts)</li> <li>Configurable result filtering via environment variables</li> <li>Support for regional and language-specific searches</li> </ul>"},{"location":"tool_searching/#wiki_get_page_contententity-str-first_sentences-int-10","title":"<code>wiki_get_page_content(entity: str, first_sentences: int = 10)</code>","text":"<p>Get specific Wikipedia page content for entities (people, places, concepts, events) and return structured information.</p> <p>Parameters:</p> <ul> <li><code>entity</code>: The entity to search for in Wikipedia</li> <li><code>first_sentences</code>: Number of first sentences to return from the page. Set to 0 to return full content. Default: 10</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted content containing page title, introduction/full content, and URL</li> </ul> <p>Features:</p> <ul> <li>Handles disambiguation pages automatically</li> <li>Provides clean, structured output</li> <li>Fallback search suggestions when page not found</li> <li>Automatic content truncation for manageable output</li> </ul>"},{"location":"tool_searching/#search_wiki_revisionentity-str-year-int-month-int-max_revisions-int-50","title":"<code>search_wiki_revision(entity: str, year: int, month: int, max_revisions: int = 50)</code>","text":"<p>Search for an entity in Wikipedia and return the revision history for a specific month.</p> <p>Parameters:</p> <ul> <li><code>entity</code>: The entity to search for in Wikipedia</li> <li><code>year</code>: The year of the revision (e.g., 2024)</li> <li><code>month</code>: The month of the revision (1-12)</li> <li><code>max_revisions</code>: Maximum number of revisions to return. Default: 50</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted revision history with timestamps, revision IDs, and URLs</li> </ul> <p>Features:</p> <ul> <li>Automatic date validation and adjustment</li> <li>Support for date range from 2000 to current year</li> <li>Detailed revision metadata including timestamps and direct links</li> <li>Clear error handling for invalid dates or missing pages</li> </ul>"},{"location":"tool_searching/#search_archived_webpageurl-str-year-int-month-int-day-int","title":"<code>search_archived_webpage(url: str, year: int, month: int, day: int)</code>","text":"<p>Search the Wayback Machine (archive.org) for archived versions of a webpage for a specific date.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL to search for in the Wayback Machine</li> <li><code>year</code>: The target year (e.g., 2023)</li> <li><code>month</code>: The target month (1-12)</li> <li><code>day</code>: The target day (1-31)</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Formatted archive information including archived URL, timestamp, and availability status</li> </ul> <p>Features:</p> <ul> <li>Automatic URL protocol detection and correction</li> <li>Date validation and adjustment (1995 to present)</li> <li>Fallback to most recent archive if specific date not found</li> <li>Special handling for Wikipedia URLs with tool suggestions</li> <li>Automatic retry mechanism for reliable results</li> </ul>"},{"location":"tool_searching/#scrape_websiteurl-str","title":"<code>scrape_website(url: str)</code>","text":"<p>Scrape website content including support for regular websites and YouTube video information.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL of the website to scrape</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Scraped website content including text, metadata, and structured information</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_searching_serper/","title":"Searching Tools - Serper (<code>tool-searching-serper</code>)","text":"<p>Lightweight Google search and web scraping via Serper API using NPM package.</p> <p>Which Tool to Use?</p> <ul> <li><code>tool-searching-serper</code>: Fast Google search + basic scraping (NPM-based)</li> <li><code>tool-searching</code>: Full-featured with Wikipedia, Archive.org, JINA (Python-based)</li> </ul>"},{"location":"tool_searching_serper/#configuration","title":"Configuration","text":"Agent Configuration<pre><code>main_agent:\n  tool_config: \n    - tool-searching-serper\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>SERPER_API_KEY</code>: Required. Get at serper.dev</li> </ul>"},{"location":"tool_searching_serper/#function-reference","title":"Function Reference","text":""},{"location":"tool_searching_serper/#google_searchq-str-gl-str-us-hl-str-en-location-str-none-num-int-10-tbs-str-none-page-int-1","title":"<code>google_search(q: str, gl: str = \"us\", hl: str = \"en\", location: str = None, num: int = 10, tbs: str = None, page: int = 1)</code>","text":"<p>Perform Google searches via Serper API.</p> <p>Parameters:</p> <ul> <li><code>q</code>: Search query (required)</li> <li><code>gl</code>: Country code (e.g., 'us', 'uk', 'cn'). Default: 'us'</li> <li><code>hl</code>: Language (e.g., 'en', 'zh', 'es'). Default: 'en'</li> <li><code>location</code>: City location (e.g., 'San Francisco, California, United States')</li> <li><code>num</code>: Number of results. Default: 10</li> <li><code>tbs</code>: Time filter ('qdr:h'=hour, 'qdr:d'=day, 'qdr:w'=week, 'qdr:m'=month, 'qdr:y'=year)</li> <li><code>page</code>: Page number. Default: 1</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: JSON formatted search results</li> </ul> <p>Example:</p> <pre><code># Basic search\nresults = await google_search(\"artificial intelligence\")\n\n# With filters\nresults = await google_search(\"latest news\", tbs=\"qdr:d\", num=20)\n</code></pre>"},{"location":"tool_searching_serper/#scrapeurl-str","title":"<code>scrape(url: str)</code>","text":"<p>Scrape website content using Serper.</p> <p>Parameters:</p> <ul> <li><code>url</code>: Website URL to scrape</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Scraped content</li> </ul> <p>Example:</p> <pre><code>content = await scrape(\"https://example.com/article\")\n</code></pre>"},{"location":"tool_searching_serper/#comparison-serper-vs-full-searching","title":"Comparison: Serper vs Full Searching","text":"Feature <code>tool-searching-serper</code> <code>tool-searching</code> Google Search \u2705 \u2705 Web Scraping \u2705 Basic \u2705 Advanced Wikipedia \u274c \u2705 Archive.org \u274c \u2705 YouTube Info \u274c \u2705 Speed \u26a1 Faster Slightly slower Dependencies Node.js/NPM Python only <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_vqa/","title":"Vision Tools (<code>vision_mcp_server.py</code>)","text":"<p>The Vision MCP Server enables OCR + Visual Question Answering (VQA) over images and multimodal understanding of YouTube videos, with pluggable backends (Anthropic, OpenAI, Google Gemini).</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Visual Question Answering: OCR and VQA analysis of images with dual-pass processing</li> <li>YouTube Video Analysis: Audio and visual analysis of public YouTube videos</li> <li>Multi-Backend Support: Configurable vision backends (Anthropic, OpenAI, Gemini)</li> </ul>"},{"location":"tool_vqa/#environment-variables","title":"Environment Variables","text":"<p>Configuration Location</p> <p>The <code>vision_mcp_server.py</code> reads environment variables that are passed through the <code>tool-image-video.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <p>Vision Backend Control:</p> <ul> <li><code>ENABLE_CLAUDE_VISION</code>: <code>\"true\"</code> to allow Anthropic Vision backend</li> <li><code>ENABLE_OPENAI_VISION</code>: <code>\"true\"</code> to allow OpenAI Vision backend</li> </ul> <p>Anthropic Configuration:</p> <ul> <li><code>ANTHROPIC_API_KEY</code>: Required API key for Anthropic services</li> <li><code>ANTHROPIC_BASE_URL</code>: Default = <code>https://api.anthropic.com</code></li> <li><code>ANTHROPIC_MODEL_NAME</code>: Default = <code>claude-3-7-sonnet-20250219</code></li> </ul> <p>OpenAI Configuration:</p> <ul> <li><code>OPENAI_API_KEY</code>: Required API key for OpenAI services</li> <li><code>OPENAI_BASE_URL</code>: Default = <code>https://api.openai.com/v1</code></li> <li><code>OPENAI_MODEL_NAME</code>: Default = <code>gpt-4o</code></li> </ul> <p>Gemini Configuration:</p> <ul> <li><code>GEMINI_API_KEY</code>: Required API key for Google Gemini services</li> <li><code>GEMINI_MODEL_NAME</code>: Default = <code>gemini-2.5-pro</code></li> </ul>"},{"location":"tool_vqa/#function-reference","title":"Function Reference","text":"<p>The following functions are provided by the <code>vision_mcp_server.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_vqa/#visual_question_answeringimage_path_or_url-str-question-str","title":"<code>visual_question_answering(image_path_or_url: str, question: str)</code>","text":"<p>Ask questions about an image using a dual-pass analysis approach for comprehensive understanding.</p> <p>Two-Pass Analysis</p> <p>This function runs two passes:</p> <ol> <li>OCR pass using the selected vision backend with a meticulous extraction prompt</li> <li>VQA pass that analyzes the image and cross-checks against OCR text</li> </ol> <p>Parameters:</p> <ul> <li><code>image_path_or_url</code>: Local path (accessible to server) or web URL. HTTP URLs are auto-upgraded/validated to HTTPS for some backends</li> <li><code>question</code>: The user's question about the image</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Concatenated text with:<ul> <li><code>OCR results: ...</code></li> <li><code>VQA result: ...</code></li> </ul> </li> </ul> <p>Features:</p> <ul> <li>Automatic MIME detection, reads magic bytes, falls back to extension, final default is <code>image/jpeg</code></li> <li>Multi-backend support for different vision models</li> <li>Cross-validation between OCR and VQA results</li> </ul>"},{"location":"tool_vqa/#visual_audio_youtube_analyzingurl-str-question-str-provide_transcribe-bool-false","title":"<code>visual_audio_youtube_analyzing(url: str, question: str = \"\", provide_transcribe: bool = False)</code>","text":"<p>Analyze public YouTube videos (audio + visual). Supports watch pages, Shorts, and Live VODs.</p> <p>Supported URL Patterns</p> <p>Accepted URL patterns: <code>youtube.com/watch</code>, <code>youtube.com/shorts</code>, <code>youtube.com/live</code></p> <p>Parameters:</p> <ul> <li><code>url</code>: YouTube video URL (publicly accessible)</li> <li><code>question</code> (optional): A specific question about the video. You can scope by time using <code>MM:SS</code> or <code>MM:SS-MM:SS</code> (e.g., <code>01:45</code>, <code>03:20-03:45</code>)</li> <li><code>provide_transcribe</code> (optional, default <code>False</code>): If <code>True</code>, returns a timestamped transcription including salient events and brief visual descriptions</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Transcription of the video (if requested) and answer to the question</li> </ul> <p>Features:</p> <ul> <li>Gemini-powered video analysis (requires <code>GEMINI_API_KEY</code>)</li> <li>Dual mode: full transcript, targeted Q&amp;A, or both</li> <li>Time-scoped question answering for specific video segments</li> <li>Support for multiple YouTube video formats</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"tool_vqa_os/","title":"Vision Tools - Open Source (<code>vision_mcp_server_os.py</code>)","text":"<p>The Vision MCP Server (Open Source) enables Visual Question Answering (VQA) over images using open-source vision-language models. It provides comprehensive image analysis with support for local files and URLs.</p> <p>Available Functions</p> <p>This MCP server provides the following functions that agents can call:</p> <ul> <li>Visual Question Answering: Comprehensive image analysis and question answering</li> <li>Multi-Format Support: JPEG, PNG, GIF image formats</li> <li>Flexible Input: Local file paths and web URLs</li> <li>Open-Source Model Support: Qwen2.5-VL-72B-Instruct with automatic encoding</li> </ul>"},{"location":"tool_vqa_os/#environment-variables","title":"Environment Variables","text":"<p>Configuration Location</p> <p>The <code>vision_mcp_server_os.py</code> reads environment variables that are passed through the <code>tool-image-video-os.yaml</code> configuration file, not directly from <code>.env</code> file.</p> <p>Open-Source Model Configuration:</p> <ul> <li><code>VISION_API_KEY</code>: Required API key for the open-source vision service</li> <li><code>VISION_BASE_URL</code>: Base URL for the vision service API endpoint</li> <li><code>VISION_MODEL_NAME</code>: Model name (default: <code>Qwen/Qwen2.5-VL-72B-Instruct</code>)</li> </ul> <p>Example Configuration: <pre><code># API for Open-Source VQA Tool (for benchmark testing)\nVISION_MODEL_NAME=\"Qwen/Qwen2.5-VL-72B-Instruct\"\nVISION_API_KEY=your_vision_key\nVISION_BASE_URL=\"https://your_vision_base_url/v1/chat/completions\"\n</code></pre></p>"},{"location":"tool_vqa_os/#local-deployment","title":"Local Deployment","text":""},{"location":"tool_vqa_os/#using-sglang-server","title":"Using SGLang Server","text":"<p>For optimal performance with the Qwen2.5-VL-72B-Instruct model, deploy using SGLang (suggested SGLang version is <code>0.5.2</code>, as lower versions have potential issues with the model):</p> <pre><code>python3 -m sglang.launch_server \\\n  --model-path /path/to/Qwen2.5-VL-72B-Instruct \\\n  --tp 8 --host 0.0.0.0 --port 1234 \\\n  --trust-remote-code --enable-metrics \\\n  --log-level debug --log-level-http debug \\\n  --log-requests --log-requests-level 2 --show-time-cost\n</code></pre>"},{"location":"tool_vqa_os/#configuration-for-local-deployment","title":"Configuration for Local Deployment","text":"<p>When using local deployment, configure your environment variables:</p> <pre><code>VISION_MODEL_NAME=\"Qwen/Qwen2.5-VL-72B-Instruct\"\nVISION_API_KEY=\"dummy_key\"  # Not required for local deployment\nVISION_BASE_URL=\"http://localhost:1234/v1/chat/completions\"\n</code></pre>"},{"location":"tool_vqa_os/#function-reference","title":"Function Reference","text":"<p>The following function is provided by the <code>vision_mcp_server_os.py</code> MCP tool and can be called by agents:</p>"},{"location":"tool_vqa_os/#visual_question_answeringimage_path_or_url-str-question-str","title":"<code>visual_question_answering(image_path_or_url: str, question: str)</code>","text":"<p>Ask questions about images using open-source vision-language models. Supports both local files and web URLs with automatic format detection and encoding.</p> <p>Parameters:</p> <ul> <li><code>image_path_or_url</code>: Local file path (accessible to server) or web URL</li> <li><code>question</code>: The user's question about the image</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: The model's answer to the image-related question</li> </ul> <p>Supported Image Formats: - JPEG (.jpg, .jpeg) - PNG (.png) - GIF (.gif) - Default fallback to JPEG for unknown formats</p>"},{"location":"tool_vqa_os/#usage-examples","title":"Usage Examples","text":""},{"location":"tool_vqa_os/#image-analysis","title":"Image Analysis","text":"<pre><code># Local file analysis\nresult = visual_question_answering(\n    image_path_or_url=\"/path/to/image.jpg\",\n    question=\"What objects can you see in this image?\"\n)\n\n# URL analysis\nresult = visual_question_answering(\n    image_path_or_url=\"https://example.com/image.png\",\n    question=\"Describe the scene in detail.\"\n)\n</code></pre>"},{"location":"tool_vqa_os/#ocr-and-text-extraction","title":"OCR and Text Extraction","text":"<pre><code>result = visual_question_answering(\n    image_path_or_url=\"document.jpg\",\n    question=\"Extract all the text from this document.\"\n)\n</code></pre>"},{"location":"tool_vqa_os/#object-detection-and-counting","title":"Object Detection and Counting","text":"<pre><code>result = visual_question_answering(\n    image_path_or_url=\"scene.jpg\",\n    question=\"Count how many people are in this image and describe their activities.\"\n)\n</code></pre>"},{"location":"tool_vqa_os/#technical-diagram-analysis","title":"Technical Diagram Analysis","text":"<pre><code>result = visual_question_answering(\n    image_path_or_url=\"diagram.png\",\n    question=\"Explain this technical diagram and identify the key components.\"\n)\n</code></pre>"},{"location":"tool_vqa_os/#technical-implementation","title":"Technical Implementation","text":""},{"location":"tool_vqa_os/#image-processing-pipeline","title":"Image Processing Pipeline","text":"<ol> <li>Input Validation: Checks if input is local file or URL</li> <li>Format Detection: Determines MIME type from extension or headers</li> <li>Encoding: Converts images to Base64 for API transmission</li> <li>API Request: Sends structured request to vision model</li> <li>Response Processing: Extracts and returns model response</li> </ol>"},{"location":"tool_vqa_os/#error-handling","title":"Error Handling","text":"<ul> <li>File Access Errors: Graceful handling of inaccessible local files</li> <li>Network Errors: Robust URL fetching with proper error messages</li> <li>Format Errors: Fallback MIME type detection for unknown formats</li> <li>API Errors: Clear error reporting for service issues</li> </ul> <p>Documentation Info</p> <p>Last Updated: October 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"xbench_ds/","title":"xbench-DeepSearch","text":"<p>The xbench benchmark is an evaluation framework designed to measure both the intelligence frontier and real-world utility of AI agents. It consists of complementary tracks that test core model capabilities like reasoning, tool use, memory, and workflows grounded in business and professional settings. Its DeepSearch sub-track measures agents\u2019 ability to conduct open-domain information retrieval, combining fact finding, comparison, and synthesis through multi-step search and tool use.</p> <p>See more details at xbench official website and xbench-DeepSearch Eval Card.</p>"},{"location":"xbench_ds/#setup-and-evaluation-guide","title":"Setup and Evaluation Guide","text":""},{"location":"xbench_ds/#step-1-download-the-xbench-deepsearch-dataset","title":"Step 1: Download the xbench-DeepSearch Dataset","text":"<p>Direct Download (Recommended)</p> <p>Dataset Setup</p> <p>Use the integrated prepare-benchmark command to download and process the dataset:</p> <pre><code>uv run main.py prepare-benchmark get xbench-ds\n</code></pre> <p>By default, this will create the standardized dataset at data/xbench-ds/standardized_data.jsonl.</p>"},{"location":"xbench_ds/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<p>Required API Configuration</p> <p>Set up the required API keys for model access and tool functionality. Update the <code>.env</code> file to include the following keys:</p> .env Configuration<pre><code># Search and web scraping capabilities\nSERPER_API_KEY=\"your-serper-api-key\"\nJINA_API_KEY=\"your-jina-api-key\"\n\n# Code execution environment\nE2B_API_KEY=\"your-e2b-api-key\"\n\n# Primary LLM provider (Claude-3.7-Sonnet via OpenRouter)\nOPENROUTER_API_KEY=\"your-openrouter-api-key\"\nOPENROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\n\n# Vision understanding capabilities\nANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nGEMINI_API_KEY=\"your-gemini-api-key\"\n\n# LLM as judge, reasoning, and hint generation\nOPENAI_API_KEY=\"your-openai-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"xbench_ds/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_xbench-ds_claude37sonnet \\\n  output_dir=\"logs/xbench-ds/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre>"},{"location":"xbench_ds/#step-4-monitor-progress-and-resume","title":"Step 4: Monitor Progress and Resume","text":"<p>Progress Tracking</p> <p>You can monitor the evaluation progress in real-time:</p> Check Progress<pre><code>uv run utils/progress_check/check_xbench_progress.py $PATH_TO_LOG\n</code></pre> <p>Replace <code>$PATH_TO_LOG</code> with your actual output directory path.</p> <p>Resume Capability</p> <p>If the evaluation is interrupted, you can resume from where it left off by specifying the same output directory:</p> Resume Interrupted Evaluation<pre><code>uv run main.py common-benchmark \\\n  --config_file_name=agent_xbench-ds_claude37sonnet \\\n  output_dir=\"logs/xbench-ds/20250922_1430\"\n</code></pre>"},{"location":"xbench_ds/#post-processing-for-enhanced-performance","title":"Post-Processing for Enhanced Performance","text":"<p>Test-Time Scaling for Improved Reliability</p> <p>Test-time scaling can significantly improve the reliability of model responses. Instead of simple majority voting, we employ a comprehensive parallel thinking approach that:</p> <ul> <li>Aggregates final summary steps from each agent run before outputting results</li> <li>Uses another agent (o3 by default) to make final decisions based on equivalence and source reliability criteria</li> <li>Provides more robust and accurate final answers</li> </ul> <p>Execute the following command to run multiple xbench-DeepSearch evaluations and automatically enable parallel thinking for enhanced performance.</p> Multiple runs with parallel thinking post-processing<pre><code>bash scripts/run_evaluate_mulitple_runs_xbench-ds.sh\n</code></pre>"},{"location":"xbench_ds/#running-parallel-thinking-analysis-alone","title":"Running Parallel Thinking Analysis alone","text":"<p>After completing evaluations (single or multiple runs), you can apply parallel thinking post-processing to aggregate and generate the final result.</p> Parallel Thinking Post-Processing<pre><code>uv run utils/util_llm_parallel_thinking.py \\\n  --benchmark xbench-ds \\\n  --results_dir \"logs/xbench-ds/20250922_1430\"\n</code></pre> <p>The program automatically reads results from each run in the specified directory and performs aggregated analysis. The final output files are generated in the <code>results_dir</code>:</p> <ul> <li><code>llm_parallel_thinking_Nruns.json</code> - Detailed analysis results</li> <li><code>llm_parallel_thinking_accuracy_Nruns.txt</code> - Final accuracy</li> </ul> <p>Where <code>N</code> represents the total number of experimental runs (minimum of 1).</p> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"},{"location":"yaml_config/","title":"YAML Configuration Guide","text":"<p>MiroFlow uses a Hydra-based configuration system for customizing AI agents, tools, and benchmarks.</p>"},{"location":"yaml_config/#configuration-structure","title":"Configuration Structure","text":"Configuration Directory<pre><code>config/\n\u251c\u2500\u2500 agent_*.yaml                      # Agent configurations\n\u251c\u2500\u2500 agent_prompts/                    # Prompt classes\n\u251c\u2500\u2500 benchmark/                        # Benchmark settings\n\u2514\u2500\u2500 tool/                             # Tool configurations\n</code></pre>"},{"location":"yaml_config/#quick-start","title":"Quick Start","text":"<p>Run Benchmarks <pre><code># GAIA validation\nuv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation \\\n  output_dir=\"logs/gaia-validation/$(date +\"%Y%m%d_%H%M\")\"\n\n# GAIA text-only\nuv run main.py common-benchmark \\\n  --config_file_name=agent_gaia-validation-text-only \\\n  output_dir=\"logs/gaia-validation-text-only/$(date +\"%Y%m%d_%H%M\")\"\n</code></pre></p> <p>Single Task <pre><code>uv run main.py trace \\\n  --config_file_name=agent_quickstart_reading \\\n  --task=\"Your task here\" \\\n  --task_file_name=\"data/file.xlsx\"\n</code></pre></p>"},{"location":"yaml_config/#core-configuration","title":"Core Configuration","text":""},{"location":"yaml_config/#basic-agent-setup","title":"Basic Agent Setup","text":"Basic Agent Configuration<pre><code>defaults:\n  - benchmark: gaia-validation\n  - override hydra/job_logging: none\n  - _self_\n\nmain_agent:\n  prompt_class: MainAgentPromptBoxedAnswer\n  llm:\n    provider_class: \"ClaudeOpenRouterClient\"\n    model_name: \"anthropic/claude-3.7-sonnet\"\n    temperature: 0.3\n    max_tokens: 32000\n    openrouter_api_key: \"${oc.env:OPENROUTER_API_KEY,???}\"\n\n  tool_config: []  # Tools for main agent\n  max_turns: -1    # -1 = unlimited\n\nsub_agents:\n  agent-worker:\n    prompt_class: SubAgentWorkerPrompt\n    llm:\n      provider_class: \"ClaudeOpenRouterClient\"\n      model_name: \"anthropic/claude-3.7-sonnet\"\n    tool_config:\n      - tool-reading\n      - tool-searching\n    max_turns: -1\n\noutput_dir: logs/\ndata_dir: \"${oc.env:DATA_DIR,data}\"\n</code></pre>"},{"location":"yaml_config/#llm-providers","title":"LLM Providers","text":"<p>Available Providers</p> <ul> <li>Claude: <code>ClaudeOpenRouterClient</code>, <code>ClaudeAnthropicClient</code></li> <li>OpenAI: <code>GPTOpenAIClient</code></li> <li>MiroThinker: <code>MiroThinkerSGLangClient</code></li> <li>Qwen: <code>QwenSGLangClient</code></li> <li>DeepSeek: <code>DeepSeekNewAPIClient</code> (limited support)</li> </ul> <p>See LLM Clients Overview for details.</p>"},{"location":"yaml_config/#available-tools","title":"Available Tools","text":"<p>Tool Options</p> <ul> <li><code>tool-reasoning</code>: Enhanced reasoning capabilities</li> <li><code>tool-searching</code>: Web search and retrieval</li> <li><code>tool-reading</code>: Document processing</li> <li><code>tool-code</code>: Python code execution</li> <li><code>tool-image-video</code>: Visual content analysis</li> <li><code>tool-audio</code>: Audio processing</li> <li><code>tool-browsing</code>: Web browsing</li> </ul> <p>See Tool Overview for configurations.</p>"},{"location":"yaml_config/#advanced-features","title":"Advanced Features","text":""},{"location":"yaml_config/#gaia-benchmark-configuration","title":"GAIA Benchmark Configuration","text":"GAIA-Optimized Setup<pre><code>main_agent:\n  prompt_class: MainAgentPrompt_GAIA\n  tool_config:\n    - tool-reasoning\n\n  input_process:\n    hint_generation: true      # Use LLM for task hint generation\n  output_process:\n    final_answer_extraction: true  # Use LLM for answer extraction\n\nsub_agents:\n  agent-worker:\n    tool_config:\n      - tool-searching\n      - tool-reading\n      - tool-code\n      - tool-image-video\n      - tool-audio\n</code></pre>"},{"location":"yaml_config/#benchmark-settings","title":"Benchmark Settings","text":"Benchmark Configuration<pre><code>name: \"your-benchmark\"\ndata:\n  data_dir: \"${data_dir}/your-data\"\nexecution:\n  max_tasks: null      # null = no limit\n  max_concurrent: 3    # Parallel tasks\n  pass_at_k: 1         # Attempts per task\n</code></pre>"},{"location":"yaml_config/#environment-variables","title":"Environment Variables","text":"Required .env Configuration<pre><code># LLM Providers\nOPENROUTER_API_KEY=\"your_key\"\nANTHROPIC_API_KEY=\"your_key\"\nOPENAI_API_KEY=\"your_key\"\n\n# Tools\nSERPER_API_KEY=\"your_key\"\nJINA_API_KEY=\"your_key\"\nE2B_API_KEY=\"your_key\"\n\n# Optional\nDATA_DIR=\"data/\"\nCHINESE_CONTEXT=\"false\"\n</code></pre>"},{"location":"yaml_config/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>temperature</code> LLM creativity (0.0-1.0) 0.3 <code>max_tokens</code> Response length limit 32000 <code>max_turns</code> Conversation turns (-1 = unlimited) -1 <code>max_tool_calls_per_turn</code> Tool calls per turn 10 <code>max_concurrent</code> Parallel benchmark tasks 3"},{"location":"yaml_config/#best-practices","title":"Best Practices","text":"<p>Quick Tips</p> <ul> <li>Start simple: Use <code>agent_quickstart_reading.yaml</code> as a base</li> <li>Tool selection: Choose tools based on your task requirements</li> <li>API keys: Always use environment variables, never hardcode</li> <li>Resource limits: Set <code>max_concurrent</code> and <code>max_tokens</code> appropriately</li> <li>Development: Use higher <code>temperature</code> and unlimited <code>max_turns</code> for exploration</li> </ul> <p>Documentation Info</p> <p>Last Updated: September 2025 \u00b7 Doc Contributor: Team @ MiroMind AI</p>"}]}